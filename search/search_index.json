{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"KSML Documentation Welcome to the KSML documentation. KSML allows you to build powerful Kafka Streams applications using YAML and Python, without writing Java code. 1. Getting Started Introduction to KSML Installation and Setup KSML Basics Tutorial 2. Core Concepts Data Types Stream Types Notations Functions Pipelines Operations 3. Tutorials Beginner Tutorials Intermediate Tutorials Advanced Tutorials 4. Use Case Guides Use Case Guides 5. Reference KSML Language Reference Data Types Reference Stream Types Reference Notations Reference Functions Reference Operations Reference Configuration Reference 6. Resources Examples Library Troubleshooting Guide Migration Guide Community and Support Latest Release Release Notes","title":"Home"},{"location":"#ksml-documentation","text":"Welcome to the KSML documentation. KSML allows you to build powerful Kafka Streams applications using YAML and Python, without writing Java code.","title":"KSML Documentation"},{"location":"#1-getting-started","text":"Introduction to KSML Installation and Setup KSML Basics Tutorial","title":"1. Getting Started"},{"location":"#2-core-concepts","text":"Data Types Stream Types Notations Functions Pipelines Operations","title":"2. Core Concepts"},{"location":"#3-tutorials","text":"Beginner Tutorials Intermediate Tutorials Advanced Tutorials","title":"3. Tutorials"},{"location":"#4-use-case-guides","text":"Use Case Guides","title":"4. Use Case Guides"},{"location":"#5-reference","text":"KSML Language Reference Data Types Reference Stream Types Reference Notations Reference Functions Reference Operations Reference Configuration Reference","title":"5. Reference"},{"location":"#6-resources","text":"Examples Library Troubleshooting Guide Migration Guide Community and Support","title":"6. Resources"},{"location":"#latest-release","text":"Release Notes","title":"Latest Release"},{"location":"about/","text":"About KSML Kafka Streams The Kafka Streams framework is at the heart of many organisations' event sourcing, analytical processing, real-time, batch or ML workloads. It has a beautiful Java DSL that allows developers to focus on what their applications need to do, not how to do it. But the Java requirement for Kafka Streams also holds people back. If you don\u2019t know Java, does that mean you cannot use Kafka Streams? KSML Enter KSML. KSML is a wrapper language and interpreter around Kafka Streams that lets you express any topology in a YAML syntax. Simply define your topology as a processing pipeline with a series of steps that your data passes through. Your custom functions can be expressed inline in Python. KSML will read your definition and construct the topology dynamically via the Kafka Streams DSL and run it in GraalVM. Documentation This documentation takes you for a deep-dive into KSML. It covers the basic concepts, how to run, and of course contains lots of useful and near-real-world examples. After going through these docs, you will understand how easily and quickly you can develop Kafka Streams applications without writing a single line of Java.","title":"About"},{"location":"about/#about-ksml","text":"","title":"About KSML"},{"location":"about/#kafka-streams","text":"The Kafka Streams framework is at the heart of many organisations' event sourcing, analytical processing, real-time, batch or ML workloads. It has a beautiful Java DSL that allows developers to focus on what their applications need to do, not how to do it. But the Java requirement for Kafka Streams also holds people back. If you don\u2019t know Java, does that mean you cannot use Kafka Streams?","title":"Kafka Streams"},{"location":"about/#ksml","text":"Enter KSML. KSML is a wrapper language and interpreter around Kafka Streams that lets you express any topology in a YAML syntax. Simply define your topology as a processing pipeline with a series of steps that your data passes through. Your custom functions can be expressed inline in Python. KSML will read your definition and construct the topology dynamically via the Kafka Streams DSL and run it in GraalVM.","title":"KSML"},{"location":"about/#documentation","text":"This documentation takes you for a deep-dive into KSML. It covers the basic concepts, how to run, and of course contains lots of useful and near-real-world examples. After going through these docs, you will understand how easily and quickly you can develop Kafka Streams applications without writing a single line of Java.","title":"Documentation"},{"location":"functions/","text":"Functions Table of Contents Introduction Data types in Python Data type mapping Automatic conversion Function Types Function parameters Logger Metrics State stores Introduction Functions can be specified in the functions section of a KSML definition file. The layout typically looks like this: functions: my_first_predicate: type: predicate expression: key=='Some string' compare_params: type: generic parameters: - name: firstParam type: string - name: secondParam type: int globalCode: | import something from package globalVar = 3 code: | print('Hello there!') expression: firstParam == str(secondParam) Functions are defined by the following tags: Parameter Value Type Default Description type string generic The type of the function defined parameters List of parameter definitions empty list A list of parameters, each of which contains the mandatory fields name and type . See example above. globalCode string empty Snippet of Python code that is executed once upon creation of the Kafka Streams topology. This section can contain statements like import to import function libraries used in the code and expression sections. code string empty Python source code, which will be included in the called function. expression string empty Python expression that contains the returned function result. See below for the list of supported function types. Data types in Python Internally, KSML uses an abstraction to deal with all kinds of data types. See types for more information on data types. Data type mapping Data types are automatically converted to/from Python in the following manner: Data type Python type Example boolean bool True, False bytes bytearray double float 3.145 float float 1.23456 byte int between -128 and 127 short int between -65,536 and 65,535 int int between -2,147,483,648 and 2,147,483,647 long int between -9,223,372,036,854,775,808 and 9,223,372,036,854,775,807 string str \"text\" enum str enum string literal, eg. \"BLUE\", \"EUROPE\" list array [ \"key1\", \"key2\" ] struct dict { \"key1\": \"value1\", \"key2\": \"value2\" } struct with schema dict { \"key1\": \"value1\", \"key2\": \"value2\", \"@type\": \"SensorData\", \"@schema\": \"...\" } tuple tuple (1, \"text\", 3.14, { \"key\": \"value\" }) union Real value is translated as specified in this table Automatic conversion KSML is able to automatically convert between types. Examples are: To/from string conversion is handled automatically for almost all data types, including string-notations such as CSV, JSON and XML. When a string is expected, but a struct is passed in, the struct is automatically converted to string. When a struct is expected, but a string is passed in, the string is parsed according to the notation specified. Field matching and field type conversion is done automatically. For instance, if a struct contains an integer field, but the target schema expects a string, the integer is automatically converted to string. Function Types Functions in KSML always have a type . When no type is specified, the function type is inferred from the context, or it defaults back to generic . This section discusses the purpose of every function type, and what fixed arguments every call gets passed in. Aggregator An aggregator incrementally integrates a new keu/value into an aggregatedValue. It is called for every new message that becomes part of the aggregated result. The following highlights which calls are made to which function type during a regular aggregation, in this case for counting the number of messages: # Aggregation starts initializer() -> 0 msg1: aggregator(msg1.key, msg1.value, 0) -> 1 msg2: aggregator(msg2.key, msg2.value, 1) -> 1 msg3: aggregator(msg3.key, msg3.value, 2) -> 3 The result in this example is 3. Aggregators get the following fixed arguments: Parameter Value Type Description key any The key of the message to be included in the aggregated result thus far. value any The value of the message to be included in the aggregated result thus far. aggregatedValue any The aggregated value thus far. returns any The new aggregated result, which includes the latest message. ForEach A forEach function is called for every message in a stream. When part of a forEach operation at the end of a pipeline, the function is the last one called for every message. When this function is called during peek operations, it may look at the messages and cause side effects (e.g. printing the message to stdout), and the pipeline will continue with the unmodified message after doing so. ForEach functions get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns none Nothing is returned. ForeignKeyExtractor A foreignKeyExtractor is a function used during (left) joins of two tables. The function translates a value from \"this table\" and translates it into a key of the \"other table\" that is joined with. ForEach functions get the following fixed arguments: Parameter Value Type Description value any The value of the message. returns any The key looked up in the table joined with. Generator A generator is a function that generates new messages out of thin air. It is most often used to generate mock data for testing purposes. Generators get no arguments, and return messages to be sent to the output stream. Parameter Value Type Description returns (_any_, _any ) The key/value of the message to be sent to the output stream. Generic A generic function can be used for generic purposes. It can be used for any operation, as long as its parameters match the expected types of the operation's function. Generic functions get any arguments, and may return anything. Parameter Value Type Description self-defined any Self-defined parameters can be passed in. returns any Can return any value. Initializer An initializer is called upon the start of every (part of an) aggregation. It takes no arguments and should return an initial value for the aggregation. Parameter Value Type Description returns any An initial value for the aggregation. In a counting aggregation, this would be 0 . KeyTransformer A keyTransformer is able to transform a key/value into a new key, which then gets combined with the original value as a new message on the output stream. KeyTransformers get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns any The key of the output message. KeyValuePrinter A keyValuePrinter takes a message and converts it to string before outputting it to a file or printing it to stdout. KeyValuePrinters get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns string The string to be written to file or stdout. KeyValueToKeyValueListTransformer A keyValueToKeyValueListTransformer takes one message and converts it into a list of output messages, which then get sent to the output stream. An example for this type of function would be a message, which contains a list of items in its value (e.g. (k, [item]) . Using a transformKeyValueToKeyValueList operation, this message can be converted into individual messages (k,item1), (k,item2), ... on the output stream. KeyValueToKeyValueListTransformers get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns [(_any_, _any_)] A list of messages for the output stream. KeyValueToValueListTransformer A keyValueToValueListTransformer takes one message and converts it into a list of output values, which then get combined with the original key and sent to the output stream. An example for this type of function would be a message, which contains a list of items in its value (e.g. (k, [item]) . Using a transformKeyValueToValueList operation, this message can be converted into a list of values [item1, item2, ...] which get combined with the key of the message into (k,item1), (k,item2), ... on the output stream. KeyValueToValueListTransformers get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns [_any_] A list of values to be combined with the key on the output stream. KeyValueTransformer A keyValueTransformer takes one message and converts it into another message, which may have different key/value types. KeyValueTransformers get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns (_any_, _any_) The transformed message. Merger A merger takes a key and two values, and merges those values together into a new value. That value is combined with the original key and sent to the output stream. Mergers get the following fixed arguments: Parameter Value Type Description key any The key of the message. value1 any The value of the first message. value2 any The value of the second message. returns any The merged value of the output message. MetadataTransformer A metadataTransformer can transform a message's metadata (headers and timestamp). MetadataTransformers get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. metadata dict Contains the headers and timestamp of the message. returns dict The (optionally) modified metadata for the output message. This structure should have the same type as the metadata passed in. Predicate A predicate is a function that takes the key/value of a message and returns True or False . It is used for filtering and branching purposes (e.g. routing messages based on content). Predicates get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns boolean True or False . Reducer A reducer is a function that combines two aggregated results into one. Reducers get the following fixed arguments: Parameter Value Type Description value1 any The value of the first aggregation result. value2 any The value of the second aggregation result. returns any The value of the combined aggregation result. StreamPartitioner A streamPartitioner is a function that can assign a partition number to every message. It is used to repartition Kafka topics, based on message contents. StreamPartitioners get the following fixed arguments: Parameter Value Type Description topic string The topic of the message. key any The key of the message. value any The value of the message. numPartitions integer The number of partitions available on the output topic. returns integer The partition number to which this message gets sent. TimestampExtractor A timestampExtractor is a function which can determine a timestamp from a given input message, which is used for all downstream processing. TimestampExtractors get the following fixed arguments: Parameter Value Type Description record struct A dictionary containing the timestamp , timestampType , key , value , topic , partition and offset of the input message. previousTimestamp long The timestamp of the last message (before this one). returns long The timestamp to apply to this message. TopicNameExtractor A topicNameExtractor is a function which can derive a topic name from a message, for example by getting the customer name from a message and deriving the topic name from that. It is used by toTopicNameExtractor operations to send messages to individually determined topics. TopicNameExtractors get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns string The name of the topic to send this message to. ValueJoiner A valueJoiner takes a key and two values, and combines the two values into one. That value is then combined with the original key and sent to the output stream. ValueJoiners get the following fixed arguments: Parameter Value Type Description key any The key of the message. value1 any The value of the first message. value2 any The value of the second message. returns any The joined value of the output message. ValueTransformer A valueTransformer takes a key/value and transforms it into a new value, which is combined with the original key and sent to the output stream. ValueTransformers get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns any The value of the output message. Function parameters Besides the parameters mentioned above, all Python functions in KSML get special parameters passed in: Logger Every function can access the log variable, which is mapped to a plain Java Logger object. It can be used to send output to the KSML log by calling its methods. It supports the following operations: Method Description error(message: str, value_params...) logs an error message warn(message: str, value_params...) logs a warning message info(message: str, value_params...) logs an informational message debug(message: str, value_params...) logs a debug message trace(message: str, value_params...) logs a trace message The message contains double curly brackets {} , which will be substituted by the value parameters. Examples are: log.error(\"Something went completely bad here!\") log.info(\"Received message from topic: key={}, value={}\", key, value) log.debug(\"I'm printing five variables here: {}, {}, {}, {}, {}. Lovely isn't it?\", 1, 2, 3, \"text\", {\"json\":\"is cool\"}) Output of the above statements looks like: [LOG TIMESTAMP] ERROR function.name Something went completely bad here! [LOG TIMESTAMP] INFO function.name Received message from topic: key=123, value={\"key\":\"value\"} [LOG TIMESTAMP] DEBUG function.name I'm printing five variables here: 1, 2, 3, text, {\"json\":\"is cool\"}. Lovely isn't it? Metrics KSML supports metric collection and exposure through JMX and built-in Prometheus agent. Metrics for Python functions are automatically generated and collected, but users can also specify their own metrics. For an example, see 17-example-inspect-with-metrics.yaml in the examples directory. KSML supports the following metric types: Counter: an increasing integer, which counts for example the number of calls made to a Python function. Meter: used for periodically updating a measurement value. Preferred over Counter when don't care too much about exact averages, but want to monitor trends instead. Timer: measures the time spent by processes or functions, that get called internally. Every Python function in KSML can use the metrics variable, which is made available by KSML. The object supports the following methods to create your own metrics: counter(name: str, tags: dict) -> Counter counter(name: str) -> Counter meter(name: str, tags: dict) -> Meter meter(name: str) -> Meter timer(name: str, tags: dict) -> Timer timer(name: str) -> Timer In turn these objects support the following: Counter increment() increment(delta: int) Meter mark() mark(nrOfEvents: int) Timer updateSeconds(valueSeconds: int) updateMillis(valueMillis: int) updateNanos(valueNanos: int) State stores Some functions are allowed to access local state stores. These functions specify the stores attribute in their definitions. The state stores they reference are accessible as variables with the same name as the state store. Examples: streams: sensor_source_avro: topic: ksml_sensordata_avro keyType: string valueType: avro:SensorData stores: last_sensor_data_store: type: keyValue keyType: string valueType: json persistent: false historyRetention: 1h caching: false logging: false functions: process_message: type: forEach code: | last_value = last_sensor_data_store.get(key) if last_value is not None: log.info(\"Found last value: {} = {}\", key, last_value) last_sensor_data_store.put(key, value) if value is not None: log.info(\"Stored new value: {} = {}\", key, value) stores: - last_sensor_data_store pipelines: process_message: from: sensor_source_avro forEach: process_message In this example the function process_message uses the state store last_sensor_data_store directly as a variable. It is allowed to do that when it declares such use in its definition under the stores attribute. State stores have common methods like get and put , which you can call directly from Python code.","title":"Functions"},{"location":"functions/#functions","text":"","title":"Functions"},{"location":"functions/#table-of-contents","text":"Introduction Data types in Python Data type mapping Automatic conversion Function Types Function parameters Logger Metrics State stores","title":"Table of Contents"},{"location":"functions/#introduction","text":"Functions can be specified in the functions section of a KSML definition file. The layout typically looks like this: functions: my_first_predicate: type: predicate expression: key=='Some string' compare_params: type: generic parameters: - name: firstParam type: string - name: secondParam type: int globalCode: | import something from package globalVar = 3 code: | print('Hello there!') expression: firstParam == str(secondParam) Functions are defined by the following tags: Parameter Value Type Default Description type string generic The type of the function defined parameters List of parameter definitions empty list A list of parameters, each of which contains the mandatory fields name and type . See example above. globalCode string empty Snippet of Python code that is executed once upon creation of the Kafka Streams topology. This section can contain statements like import to import function libraries used in the code and expression sections. code string empty Python source code, which will be included in the called function. expression string empty Python expression that contains the returned function result. See below for the list of supported function types.","title":"Introduction"},{"location":"functions/#data-types-in-python","text":"Internally, KSML uses an abstraction to deal with all kinds of data types. See types for more information on data types.","title":"Data types in Python"},{"location":"functions/#data-type-mapping","text":"Data types are automatically converted to/from Python in the following manner: Data type Python type Example boolean bool True, False bytes bytearray double float 3.145 float float 1.23456 byte int between -128 and 127 short int between -65,536 and 65,535 int int between -2,147,483,648 and 2,147,483,647 long int between -9,223,372,036,854,775,808 and 9,223,372,036,854,775,807 string str \"text\" enum str enum string literal, eg. \"BLUE\", \"EUROPE\" list array [ \"key1\", \"key2\" ] struct dict { \"key1\": \"value1\", \"key2\": \"value2\" } struct with schema dict { \"key1\": \"value1\", \"key2\": \"value2\", \"@type\": \"SensorData\", \"@schema\": \"...\" } tuple tuple (1, \"text\", 3.14, { \"key\": \"value\" }) union Real value is translated as specified in this table","title":"Data type mapping"},{"location":"functions/#automatic-conversion","text":"KSML is able to automatically convert between types. Examples are: To/from string conversion is handled automatically for almost all data types, including string-notations such as CSV, JSON and XML. When a string is expected, but a struct is passed in, the struct is automatically converted to string. When a struct is expected, but a string is passed in, the string is parsed according to the notation specified. Field matching and field type conversion is done automatically. For instance, if a struct contains an integer field, but the target schema expects a string, the integer is automatically converted to string.","title":"Automatic conversion"},{"location":"functions/#function-types","text":"Functions in KSML always have a type . When no type is specified, the function type is inferred from the context, or it defaults back to generic . This section discusses the purpose of every function type, and what fixed arguments every call gets passed in.","title":"Function Types"},{"location":"functions/#aggregator","text":"An aggregator incrementally integrates a new keu/value into an aggregatedValue. It is called for every new message that becomes part of the aggregated result. The following highlights which calls are made to which function type during a regular aggregation, in this case for counting the number of messages: # Aggregation starts initializer() -> 0 msg1: aggregator(msg1.key, msg1.value, 0) -> 1 msg2: aggregator(msg2.key, msg2.value, 1) -> 1 msg3: aggregator(msg3.key, msg3.value, 2) -> 3 The result in this example is 3. Aggregators get the following fixed arguments: Parameter Value Type Description key any The key of the message to be included in the aggregated result thus far. value any The value of the message to be included in the aggregated result thus far. aggregatedValue any The aggregated value thus far. returns any The new aggregated result, which includes the latest message.","title":"Aggregator"},{"location":"functions/#foreach","text":"A forEach function is called for every message in a stream. When part of a forEach operation at the end of a pipeline, the function is the last one called for every message. When this function is called during peek operations, it may look at the messages and cause side effects (e.g. printing the message to stdout), and the pipeline will continue with the unmodified message after doing so. ForEach functions get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns none Nothing is returned.","title":"ForEach"},{"location":"functions/#foreignkeyextractor","text":"A foreignKeyExtractor is a function used during (left) joins of two tables. The function translates a value from \"this table\" and translates it into a key of the \"other table\" that is joined with. ForEach functions get the following fixed arguments: Parameter Value Type Description value any The value of the message. returns any The key looked up in the table joined with.","title":"ForeignKeyExtractor"},{"location":"functions/#generator","text":"A generator is a function that generates new messages out of thin air. It is most often used to generate mock data for testing purposes. Generators get no arguments, and return messages to be sent to the output stream. Parameter Value Type Description returns (_any_, _any ) The key/value of the message to be sent to the output stream.","title":"Generator"},{"location":"functions/#generic","text":"A generic function can be used for generic purposes. It can be used for any operation, as long as its parameters match the expected types of the operation's function. Generic functions get any arguments, and may return anything. Parameter Value Type Description self-defined any Self-defined parameters can be passed in. returns any Can return any value.","title":"Generic"},{"location":"functions/#initializer","text":"An initializer is called upon the start of every (part of an) aggregation. It takes no arguments and should return an initial value for the aggregation. Parameter Value Type Description returns any An initial value for the aggregation. In a counting aggregation, this would be 0 .","title":"Initializer"},{"location":"functions/#keytransformer","text":"A keyTransformer is able to transform a key/value into a new key, which then gets combined with the original value as a new message on the output stream. KeyTransformers get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns any The key of the output message.","title":"KeyTransformer"},{"location":"functions/#keyvalueprinter","text":"A keyValuePrinter takes a message and converts it to string before outputting it to a file or printing it to stdout. KeyValuePrinters get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns string The string to be written to file or stdout.","title":"KeyValuePrinter"},{"location":"functions/#keyvaluetokeyvaluelisttransformer","text":"A keyValueToKeyValueListTransformer takes one message and converts it into a list of output messages, which then get sent to the output stream. An example for this type of function would be a message, which contains a list of items in its value (e.g. (k, [item]) . Using a transformKeyValueToKeyValueList operation, this message can be converted into individual messages (k,item1), (k,item2), ... on the output stream. KeyValueToKeyValueListTransformers get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns [(_any_, _any_)] A list of messages for the output stream.","title":"KeyValueToKeyValueListTransformer"},{"location":"functions/#keyvaluetovaluelisttransformer","text":"A keyValueToValueListTransformer takes one message and converts it into a list of output values, which then get combined with the original key and sent to the output stream. An example for this type of function would be a message, which contains a list of items in its value (e.g. (k, [item]) . Using a transformKeyValueToValueList operation, this message can be converted into a list of values [item1, item2, ...] which get combined with the key of the message into (k,item1), (k,item2), ... on the output stream. KeyValueToValueListTransformers get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns [_any_] A list of values to be combined with the key on the output stream.","title":"KeyValueToValueListTransformer"},{"location":"functions/#keyvaluetransformer","text":"A keyValueTransformer takes one message and converts it into another message, which may have different key/value types. KeyValueTransformers get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns (_any_, _any_) The transformed message.","title":"KeyValueTransformer"},{"location":"functions/#merger","text":"A merger takes a key and two values, and merges those values together into a new value. That value is combined with the original key and sent to the output stream. Mergers get the following fixed arguments: Parameter Value Type Description key any The key of the message. value1 any The value of the first message. value2 any The value of the second message. returns any The merged value of the output message.","title":"Merger"},{"location":"functions/#metadatatransformer","text":"A metadataTransformer can transform a message's metadata (headers and timestamp). MetadataTransformers get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. metadata dict Contains the headers and timestamp of the message. returns dict The (optionally) modified metadata for the output message. This structure should have the same type as the metadata passed in.","title":"MetadataTransformer"},{"location":"functions/#predicate","text":"A predicate is a function that takes the key/value of a message and returns True or False . It is used for filtering and branching purposes (e.g. routing messages based on content). Predicates get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns boolean True or False .","title":"Predicate"},{"location":"functions/#reducer","text":"A reducer is a function that combines two aggregated results into one. Reducers get the following fixed arguments: Parameter Value Type Description value1 any The value of the first aggregation result. value2 any The value of the second aggregation result. returns any The value of the combined aggregation result.","title":"Reducer"},{"location":"functions/#streampartitioner","text":"A streamPartitioner is a function that can assign a partition number to every message. It is used to repartition Kafka topics, based on message contents. StreamPartitioners get the following fixed arguments: Parameter Value Type Description topic string The topic of the message. key any The key of the message. value any The value of the message. numPartitions integer The number of partitions available on the output topic. returns integer The partition number to which this message gets sent.","title":"StreamPartitioner"},{"location":"functions/#timestampextractor","text":"A timestampExtractor is a function which can determine a timestamp from a given input message, which is used for all downstream processing. TimestampExtractors get the following fixed arguments: Parameter Value Type Description record struct A dictionary containing the timestamp , timestampType , key , value , topic , partition and offset of the input message. previousTimestamp long The timestamp of the last message (before this one). returns long The timestamp to apply to this message.","title":"TimestampExtractor"},{"location":"functions/#topicnameextractor","text":"A topicNameExtractor is a function which can derive a topic name from a message, for example by getting the customer name from a message and deriving the topic name from that. It is used by toTopicNameExtractor operations to send messages to individually determined topics. TopicNameExtractors get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns string The name of the topic to send this message to.","title":"TopicNameExtractor"},{"location":"functions/#valuejoiner","text":"A valueJoiner takes a key and two values, and combines the two values into one. That value is then combined with the original key and sent to the output stream. ValueJoiners get the following fixed arguments: Parameter Value Type Description key any The key of the message. value1 any The value of the first message. value2 any The value of the second message. returns any The joined value of the output message.","title":"ValueJoiner"},{"location":"functions/#valuetransformer","text":"A valueTransformer takes a key/value and transforms it into a new value, which is combined with the original key and sent to the output stream. ValueTransformers get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns any The value of the output message.","title":"ValueTransformer"},{"location":"functions/#function-parameters","text":"Besides the parameters mentioned above, all Python functions in KSML get special parameters passed in:","title":"Function parameters"},{"location":"functions/#logger","text":"Every function can access the log variable, which is mapped to a plain Java Logger object. It can be used to send output to the KSML log by calling its methods. It supports the following operations: Method Description error(message: str, value_params...) logs an error message warn(message: str, value_params...) logs a warning message info(message: str, value_params...) logs an informational message debug(message: str, value_params...) logs a debug message trace(message: str, value_params...) logs a trace message The message contains double curly brackets {} , which will be substituted by the value parameters. Examples are: log.error(\"Something went completely bad here!\") log.info(\"Received message from topic: key={}, value={}\", key, value) log.debug(\"I'm printing five variables here: {}, {}, {}, {}, {}. Lovely isn't it?\", 1, 2, 3, \"text\", {\"json\":\"is cool\"}) Output of the above statements looks like: [LOG TIMESTAMP] ERROR function.name Something went completely bad here! [LOG TIMESTAMP] INFO function.name Received message from topic: key=123, value={\"key\":\"value\"} [LOG TIMESTAMP] DEBUG function.name I'm printing five variables here: 1, 2, 3, text, {\"json\":\"is cool\"}. Lovely isn't it?","title":"Logger"},{"location":"functions/#metrics","text":"KSML supports metric collection and exposure through JMX and built-in Prometheus agent. Metrics for Python functions are automatically generated and collected, but users can also specify their own metrics. For an example, see 17-example-inspect-with-metrics.yaml in the examples directory. KSML supports the following metric types: Counter: an increasing integer, which counts for example the number of calls made to a Python function. Meter: used for periodically updating a measurement value. Preferred over Counter when don't care too much about exact averages, but want to monitor trends instead. Timer: measures the time spent by processes or functions, that get called internally. Every Python function in KSML can use the metrics variable, which is made available by KSML. The object supports the following methods to create your own metrics: counter(name: str, tags: dict) -> Counter counter(name: str) -> Counter meter(name: str, tags: dict) -> Meter meter(name: str) -> Meter timer(name: str, tags: dict) -> Timer timer(name: str) -> Timer In turn these objects support the following:","title":"Metrics"},{"location":"functions/#counter","text":"increment() increment(delta: int)","title":"Counter"},{"location":"functions/#meter","text":"mark() mark(nrOfEvents: int)","title":"Meter"},{"location":"functions/#timer","text":"updateSeconds(valueSeconds: int) updateMillis(valueMillis: int) updateNanos(valueNanos: int)","title":"Timer"},{"location":"functions/#state-stores","text":"Some functions are allowed to access local state stores. These functions specify the stores attribute in their definitions. The state stores they reference are accessible as variables with the same name as the state store. Examples: streams: sensor_source_avro: topic: ksml_sensordata_avro keyType: string valueType: avro:SensorData stores: last_sensor_data_store: type: keyValue keyType: string valueType: json persistent: false historyRetention: 1h caching: false logging: false functions: process_message: type: forEach code: | last_value = last_sensor_data_store.get(key) if last_value is not None: log.info(\"Found last value: {} = {}\", key, last_value) last_sensor_data_store.put(key, value) if value is not None: log.info(\"Stored new value: {} = {}\", key, value) stores: - last_sensor_data_store pipelines: process_message: from: sensor_source_avro forEach: process_message In this example the function process_message uses the state store last_sensor_data_store directly as a variable. It is allowed to do that when it declares such use in its definition under the stores attribute. State stores have common methods like get and put , which you can call directly from Python code.","title":"State stores"},{"location":"introduction/","text":"KSML: Kafka Streams Without The Need For Java Abstract Kafka Streams has captured the hearts and minds of many developers that want to develop streaming applications on top of Kafka. But as powerful as the framework is, Kafka Streams has had a hard time getting around the requirement of writing Java code and setting up build pipelines. There were some attempts to rebuild Kafka Streams, but up until now popular languages like Python did not receive equally powerful (and maintained) stream processing frameworks. In this article we will present a new declarative approach to unlock Kafka Streams, called KSML. By the time you finish reading this document, you will be able to write streaming applications yourself, using only a few simple basic rules and Python snippets. Setting up a test environment KSML in practice Example 1. Inspect data on a topic Example 2. Copying data to another topic Example 3. Filtering data Example 4. Branching messages Example 5. Dynamic routing Example 6. Multiple pipelines Setting up a test environment To demonstrate KSML's capabilities, you will need a working Kafka cluster, or an Axual Platform/Cloud environment. Check out the Runners page to configure KSML. We set up a test topic, called ksml_sensordata_avro with key/value types of String / SensorData . The [SensorData] schema was created for demo purposes only and contains several fields to demonstrate KSML capabilities: { \"namespace\": \"io.axual.ksml.example\", \"doc\": \"Emulated sensor data with a few additional attributes\", \"name\": \"SensorData\", \"type\": \"record\", \"fields\": [ { \"doc\": \"The name of the sensor\", \"name\": \"name\", \"type\": \"string\" }, { \"doc\": \"The timestamp of the sensor reading\", \"name\": \"timestamp\", \"type\": \"long\" }, { \"doc\": \"The value of the sensor, represented as string\", \"name\": \"value\", \"type\": \"string\" }, { \"doc\": \"The type of the sensor\", \"name\": \"type\", \"type\": { \"name\": \"SensorType\", \"type\": \"enum\", \"doc\": \"The type of a sensor\", \"symbols\": [ \"AREA\", \"HUMIDITY\", \"LENGTH\", \"STATE\", \"TEMPERATURE\" ] } }, { \"doc\": \"The unit of the sensor\", \"name\": \"unit\", \"type\": \"string\" }, { \"doc\": \"The color of the sensor\", \"name\": \"color\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"doc\": \"The city of the sensor\", \"name\": \"city\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"doc\": \"The owner of the sensor\", \"name\": \"owner\", \"type\": [ \"null\", \"string\" ], \"default\": null } ] } For the rest of this document, we assume you have set up the ksml_sensordata_avro topic and populated it with some random data. So without any further delays, let's see how KSML allows us to process this data. KSML in practice Example 1. Inspect data on a topic The first example is one where we inspect data on a specific topic. The definition is as follows: streams: sensor_source_avro: topic: ksml_sensordata_avro keyType: string valueType: avro:SensorData functions: # Log the message using the built-in log variable that is passed in from Java log_message: type: forEach parameters: - name: format type: string code: log.info(\"Consumed {} message - key={}, value={}\", format, key, value) pipelines: # Multiple pipelines can be created in a single KSML definition consume_avro: from: sensor_source_avro forEach: code: log_message(key, value, format=\"AVRO\") Let's analyze this definition one element at a time. Before defining processing logic, we first define the streams used by the definition. In this case we define a stream named sensor_source_avro which reads from the topic ksml_sensordata_avro . The stream defines a string key and Avro SensorData values. Next is a list of functions that can be used by the processing logic. Here we define just one, log_message , which simply uses the provided logger to write the key, value and format of a message to the console. The third element pipelines defines the real processing logic. We define a pipeline called consume_avro , which takes messages from ksml_sensordata_avro and passes them to print_message . The definition file is parsed by KSML and translated into a Kafka Streams topology, which is described as follows: Topologies: Sub-topology: 0 Source: ksml_sensordata_avro (topics: [ksml_sensordata_avro]) --> inspect_inspect_pipelines_consume_avro Processor: inspect_inspect_pipelines_consume_avro (stores: []) --> none <-- ksml_sensordata_avro And the output of the generated topology looks like this: 2024-03-06T18:31:57,196Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor9, value={'city': 'Alkmaar', 'color': 'yellow', 'name': 'sensor9', 'owner': 'Bob', 'timestamp': 1709749917190, 'type': 'LENGTH', 'unit': 'm', 'value': '562', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:57,631Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor3, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor3', 'owner': 'Bob', 'timestamp': 1709749917628, 'type': 'HUMIDITY', 'unit': 'g/m3', 'value': '23', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:58,082Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor6, value={'city': 'Amsterdam', 'color': 'white', 'name': 'sensor6', 'owner': 'Bob', 'timestamp': 1709749918078, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '64', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:58,528Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor9, value={'city': 'Amsterdam', 'color': 'black', 'name': 'sensor9', 'owner': 'Evan', 'timestamp': 1709749918524, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '87', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:58,970Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor1, value={'city': 'Amsterdam', 'color': 'black', 'name': 'sensor1', 'owner': 'Bob', 'timestamp': 1709749918964, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '75', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:59,412Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor5, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor5', 'owner': 'Bob', 'timestamp': 1709749919409, 'type': 'LENGTH', 'unit': 'm', 'value': '658', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} As you can see, the output of the application is exactly that what we defined it to be in the log_message function, namely a dump of all data found on the topic. Example 2. Copying data to another topic Now that we can see what data is on a topic, we will start to manipulate its routing. In this example we are copying unmodified data to a secondary topic: streams: - topic: ksml_sensordata_avro keyType: string valueType: avro:SensorData - topic: ksml_sensordata_copy keyType: string valueType: avro:SensorData functions: # Log the message using the built-in log variable that is passed in from Java log_message: type: forEach parameters: - name: format type: string code: log.info(\"Consumed {} message - key={}, value={}\", format, key, value) pipelines: # Every pipeline logs its own message, passing in the format parameter to log_message above consume_avro: from: sensor_source_avro via: - type: peek forEach: code: log_message(key, value, format=\"AVRO\") to: sensor_copy You can see that we specified a second stream named sensor_copy in this example, which is backed by the topic ksml_sensordata_copy target topic. The log_message function is unchanged, but the pipeline did undergo some changes. Two new elements are introduced here, namely via and to . The via tag allows users to define a series of operations executed on the data. In this case there is only one, namely a peek operation which does not modify any data, but simply outputs the data on stdout as a side effect. The to operation is a so-called \"sink operation\". Sink operations are always last in a pipeline. Processing of the pipeline does not continue after it was delivered to a sink operation. Note that in the first example above forEach is also a sink operation, whereas in this example we achieve the same result by passing the log_message function as a parameter to the peek operation. When this definition is translated by KSML, the following Kafka Streams topology is created: Topologies: Sub-topology: 0 Source: ksml_sensordata_avro (topics: [ksml_sensordata_avro]) --> inspect_inspect_pipelines_consume_avro_via_1 Processor: inspect_inspect_pipelines_consume_avro_via_1 (stores: []) --> inspect_inspect_ToOperationParser_001 <-- ksml_sensordata_avro Sink: inspect_inspect_ToOperationParser_001 (topic: ksml_sensordata_copy) <-- inspect_inspect_pipelines_consume_avro_via_1 The output is similar to that of example 1, but the same data can also be found on the ksml_sensordata_copy topic now. Example 3. Filtering data Now that we can read and write data, let's see if we can apply some logic to the processing as well. In this example we will be filtering data based on the contents of the value: # This example shows how to read from four simple streams and log all messages streams: sensor_source_avro: topic: ksml_sensordata_avro keyType: string valueType: avro:SensorData sensor_filtered: topic: ksml_sensordata_filtered keyType: string valueType: avro:SensorData functions: # Log the message using the built-in log variable that is passed in from Java log_message: type: forEach parameters: - name: format type: string code: log.info(\"Consumed {} message - key={}, value={}\", format, key, value) sensor_is_blue: type: predicate code: | if value[\"color\"] == \"blue\": return True expression: False pipelines: # Every pipeline logs its own message, passing in the format parameter to log_message above consume_avro: from: sensor_source_avro via: - type: filter if: sensor_is_blue - type: peek forEach: code: log_message(key, value, format=\"AVRO\") to: sensor_filtered Again, first we define the streams and the functions involved in the processing. You can see we added a new function called filter_message which returns true or false based on the color field in the value of the message. This function is used below in the pipeline. The pipeline is extended to include a filter operation, which takes a predicate function as parameter. That function is called for every input message. Only messages for which the function returns true are propagated. All other messages are discarded. Using this definition, KSML generates the following Kafka Streams topology: Topologies: Sub-topology: 0 Source: ksml_sensordata_avro (topics: [ksml_sensordata_avro]) --> inspect_inspect_pipelines_consume_avro_via_1 Processor: inspect_inspect_pipelines_consume_avro_via_1 (stores: []) --> inspect_inspect_pipelines_consume_avro_via_2 <-- ksml_sensordata_avro Processor: inspect_inspect_pipelines_consume_avro_via_2 (stores: []) --> inspect_inspect_ToOperationParser_001 <-- inspect_inspect_pipelines_consume_avro_via_1 Sink: inspect_inspect_ToOperationParser_001 (topic: ksml_sensordata_filtered) <-- inspect_inspect_pipelines_consume_avro_via_2 When it executes, we see the following output: 2024-03-06T18:45:10,401Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor9, value={'city': 'Alkmaar', 'color': 'blue', 'name': 'sensor9', 'owner': 'Bob', 'timestamp': 1709749917190, 'type': 'LENGTH', 'unit': 'm', 'value': '562', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:45:10,735Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor3, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor3', 'owner': 'Bob', 'timestamp': 1709749917628, 'type': 'HUMIDITY', 'unit': 'g/m3', 'value': '23', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:45:11,215Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor6, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor6', 'owner': 'Bob', 'timestamp': 1709749918078, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '64', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:45:11,484Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor9, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor9', 'owner': 'Evan', 'timestamp': 1709749918524, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '87', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:45:11,893Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor1, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor1', 'owner': 'Bob', 'timestamp': 1709749918964, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '75', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:45:12,008Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor5, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor5', 'owner': 'Bob', 'timestamp': 1709749919409, 'type': 'LENGTH', 'unit': 'm', 'value': '658', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} As you can see, the filter operation did its work. Only messages with field color set to blue are passed on to the peek operation, while other messages are discarded. Example 4. Branching messages Another way to filter messages is to use a branch operation. This is also a sink operation, which closes the processing of a pipeline. It is similar to forEach and to in that respect, but has a different definition and behaviour. streams: sensor_source: topic: ksml_sensordata_avro keyType: string valueType: avro:SensorData sensor_blue: topic: ksml_sensordata_blue keyType: string valueType: avro:SensorData sensor_red: topic: ksml_sensordata_red keyType: string valueType: avro:SensorData pipelines: main: from: sensor_source via: - type: peek forEach: code: log.info(\"SOURCE MESSAGE - key={}, value={}\", key, value) branch: - if: expression: value is not None and value[\"color\"] == \"blue\" to: sensor_blue - if: expression: value is not None and value[\"color\"] == \"red\" to: sensor_red - forEach: code: log.warn(\"UNKNOWN COLOR - {}\", value[\"color\"]) The branch operation takes a list of branches as its parameters, which each specifies a processing pipeline of its own. Branches contain the keyword if , which take a predicate function that determines if a message will flow into that particular branch, or if it will be passed to the next branch(es). Every message will only end up in one branch, namely the first one in order where the if predicate function returns true . In the example we see that the first branch will be populated only with messages with color field set to blue . Once there, these messages will be written to ksml_sensordata_blue . The second branch will only contain messages with color = red and these messages will be written to ksml_sensordata_red . Finally, the last branch outputs a message that the color is unknown and ends any further processing. When translated by KSML the following Kafka Streams topology is set up: Topologies: Sub-topology: 0 Source: ksml_sensordata_avro (topics: [ksml_sensordata_avro]) --> branch_branch_pipelines_main_via_1 Processor: branch_branch_pipelines_main_via_1 (stores: []) --> branch_branch_branch_001 <-- ksml_sensordata_avro Processor: branch_branch_branch_001 (stores: []) --> branch_branch_branch_001-predicate-0, branch_branch_branch_001-predicate-1, branch_branch_branch_001-predicate-2 <-- branch_branch_pipelines_main_via_1 Processor: branch_branch_branch_001-predicate-0 (stores: []) --> branch_branch_ToOperationParser_001 <-- branch_branch_branch_001 Processor: branch_branch_branch_001-predicate-1 (stores: []) --> branch_branch_ToOperationParser_002 <-- branch_branch_branch_001 Processor: branch_branch_branch_001-predicate-2 (stores: []) --> branch_branch_pipelines_main_branch_3 <-- branch_branch_branch_001 Sink: branch_branch_ToOperationParser_001 (topic: ksml_sensordata_blue) <-- branch_branch_branch_001-predicate-0 Sink: branch_branch_ToOperationParser_002 (topic: ksml_sensordata_red) <-- branch_branch_branch_001-predicate-1 Processor: branch_branch_pipelines_main_branch_3 (stores: []) --> none <-- branch_branch_branch_001-predicate-2 It is clear that the branch operation is integrated in this topology. Its output looks like this: 2024-03-06T18:31:57,196Z INFO k.f.branch_pipelines_main_via_1_forEach SOURCE MESSAGE - key=sensor9, value={'city': 'Alkmaar', 'color': 'yellow', 'name': 'sensor9', 'owner': 'Bob', 'timestamp': 1709749917190, 'type': 'LENGTH', 'unit': 'm', 'value': '562', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:57,631Z INFO k.f.branch_pipelines_main_via_1_forEach SOURCE MESSAGE - key=sensor3, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor3', 'owner': 'Bob', 'timestamp': 1709749917628, 'type': 'HUMIDITY', 'unit': 'g/m3', 'value': '23', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:58,082Z INFO k.f.branch_pipelines_main_via_1_forEach SOURCE MESSAGE - key=sensor6, value={'city': 'Amsterdam', 'color': 'white', 'name': 'sensor6', 'owner': 'Bob', 'timestamp': 1709749918078, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '64', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:58,528Z INFO k.f.branch_pipelines_main_via_1_forEach SOURCE MESSAGE - key=sensor9, value={'city': 'Amsterdam', 'color': 'black', 'name': 'sensor9', 'owner': 'Evan', 'timestamp': 1709749918524, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '87', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:58,529Z WARN k.f.branch_pipelines_main_branch_3_forEach UNKNOWN COLOR - black 2024-03-06T18:31:58,970Z INFO k.f.branch_pipelines_main_via_1_forEach SOURCE MESSAGE - key=sensor1, value={'city': 'Amsterdam', 'color': 'black', 'name': 'sensor1', 'owner': 'Bob', 'timestamp': 1709749918964, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '75', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:58,972Z WARN k.f.branch_pipelines_main_branch_3_forEach UNKNOWN COLOR - black 2024-03-06T18:31:59,412Z INFO k.f.branch_pipelines_main_via_1_forEach SOURCE MESSAGE - key=sensor5, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor5', 'owner': 'Bob', 'timestamp': 1709749919409, 'type': 'LENGTH', 'unit': 'm', 'value': '658', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} We see that every message processed by the pipeline is logged through the k.f.branch_pipelines_main_via_1_forEach logger. But the branch operation sorts the messages and sends messages with colors blue and red into their own branches. The only colors that show up as UNKNOWN COLOR - messages are non-blue and non-red and send through the branch_pipelines_main_branch_3_forEach logger. Example 5. Dynamic routing Sometimes it is necessary to route a message to one stream or another based on the content of a message. This example shows how to route messages dynamically using a TopicNameExtractor. streams: sensor_source: topic: ksml_sensordata_avro keyType: string valueType: avro:SensorData pipelines: main: from: sensor_source via: - type: peek forEach: code: log.info(\"SOURCE MESSAGE - key={}, value={}\", key, value) to: topicNameExtractor: code: | if key == 'sensor1': return 'ksml_sensordata_sensor1' if key == 'sensor2': return 'ksml_sensordata_sensor2' return 'ksml_sensordata_sensor0' The topicNameExtractor operation takes a function, which determines the routing of every message by returning a topic name string. In this case, when the key of a message is sensor1 then the message will be sent to ksml_sensordata_sensor1 . When it contains sensor2 the message is sent to ksml_sensordata_sensor2 . All other messages are sent to ksml_sensordata_sensor0 . The equivalent Kafka Streams topology looks like this: Topologies: Sub-topology: 0 Source: ksml_sensordata_avro (topics: [ksml_sensordata_avro]) --> route_route_pipelines_main_via_1 Processor: route_route_pipelines_main_via_1 (stores: []) --> route_route_ToOperationParser_001 <-- ksml_sensordata_avro Sink: route_route_ToOperationParser_001 (extractor class: io.axual.ksml.user.UserTopicNameExtractor@5d28e108) <-- route_route_pipelines_main_via_1 The output does not show anything special compared to previous examples, since all messages are simply written by the logger. Example 6. Multiple pipelines In the previous examples there was always a single pipeline definition for processing data. KSML allows us to define multiple pipelines in a single file. In this example we combine the filtering example with the routing example. We will also define new pipelines with the sole purpose of logging the routed messages. # This example shows how to route messages to a dynamic topic. The target topic is the result of an executed function. streams: sensor_source_avro: topic: ksml_sensordata_avro keyType: string valueType: avro:SensorData sensor_filtered: topic: ksml_sensordata_filtered keyType: string valueType: avro:SensorData sensor_0: topic: ksml_sensordata_sensor0 keyType: string valueType: avro:SensorData sensor_1: topic: ksml_sensordata_sensor1 keyType: string valueType: avro:SensorData sensor_2: topic: ksml_sensordata_sensor2 keyType: string valueType: avro:SensorData functions: # Only pass the message to the next step in the pipeline if the color is blue sensor_is_blue: type: predicate code: | if value[\"color\"] == \"blue\": return True expression: False pipelines: filtering: from: sensor_source_avro via: - type: filter if: sensor_is_blue to: sensor_filtered routing: from: sensor_filtered via: - type: peek forEach: code: log.info(\"Routing Blue sensor - key={}, value={}\", key, value) to: topicNameExtractor: code: | if key == 'sensor1': return 'ksml_sensordata_sensor1' if key == 'sensor2': return 'ksml_sensordata_sensor2' return 'ksml_sensordata_sensor0' sensor0_peek: from: sensor_0 forEach: code: log.info(\"SENSOR0 - key={}, value={}\", key, value) sensor1_peek: from: sensor_1 forEach: code: log.info(\"SENSOR1 - key={}, value={}\", key, value) sensor2_peek: from: sensor_2 forEach: code: log.info(\"SENSOR2 - key={}, value={}\", key, value) In this definition we defined five pipelines: filtering which filters out all sensor messages that don't have the color blue and sends it to the sensor_filtered stream. routing which routes the data on the sensor_filtered stream to one of three target topics sensor0_peek which writes the content of the sensor_0 stream to the console sensor1_peek which writes the content of the sensor_1 stream to the console sensor2_peek which writes the content of the sensor_2 stream to the console The equivalent Kafka Streams topology looks like this: Topologies: Sub-topology: 0 Source: ksml_sensordata_avro (topics: [ksml_sensordata_avro]) --> multiple_multiple_pipelines_filtering_via_1 Processor: multiple_multiple_pipelines_filtering_via_1 (stores: []) --> multiple_multiple_ToOperationParser_001 <-- ksml_sensordata_avro Sink: multiple_multiple_ToOperationParser_001 (topic: ksml_sensordata_filtered) <-- multiple_multiple_pipelines_filtering_via_1 Sub-topology: 1 Source: ksml_sensordata_filtered (topics: [ksml_sensordata_filtered]) --> multiple_multiple_pipelines_routing_via_1 Processor: multiple_multiple_pipelines_routing_via_1 (stores: []) --> multiple_multiple_ToOperationParser_002 <-- ksml_sensordata_filtered Sink: multiple_multiple_ToOperationParser_002 (extractor class: io.axual.ksml.user.UserTopicNameExtractor@2700f556) <-- multiple_multiple_pipelines_routing_via_1 Sub-topology: 2 Source: ksml_sensordata_sensor0 (topics: [ksml_sensordata_sensor0]) --> multiple_multiple_pipelines_sensor0_peek Processor: multiple_multiple_pipelines_sensor0_peek (stores: []) --> none <-- ksml_sensordata_sensor0 Sub-topology: 3 Source: ksml_sensordata_sensor1 (topics: [ksml_sensordata_sensor1]) --> multiple_multiple_pipelines_sensor1_peek Processor: multiple_multiple_pipelines_sensor1_peek (stores: []) --> none <-- ksml_sensordata_sensor1 Sub-topology: 4 Source: ksml_sensordata_sensor2 (topics: [ksml_sensordata_sensor2]) --> multiple_multiple_pipelines_sensor2_peek Processor: multiple_multiple_pipelines_sensor2_peek (stores: []) --> none <-- ksml_sensordata_sensor2 And this is what the output would look something like this. The sensor peeks messages will not always be shown immediately after the Routing messages. This is because the pipelines are running in separate sub processes. 2024-03-06T20:11:39,520Z INFO k.f.route2_pipelines_routing_via_1_forEach Routing Blue sensor - key=sensor6, value={'city': 'Utrecht', 'color': 'blue', 'name': 'sensor6', 'owner': 'Charlie', 'timestamp': 1709755877401, 'type': 'LENGTH', 'unit': 'ft', 'value': '507', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,523Z INFO k.f.route2_pipelines_sensor0_peek_forEach SENSOR0 - key=sensor6, value={'city': 'Utrecht', 'color': 'blue', 'name': 'sensor6', 'owner': 'Charlie', 'timestamp': 1709755877401, 'type': 'LENGTH', 'unit': 'ft', 'value': '507', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,533Z INFO k.f.route2_pipelines_routing_via_1_forEach Routing Blue sensor - key=sensor1, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor1', 'owner': 'Evan', 'timestamp': 1709755889834, 'type': 'LENGTH', 'unit': 'm', 'value': '609', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,535Z INFO k.f.route2_pipelines_sensor1_peek_forEach SENSOR1 - key=sensor1, value={'city': 'Xanten', 'color': 'blue', 'name': 'sensor1', 'owner': 'Evan', 'timestamp': 1709755817913, 'type': 'STATE', 'unit': 'state', 'value': 'on', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,539Z INFO k.f.route2_pipelines_routing_via_1_forEach Routing Blue sensor - key=sensor7, value={'city': 'Utrecht', 'color': 'blue', 'name': 'sensor7', 'owner': 'Evan', 'timestamp': 1709755892051, 'type': 'HUMIDITY', 'unit': '%', 'value': '77', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,5419Z INFO k.f.route2_pipelines_sensor0_peek_forEach SENSOR0 - key=sensor7, value={'city': 'Utrecht', 'color': 'blue', 'name': 'sensor7', 'owner': 'Evan', 'timestamp': 1709755892051, 'type': 'HUMIDITY', 'unit': '%', 'value': '77', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,546Z INFO k.f.route2_pipelines_routing_via_1_forEach Routing Blue sensor - key=sensor2, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor2', 'owner': 'Bob', 'timestamp': 1709755893390, 'type': 'HUMIDITY', 'unit': '%', 'value': '75', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,549Z INFO k.f.route2_pipelines_sensor1_peek_forEach SENSOR2 - key=sensor2, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor2', 'owner': 'Bob', 'timestamp': 1709755893390, 'type': 'HUMIDITY', 'unit': '%', 'value': '75', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,552Z INFO k.f.route2_pipelines_routing_via_1_forEach Routing Blue sensor - key=sensor1, value={'city': 'Xanten', 'color': 'blue', 'name': 'sensor1', 'owner': 'Bob', 'timestamp': 1709755894717, 'type': 'HUMIDITY', 'unit': '%', 'value': '76', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,555Z INFO k.f.route2_pipelines_sensor1_peek_forEach SENSOR1 - key=sensor1, value={'city': 'Xanten', 'color': 'blue', 'name': 'sensor1', 'owner': 'Bob', 'timestamp': 1709755894717, 'type': 'HUMIDITY', 'unit': '%', 'value': '76', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,558Z INFO k.f.route2_pipelines_routing_via_1_forEach Routing Blue sensor - key=sensor9, value={'city': 'Alkmaar', 'color': 'blue', 'name': 'sensor9', 'owner': 'Alice', 'timestamp': 1709755896937, 'type': 'HUMIDITY', 'unit': '%', 'value': '65', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,562Z INFO k.f.route2_pipelines_sensor0_peek_forEach SENSOR0 - key=sensor9, value={'city': 'Alkmaar', 'color': 'blue', 'name': 'sensor9', 'owner': 'Alice', 'timestamp': 1709755896937, 'type': 'HUMIDITY', 'unit': '%', 'value': '65', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}}","title":"KSML: Kafka Streams Without The Need For Java"},{"location":"introduction/#ksml-kafka-streams-without-the-need-for-java","text":"","title":"KSML: Kafka Streams Without The Need For Java"},{"location":"introduction/#abstract","text":"Kafka Streams has captured the hearts and minds of many developers that want to develop streaming applications on top of Kafka. But as powerful as the framework is, Kafka Streams has had a hard time getting around the requirement of writing Java code and setting up build pipelines. There were some attempts to rebuild Kafka Streams, but up until now popular languages like Python did not receive equally powerful (and maintained) stream processing frameworks. In this article we will present a new declarative approach to unlock Kafka Streams, called KSML. By the time you finish reading this document, you will be able to write streaming applications yourself, using only a few simple basic rules and Python snippets. Setting up a test environment KSML in practice Example 1. Inspect data on a topic Example 2. Copying data to another topic Example 3. Filtering data Example 4. Branching messages Example 5. Dynamic routing Example 6. Multiple pipelines","title":"Abstract"},{"location":"introduction/#setting-up-a-test-environment","text":"To demonstrate KSML's capabilities, you will need a working Kafka cluster, or an Axual Platform/Cloud environment. Check out the Runners page to configure KSML. We set up a test topic, called ksml_sensordata_avro with key/value types of String / SensorData . The [SensorData] schema was created for demo purposes only and contains several fields to demonstrate KSML capabilities: { \"namespace\": \"io.axual.ksml.example\", \"doc\": \"Emulated sensor data with a few additional attributes\", \"name\": \"SensorData\", \"type\": \"record\", \"fields\": [ { \"doc\": \"The name of the sensor\", \"name\": \"name\", \"type\": \"string\" }, { \"doc\": \"The timestamp of the sensor reading\", \"name\": \"timestamp\", \"type\": \"long\" }, { \"doc\": \"The value of the sensor, represented as string\", \"name\": \"value\", \"type\": \"string\" }, { \"doc\": \"The type of the sensor\", \"name\": \"type\", \"type\": { \"name\": \"SensorType\", \"type\": \"enum\", \"doc\": \"The type of a sensor\", \"symbols\": [ \"AREA\", \"HUMIDITY\", \"LENGTH\", \"STATE\", \"TEMPERATURE\" ] } }, { \"doc\": \"The unit of the sensor\", \"name\": \"unit\", \"type\": \"string\" }, { \"doc\": \"The color of the sensor\", \"name\": \"color\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"doc\": \"The city of the sensor\", \"name\": \"city\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"doc\": \"The owner of the sensor\", \"name\": \"owner\", \"type\": [ \"null\", \"string\" ], \"default\": null } ] } For the rest of this document, we assume you have set up the ksml_sensordata_avro topic and populated it with some random data. So without any further delays, let's see how KSML allows us to process this data.","title":"Setting up a test environment"},{"location":"introduction/#ksml-in-practice","text":"","title":"KSML in practice"},{"location":"introduction/#example-1-inspect-data-on-a-topic","text":"The first example is one where we inspect data on a specific topic. The definition is as follows: streams: sensor_source_avro: topic: ksml_sensordata_avro keyType: string valueType: avro:SensorData functions: # Log the message using the built-in log variable that is passed in from Java log_message: type: forEach parameters: - name: format type: string code: log.info(\"Consumed {} message - key={}, value={}\", format, key, value) pipelines: # Multiple pipelines can be created in a single KSML definition consume_avro: from: sensor_source_avro forEach: code: log_message(key, value, format=\"AVRO\") Let's analyze this definition one element at a time. Before defining processing logic, we first define the streams used by the definition. In this case we define a stream named sensor_source_avro which reads from the topic ksml_sensordata_avro . The stream defines a string key and Avro SensorData values. Next is a list of functions that can be used by the processing logic. Here we define just one, log_message , which simply uses the provided logger to write the key, value and format of a message to the console. The third element pipelines defines the real processing logic. We define a pipeline called consume_avro , which takes messages from ksml_sensordata_avro and passes them to print_message . The definition file is parsed by KSML and translated into a Kafka Streams topology, which is described as follows: Topologies: Sub-topology: 0 Source: ksml_sensordata_avro (topics: [ksml_sensordata_avro]) --> inspect_inspect_pipelines_consume_avro Processor: inspect_inspect_pipelines_consume_avro (stores: []) --> none <-- ksml_sensordata_avro And the output of the generated topology looks like this: 2024-03-06T18:31:57,196Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor9, value={'city': 'Alkmaar', 'color': 'yellow', 'name': 'sensor9', 'owner': 'Bob', 'timestamp': 1709749917190, 'type': 'LENGTH', 'unit': 'm', 'value': '562', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:57,631Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor3, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor3', 'owner': 'Bob', 'timestamp': 1709749917628, 'type': 'HUMIDITY', 'unit': 'g/m3', 'value': '23', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:58,082Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor6, value={'city': 'Amsterdam', 'color': 'white', 'name': 'sensor6', 'owner': 'Bob', 'timestamp': 1709749918078, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '64', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:58,528Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor9, value={'city': 'Amsterdam', 'color': 'black', 'name': 'sensor9', 'owner': 'Evan', 'timestamp': 1709749918524, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '87', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:58,970Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor1, value={'city': 'Amsterdam', 'color': 'black', 'name': 'sensor1', 'owner': 'Bob', 'timestamp': 1709749918964, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '75', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:59,412Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor5, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor5', 'owner': 'Bob', 'timestamp': 1709749919409, 'type': 'LENGTH', 'unit': 'm', 'value': '658', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} As you can see, the output of the application is exactly that what we defined it to be in the log_message function, namely a dump of all data found on the topic.","title":"Example 1. Inspect data on a topic"},{"location":"introduction/#example-2-copying-data-to-another-topic","text":"Now that we can see what data is on a topic, we will start to manipulate its routing. In this example we are copying unmodified data to a secondary topic: streams: - topic: ksml_sensordata_avro keyType: string valueType: avro:SensorData - topic: ksml_sensordata_copy keyType: string valueType: avro:SensorData functions: # Log the message using the built-in log variable that is passed in from Java log_message: type: forEach parameters: - name: format type: string code: log.info(\"Consumed {} message - key={}, value={}\", format, key, value) pipelines: # Every pipeline logs its own message, passing in the format parameter to log_message above consume_avro: from: sensor_source_avro via: - type: peek forEach: code: log_message(key, value, format=\"AVRO\") to: sensor_copy You can see that we specified a second stream named sensor_copy in this example, which is backed by the topic ksml_sensordata_copy target topic. The log_message function is unchanged, but the pipeline did undergo some changes. Two new elements are introduced here, namely via and to . The via tag allows users to define a series of operations executed on the data. In this case there is only one, namely a peek operation which does not modify any data, but simply outputs the data on stdout as a side effect. The to operation is a so-called \"sink operation\". Sink operations are always last in a pipeline. Processing of the pipeline does not continue after it was delivered to a sink operation. Note that in the first example above forEach is also a sink operation, whereas in this example we achieve the same result by passing the log_message function as a parameter to the peek operation. When this definition is translated by KSML, the following Kafka Streams topology is created: Topologies: Sub-topology: 0 Source: ksml_sensordata_avro (topics: [ksml_sensordata_avro]) --> inspect_inspect_pipelines_consume_avro_via_1 Processor: inspect_inspect_pipelines_consume_avro_via_1 (stores: []) --> inspect_inspect_ToOperationParser_001 <-- ksml_sensordata_avro Sink: inspect_inspect_ToOperationParser_001 (topic: ksml_sensordata_copy) <-- inspect_inspect_pipelines_consume_avro_via_1 The output is similar to that of example 1, but the same data can also be found on the ksml_sensordata_copy topic now.","title":"Example 2. Copying data to another topic"},{"location":"introduction/#example-3-filtering-data","text":"Now that we can read and write data, let's see if we can apply some logic to the processing as well. In this example we will be filtering data based on the contents of the value: # This example shows how to read from four simple streams and log all messages streams: sensor_source_avro: topic: ksml_sensordata_avro keyType: string valueType: avro:SensorData sensor_filtered: topic: ksml_sensordata_filtered keyType: string valueType: avro:SensorData functions: # Log the message using the built-in log variable that is passed in from Java log_message: type: forEach parameters: - name: format type: string code: log.info(\"Consumed {} message - key={}, value={}\", format, key, value) sensor_is_blue: type: predicate code: | if value[\"color\"] == \"blue\": return True expression: False pipelines: # Every pipeline logs its own message, passing in the format parameter to log_message above consume_avro: from: sensor_source_avro via: - type: filter if: sensor_is_blue - type: peek forEach: code: log_message(key, value, format=\"AVRO\") to: sensor_filtered Again, first we define the streams and the functions involved in the processing. You can see we added a new function called filter_message which returns true or false based on the color field in the value of the message. This function is used below in the pipeline. The pipeline is extended to include a filter operation, which takes a predicate function as parameter. That function is called for every input message. Only messages for which the function returns true are propagated. All other messages are discarded. Using this definition, KSML generates the following Kafka Streams topology: Topologies: Sub-topology: 0 Source: ksml_sensordata_avro (topics: [ksml_sensordata_avro]) --> inspect_inspect_pipelines_consume_avro_via_1 Processor: inspect_inspect_pipelines_consume_avro_via_1 (stores: []) --> inspect_inspect_pipelines_consume_avro_via_2 <-- ksml_sensordata_avro Processor: inspect_inspect_pipelines_consume_avro_via_2 (stores: []) --> inspect_inspect_ToOperationParser_001 <-- inspect_inspect_pipelines_consume_avro_via_1 Sink: inspect_inspect_ToOperationParser_001 (topic: ksml_sensordata_filtered) <-- inspect_inspect_pipelines_consume_avro_via_2 When it executes, we see the following output: 2024-03-06T18:45:10,401Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor9, value={'city': 'Alkmaar', 'color': 'blue', 'name': 'sensor9', 'owner': 'Bob', 'timestamp': 1709749917190, 'type': 'LENGTH', 'unit': 'm', 'value': '562', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:45:10,735Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor3, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor3', 'owner': 'Bob', 'timestamp': 1709749917628, 'type': 'HUMIDITY', 'unit': 'g/m3', 'value': '23', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:45:11,215Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor6, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor6', 'owner': 'Bob', 'timestamp': 1709749918078, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '64', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:45:11,484Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor9, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor9', 'owner': 'Evan', 'timestamp': 1709749918524, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '87', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:45:11,893Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor1, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor1', 'owner': 'Bob', 'timestamp': 1709749918964, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '75', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:45:12,008Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor5, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor5', 'owner': 'Bob', 'timestamp': 1709749919409, 'type': 'LENGTH', 'unit': 'm', 'value': '658', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} As you can see, the filter operation did its work. Only messages with field color set to blue are passed on to the peek operation, while other messages are discarded.","title":"Example 3. Filtering data"},{"location":"introduction/#example-4-branching-messages","text":"Another way to filter messages is to use a branch operation. This is also a sink operation, which closes the processing of a pipeline. It is similar to forEach and to in that respect, but has a different definition and behaviour. streams: sensor_source: topic: ksml_sensordata_avro keyType: string valueType: avro:SensorData sensor_blue: topic: ksml_sensordata_blue keyType: string valueType: avro:SensorData sensor_red: topic: ksml_sensordata_red keyType: string valueType: avro:SensorData pipelines: main: from: sensor_source via: - type: peek forEach: code: log.info(\"SOURCE MESSAGE - key={}, value={}\", key, value) branch: - if: expression: value is not None and value[\"color\"] == \"blue\" to: sensor_blue - if: expression: value is not None and value[\"color\"] == \"red\" to: sensor_red - forEach: code: log.warn(\"UNKNOWN COLOR - {}\", value[\"color\"]) The branch operation takes a list of branches as its parameters, which each specifies a processing pipeline of its own. Branches contain the keyword if , which take a predicate function that determines if a message will flow into that particular branch, or if it will be passed to the next branch(es). Every message will only end up in one branch, namely the first one in order where the if predicate function returns true . In the example we see that the first branch will be populated only with messages with color field set to blue . Once there, these messages will be written to ksml_sensordata_blue . The second branch will only contain messages with color = red and these messages will be written to ksml_sensordata_red . Finally, the last branch outputs a message that the color is unknown and ends any further processing. When translated by KSML the following Kafka Streams topology is set up: Topologies: Sub-topology: 0 Source: ksml_sensordata_avro (topics: [ksml_sensordata_avro]) --> branch_branch_pipelines_main_via_1 Processor: branch_branch_pipelines_main_via_1 (stores: []) --> branch_branch_branch_001 <-- ksml_sensordata_avro Processor: branch_branch_branch_001 (stores: []) --> branch_branch_branch_001-predicate-0, branch_branch_branch_001-predicate-1, branch_branch_branch_001-predicate-2 <-- branch_branch_pipelines_main_via_1 Processor: branch_branch_branch_001-predicate-0 (stores: []) --> branch_branch_ToOperationParser_001 <-- branch_branch_branch_001 Processor: branch_branch_branch_001-predicate-1 (stores: []) --> branch_branch_ToOperationParser_002 <-- branch_branch_branch_001 Processor: branch_branch_branch_001-predicate-2 (stores: []) --> branch_branch_pipelines_main_branch_3 <-- branch_branch_branch_001 Sink: branch_branch_ToOperationParser_001 (topic: ksml_sensordata_blue) <-- branch_branch_branch_001-predicate-0 Sink: branch_branch_ToOperationParser_002 (topic: ksml_sensordata_red) <-- branch_branch_branch_001-predicate-1 Processor: branch_branch_pipelines_main_branch_3 (stores: []) --> none <-- branch_branch_branch_001-predicate-2 It is clear that the branch operation is integrated in this topology. Its output looks like this: 2024-03-06T18:31:57,196Z INFO k.f.branch_pipelines_main_via_1_forEach SOURCE MESSAGE - key=sensor9, value={'city': 'Alkmaar', 'color': 'yellow', 'name': 'sensor9', 'owner': 'Bob', 'timestamp': 1709749917190, 'type': 'LENGTH', 'unit': 'm', 'value': '562', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:57,631Z INFO k.f.branch_pipelines_main_via_1_forEach SOURCE MESSAGE - key=sensor3, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor3', 'owner': 'Bob', 'timestamp': 1709749917628, 'type': 'HUMIDITY', 'unit': 'g/m3', 'value': '23', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:58,082Z INFO k.f.branch_pipelines_main_via_1_forEach SOURCE MESSAGE - key=sensor6, value={'city': 'Amsterdam', 'color': 'white', 'name': 'sensor6', 'owner': 'Bob', 'timestamp': 1709749918078, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '64', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:58,528Z INFO k.f.branch_pipelines_main_via_1_forEach SOURCE MESSAGE - key=sensor9, value={'city': 'Amsterdam', 'color': 'black', 'name': 'sensor9', 'owner': 'Evan', 'timestamp': 1709749918524, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '87', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:58,529Z WARN k.f.branch_pipelines_main_branch_3_forEach UNKNOWN COLOR - black 2024-03-06T18:31:58,970Z INFO k.f.branch_pipelines_main_via_1_forEach SOURCE MESSAGE - key=sensor1, value={'city': 'Amsterdam', 'color': 'black', 'name': 'sensor1', 'owner': 'Bob', 'timestamp': 1709749918964, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '75', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:58,972Z WARN k.f.branch_pipelines_main_branch_3_forEach UNKNOWN COLOR - black 2024-03-06T18:31:59,412Z INFO k.f.branch_pipelines_main_via_1_forEach SOURCE MESSAGE - key=sensor5, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor5', 'owner': 'Bob', 'timestamp': 1709749919409, 'type': 'LENGTH', 'unit': 'm', 'value': '658', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} We see that every message processed by the pipeline is logged through the k.f.branch_pipelines_main_via_1_forEach logger. But the branch operation sorts the messages and sends messages with colors blue and red into their own branches. The only colors that show up as UNKNOWN COLOR - messages are non-blue and non-red and send through the branch_pipelines_main_branch_3_forEach logger.","title":"Example 4. Branching messages"},{"location":"introduction/#example-5-dynamic-routing","text":"Sometimes it is necessary to route a message to one stream or another based on the content of a message. This example shows how to route messages dynamically using a TopicNameExtractor. streams: sensor_source: topic: ksml_sensordata_avro keyType: string valueType: avro:SensorData pipelines: main: from: sensor_source via: - type: peek forEach: code: log.info(\"SOURCE MESSAGE - key={}, value={}\", key, value) to: topicNameExtractor: code: | if key == 'sensor1': return 'ksml_sensordata_sensor1' if key == 'sensor2': return 'ksml_sensordata_sensor2' return 'ksml_sensordata_sensor0' The topicNameExtractor operation takes a function, which determines the routing of every message by returning a topic name string. In this case, when the key of a message is sensor1 then the message will be sent to ksml_sensordata_sensor1 . When it contains sensor2 the message is sent to ksml_sensordata_sensor2 . All other messages are sent to ksml_sensordata_sensor0 . The equivalent Kafka Streams topology looks like this: Topologies: Sub-topology: 0 Source: ksml_sensordata_avro (topics: [ksml_sensordata_avro]) --> route_route_pipelines_main_via_1 Processor: route_route_pipelines_main_via_1 (stores: []) --> route_route_ToOperationParser_001 <-- ksml_sensordata_avro Sink: route_route_ToOperationParser_001 (extractor class: io.axual.ksml.user.UserTopicNameExtractor@5d28e108) <-- route_route_pipelines_main_via_1 The output does not show anything special compared to previous examples, since all messages are simply written by the logger.","title":"Example 5. Dynamic routing"},{"location":"introduction/#example-6-multiple-pipelines","text":"In the previous examples there was always a single pipeline definition for processing data. KSML allows us to define multiple pipelines in a single file. In this example we combine the filtering example with the routing example. We will also define new pipelines with the sole purpose of logging the routed messages. # This example shows how to route messages to a dynamic topic. The target topic is the result of an executed function. streams: sensor_source_avro: topic: ksml_sensordata_avro keyType: string valueType: avro:SensorData sensor_filtered: topic: ksml_sensordata_filtered keyType: string valueType: avro:SensorData sensor_0: topic: ksml_sensordata_sensor0 keyType: string valueType: avro:SensorData sensor_1: topic: ksml_sensordata_sensor1 keyType: string valueType: avro:SensorData sensor_2: topic: ksml_sensordata_sensor2 keyType: string valueType: avro:SensorData functions: # Only pass the message to the next step in the pipeline if the color is blue sensor_is_blue: type: predicate code: | if value[\"color\"] == \"blue\": return True expression: False pipelines: filtering: from: sensor_source_avro via: - type: filter if: sensor_is_blue to: sensor_filtered routing: from: sensor_filtered via: - type: peek forEach: code: log.info(\"Routing Blue sensor - key={}, value={}\", key, value) to: topicNameExtractor: code: | if key == 'sensor1': return 'ksml_sensordata_sensor1' if key == 'sensor2': return 'ksml_sensordata_sensor2' return 'ksml_sensordata_sensor0' sensor0_peek: from: sensor_0 forEach: code: log.info(\"SENSOR0 - key={}, value={}\", key, value) sensor1_peek: from: sensor_1 forEach: code: log.info(\"SENSOR1 - key={}, value={}\", key, value) sensor2_peek: from: sensor_2 forEach: code: log.info(\"SENSOR2 - key={}, value={}\", key, value) In this definition we defined five pipelines: filtering which filters out all sensor messages that don't have the color blue and sends it to the sensor_filtered stream. routing which routes the data on the sensor_filtered stream to one of three target topics sensor0_peek which writes the content of the sensor_0 stream to the console sensor1_peek which writes the content of the sensor_1 stream to the console sensor2_peek which writes the content of the sensor_2 stream to the console The equivalent Kafka Streams topology looks like this: Topologies: Sub-topology: 0 Source: ksml_sensordata_avro (topics: [ksml_sensordata_avro]) --> multiple_multiple_pipelines_filtering_via_1 Processor: multiple_multiple_pipelines_filtering_via_1 (stores: []) --> multiple_multiple_ToOperationParser_001 <-- ksml_sensordata_avro Sink: multiple_multiple_ToOperationParser_001 (topic: ksml_sensordata_filtered) <-- multiple_multiple_pipelines_filtering_via_1 Sub-topology: 1 Source: ksml_sensordata_filtered (topics: [ksml_sensordata_filtered]) --> multiple_multiple_pipelines_routing_via_1 Processor: multiple_multiple_pipelines_routing_via_1 (stores: []) --> multiple_multiple_ToOperationParser_002 <-- ksml_sensordata_filtered Sink: multiple_multiple_ToOperationParser_002 (extractor class: io.axual.ksml.user.UserTopicNameExtractor@2700f556) <-- multiple_multiple_pipelines_routing_via_1 Sub-topology: 2 Source: ksml_sensordata_sensor0 (topics: [ksml_sensordata_sensor0]) --> multiple_multiple_pipelines_sensor0_peek Processor: multiple_multiple_pipelines_sensor0_peek (stores: []) --> none <-- ksml_sensordata_sensor0 Sub-topology: 3 Source: ksml_sensordata_sensor1 (topics: [ksml_sensordata_sensor1]) --> multiple_multiple_pipelines_sensor1_peek Processor: multiple_multiple_pipelines_sensor1_peek (stores: []) --> none <-- ksml_sensordata_sensor1 Sub-topology: 4 Source: ksml_sensordata_sensor2 (topics: [ksml_sensordata_sensor2]) --> multiple_multiple_pipelines_sensor2_peek Processor: multiple_multiple_pipelines_sensor2_peek (stores: []) --> none <-- ksml_sensordata_sensor2 And this is what the output would look something like this. The sensor peeks messages will not always be shown immediately after the Routing messages. This is because the pipelines are running in separate sub processes. 2024-03-06T20:11:39,520Z INFO k.f.route2_pipelines_routing_via_1_forEach Routing Blue sensor - key=sensor6, value={'city': 'Utrecht', 'color': 'blue', 'name': 'sensor6', 'owner': 'Charlie', 'timestamp': 1709755877401, 'type': 'LENGTH', 'unit': 'ft', 'value': '507', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,523Z INFO k.f.route2_pipelines_sensor0_peek_forEach SENSOR0 - key=sensor6, value={'city': 'Utrecht', 'color': 'blue', 'name': 'sensor6', 'owner': 'Charlie', 'timestamp': 1709755877401, 'type': 'LENGTH', 'unit': 'ft', 'value': '507', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,533Z INFO k.f.route2_pipelines_routing_via_1_forEach Routing Blue sensor - key=sensor1, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor1', 'owner': 'Evan', 'timestamp': 1709755889834, 'type': 'LENGTH', 'unit': 'm', 'value': '609', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,535Z INFO k.f.route2_pipelines_sensor1_peek_forEach SENSOR1 - key=sensor1, value={'city': 'Xanten', 'color': 'blue', 'name': 'sensor1', 'owner': 'Evan', 'timestamp': 1709755817913, 'type': 'STATE', 'unit': 'state', 'value': 'on', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,539Z INFO k.f.route2_pipelines_routing_via_1_forEach Routing Blue sensor - key=sensor7, value={'city': 'Utrecht', 'color': 'blue', 'name': 'sensor7', 'owner': 'Evan', 'timestamp': 1709755892051, 'type': 'HUMIDITY', 'unit': '%', 'value': '77', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,5419Z INFO k.f.route2_pipelines_sensor0_peek_forEach SENSOR0 - key=sensor7, value={'city': 'Utrecht', 'color': 'blue', 'name': 'sensor7', 'owner': 'Evan', 'timestamp': 1709755892051, 'type': 'HUMIDITY', 'unit': '%', 'value': '77', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,546Z INFO k.f.route2_pipelines_routing_via_1_forEach Routing Blue sensor - key=sensor2, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor2', 'owner': 'Bob', 'timestamp': 1709755893390, 'type': 'HUMIDITY', 'unit': '%', 'value': '75', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,549Z INFO k.f.route2_pipelines_sensor1_peek_forEach SENSOR2 - key=sensor2, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor2', 'owner': 'Bob', 'timestamp': 1709755893390, 'type': 'HUMIDITY', 'unit': '%', 'value': '75', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,552Z INFO k.f.route2_pipelines_routing_via_1_forEach Routing Blue sensor - key=sensor1, value={'city': 'Xanten', 'color': 'blue', 'name': 'sensor1', 'owner': 'Bob', 'timestamp': 1709755894717, 'type': 'HUMIDITY', 'unit': '%', 'value': '76', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,555Z INFO k.f.route2_pipelines_sensor1_peek_forEach SENSOR1 - key=sensor1, value={'city': 'Xanten', 'color': 'blue', 'name': 'sensor1', 'owner': 'Bob', 'timestamp': 1709755894717, 'type': 'HUMIDITY', 'unit': '%', 'value': '76', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,558Z INFO k.f.route2_pipelines_routing_via_1_forEach Routing Blue sensor - key=sensor9, value={'city': 'Alkmaar', 'color': 'blue', 'name': 'sensor9', 'owner': 'Alice', 'timestamp': 1709755896937, 'type': 'HUMIDITY', 'unit': '%', 'value': '65', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,562Z INFO k.f.route2_pipelines_sensor0_peek_forEach SENSOR0 - key=sensor9, value={'city': 'Alkmaar', 'color': 'blue', 'name': 'sensor9', 'owner': 'Alice', 'timestamp': 1709755896937, 'type': 'HUMIDITY', 'unit': '%', 'value': '65', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}}","title":"Example 6. Multiple pipelines"},{"location":"ksml-language-spec/","text":"TopologyDefinition KSML definition Properties description (string) : (optional) The description of this KSML definition. functions (object) : (optional) Functions that can be referenced in producers and pipelines. ^[a-zA-Z0-9_]+$ Any of object : Refer to #/definitions/AggregatorDefinition . object : Refer to #/definitions/ForEachActionDefinition . object : Refer to #/definitions/ForeignKeyExtractorDefinition . object : Refer to #/definitions/GeneratorDefinition . object : Refer to #/definitions/GenericFunctionDefinitionWithImplicitStoreType . object : Refer to #/definitions/InitializerDefinition . object : Refer to #/definitions/KeyTransformerDefinition . object : Refer to #/definitions/KeyValueMapperDefinition . object : Refer to #/definitions/KeyValuePrinterDefinition . object : Refer to #/definitions/KeyValueToKeyValueListTransformerDefinition . object : Refer to #/definitions/KeyValueToValueListTransformerDefinition . object : Refer to #/definitions/KeyValueTransformerDefinition . object : Refer to #/definitions/MergerDefinition . object : Refer to #/definitions/MetadataTransformerDefinition . object : Refer to #/definitions/PredicateDefinition . object : Refer to #/definitions/ReducerDefinition . object : Refer to #/definitions/StreamPartitionerDefinition . object : Refer to #/definitions/TimestampExtractorDefinition . object : Refer to #/definitions/TopicNameExtractorDefinition . object : Refer to #/definitions/ValueJoinerDefinition . object : Refer to #/definitions/ValueTransformerDefinition . globalTables (object) : (optional) GlobalTables that can be referenced in producers and pipelines. ^[a-zA-Z0-9_]+$ (object) : Refer to #/definitions/GlobalTableDefinition . name (string) : (optional) The name of this KSML definition. pipelines (object) : (optional) Collection of named pipelines. ^[a-zA-Z0-9_]+$ (object) : Refer to #/definitions/PipelineDefinition . producers (object) : (optional) Collection of named producers. ^[a-zA-Z0-9_]+$ (object) : Refer to #/definitions/ProducerDefinition . stores (object) : (optional) State stores that can be referenced in producers and pipelines. ^[a-zA-Z0-9_]+$ Any of object : Refer to #/definitions/KeyValueStateStoreDefinition . object : Refer to #/definitions/SessionStateStoreDefinition . object : Refer to #/definitions/WindowStateStoreDefinition . streams (object) : (optional) Streams that can be referenced in producers and pipelines. ^[a-zA-Z0-9_]+$ (object) : Refer to #/definitions/StreamDefinition . tables (object) : (optional) Tables that can be referenced in producers and pipelines. ^[a-zA-Z0-9_]+$ (object) : Refer to #/definitions/TableDefinition . version (string) : (optional) The version of this KSML definition. Definitions AggregateOperation (object) : An aggregate operation. Cannot contain additional properties. adder : (optional) (GroupedTable) A function that adds a record to the aggregation result. Any of string object : Refer to #/definitions/AggregatorDefinitionWithImplicitStoreType . aggregator : (optional) (GroupedStream, SessionWindowedStream, TimeWindowedStream) The aggregator function, which combines a value with the previous aggregation result and outputs a new aggregation result. Any of string object : Refer to #/definitions/AggregatorDefinitionWithImplicitStoreType . initializer : The initializer function, which generates an initial value for every set of aggregated records. Any of string object : Refer to #/definitions/InitializerDefinitionWithImplicitStoreType . merger : (optional) (SessionWindowedStream, SessionWindowedCogroupedStream) A function that combines two aggregation results. Any of string object : Refer to #/definitions/MergerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the result aggregation. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinition . object : Refer to #/definitions/SessionStateStoreDefinition . object : Refer to #/definitions/WindowStateStoreDefinition . subtractor : (optional) (GroupedTable) A function that removes a record from the aggregation result. Any of string object : Refer to #/definitions/AggregatorDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"aggregate\"] . AggregatorDefinition (object) : Defines a aggregator function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the aggregator. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the aggregator. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the aggregator. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the aggregator. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the aggregator. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the aggregator. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"aggregator\"] . AggregatorDefinitionWithImplicitStoreType (object) : Defines a aggregator function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the aggregator. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the aggregator. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the aggregator. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the aggregator. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the aggregator. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the aggregator. Only required for function types, which are not pre-defined. BranchDefinitionWithPipeline (object) : Defines a branch with sub-pipeline in a BranchOperation. Cannot contain additional properties. as (string) : (optional) The name to register the pipeline result under, which can be used as source by follow-up pipelines. branch (array) : (optional) Defines a single branch, consisting of a condition and a pipeline to execute for messages that fulfil the predicate. Items (object) : Refer to #/definitions/StringOrInlinePredicateDefinitionWithImplicitStoreType . forEach : (optional) A function that gets called for every message in the stream. Any of string object : Refer to #/definitions/ForEachActionDefinitionWithImplicitStoreType . if : (optional) Defines the condition under which messages get sent down this branch. Any of string object : Refer to #/definitions/PredicateDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. print (object) : (optional) The specification of where to print messages to. Refer to #/definitions/PrintOperation . to : (optional) Ends the pipeline by sending all messages to a stream, table or globalTable, or to an inline defined output topic and optional partitioner. Any of string object : Refer to #/definitions/ToTopicDefinition . toTopicNameExtractor : (optional) Ends the pipeline by sending all messages to a topic provided by a pre-defined topic name extractor function, or to a topic provided by an inline defined topic name extractor and optional partitioner. Any of string object : Refer to #/definitions/ToTopicNameExtractorDefinition . via (array) : (optional) A series of operations performed on the input stream. Items Any of object : Refer to #/definitions/AggregateOperation . object : Refer to #/definitions/CogroupOperation . object : Refer to #/definitions/ConvertKeyOperation . object : Refer to #/definitions/ConvertKeyValueOperation . object : Refer to #/definitions/ConvertValueOperation . object : Refer to #/definitions/CountOperation . object : Refer to #/definitions/FilterNotOperation . object : Refer to #/definitions/FilterOperation . object : Refer to #/definitions/GroupByKeyOperation . object : Refer to #/definitions/GroupByOperation . object : Refer to #/definitions/JoinWithGlobalTableOperation . object : Refer to #/definitions/JoinWithStreamOperation . object : Refer to #/definitions/JoinWithTableOperation . object : Refer to #/definitions/LeftJoinWithGlobalTableOperation . object : Refer to #/definitions/LeftJoinWithStreamOperation . object : Refer to #/definitions/LeftJoinWithTableOperation . object : Refer to #/definitions/MergeOperation . object : Refer to #/definitions/OuterJoinWithStreamOperation . object : Refer to #/definitions/OuterJoinWithTableOperation . object : Refer to #/definitions/PeekOperation . object : Refer to #/definitions/ReduceOperationWithAdderAndSubtractor . object : Refer to #/definitions/ReduceOperationWithReducer . object : Refer to #/definitions/RepartitionOperation . object : Refer to #/definitions/SuppressOperationUntilTimeLimit . object : Refer to #/definitions/SuppressOperationUntilWindowCloses . object : Refer to #/definitions/ToStreamOperation . object : Refer to #/definitions/ToTableOperation . object : Refer to #/definitions/TransformKeyOperation . object : Refer to #/definitions/TransformKeyValueOperation . object : Refer to #/definitions/TransformKeyValueToKeyValueListOperation . object : Refer to #/definitions/TransformKeyValueToValueListOperation . object : Refer to #/definitions/TransformMetadataOperation . object : Refer to #/definitions/TransformValueOperation . object : Refer to #/definitions/WindowBySessionOperation . object : Refer to #/definitions/WindowByTimeOperationWithHoppingWindow . object : Refer to #/definitions/WindowByTimeOperationWithSlidingWindow . object : Refer to #/definitions/WindowByTimeOperationWithTumblingWindow . CogroupOperation (object) : A cogroup operation. Cannot contain additional properties. aggregator : (GroupedStream, SessionWindowedStream, TimeWindowedStream) The aggregator function, which combines a value with the previous aggregation result and outputs a new aggregation result. Any of string object : Refer to #/definitions/AggregatorDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"cogroup\"] . ConvertKeyOperation (object) : An operation to convert the stream key type to another type. Conversion is only syntactic, eg. from Avro to XML. Cannot contain additional properties. into (string, required) : The type to convert the stream key into. name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"convertKey\"] . ConvertKeyValueOperation (object) : An operation to convert the stream key and value types to other types. Conversion is only syntactic, eg. from Avro to XML. Cannot contain additional properties. into (string, required) : The tuple type to convert the stream key/value into. name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"convertKeyValue\"] . ConvertValueOperation (object) : An operation to convert the stream value type to another type. Conversion is only syntactic, eg. from Avro to XML. Cannot contain additional properties. into (string, required) : The type to convert the stream value into. name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"convertValue\"] . CountOperation (object) : Count the number of times a key is seen in a given window. Cannot contain additional properties. name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the count operation's result. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinition . object : Refer to #/definitions/SessionStateStoreDefinition . object : Refer to #/definitions/WindowStateStoreDefinition . type : The type of the operation. Must be one of: [\"count\"] . FilterNotOperation (object) : Filter records based on the inverse result of a predicate function. Cannot contain additional properties. if : A function that returns \"false\" when records are accepted, \"true\" otherwise. Any of string object : Refer to #/definitions/PredicateDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the filtered table (only applies to tables, ignored for streams). Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"filterNot\"] . FilterOperation (object) : Filter records based on a predicate function. Cannot contain additional properties. if : A function that returns \"true\" when records are accepted, \"false\" otherwise. Any of string object : Refer to #/definitions/PredicateDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the filtered table (only applies to tables, ignored for streams). Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"filter\"] . ForEachActionDefinition (object) : Defines a foreach action function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the foreach action. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the foreach action. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the foreach action. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the foreach action. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the foreach action. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the foreach action. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the foreach action uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"forEach\"] . ForEachActionDefinitionWithImplicitStoreType (object) : Defines a foreach action function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the foreach action. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the foreach action. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the foreach action. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the foreach action. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the foreach action. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the foreach action. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the foreach action uses. Only required if the function wants to use a state store. Items (string) ForeignKeyExtractorDefinition (object) : Defines a foreign key extractor function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the foreign key extractor. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the foreign key extractor. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the foreign key extractor. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the foreign key extractor. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the foreign key extractor. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the foreign key extractor. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"foreignKeyExtractor\"] . ForeignKeyExtractorDefinitionWithImplicitStoreType (object) : Defines a foreign key extractor function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the foreign key extractor. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the foreign key extractor. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the foreign key extractor. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the foreign key extractor. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the foreign key extractor. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the foreign key extractor. Only required for function types, which are not pre-defined. GeneratorDefinition (object) : Defines a message generator function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the message generator. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the message generator. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the message generator. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the message generator. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the message generator. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the message generator. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"generator\"] . GeneratorDefinitionWithImplicitStoreType (object) : Defines a message generator function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the message generator. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the message generator. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the message generator. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the message generator. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the message generator. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the message generator. Only required for function types, which are not pre-defined. GenericFunctionDefinitionWithImplicitStoreType (object) : Defines a generic function function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the generic function. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the generic function. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the generic function. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the generic function. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the generic function. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the generic function. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"generic\"] . GlobalTableDefinition (object) : Contains a definition of a globalTable, which can be referenced by producers and pipelines. Cannot contain additional properties. keyType (string, required) : The key type of the globalTable. offsetResetPolicy (string) : (optional) The policy that determines what to do when there is no initial consumer offset in Kafka, or if the message at the committed consumer offset does not exist (e.g. because that data has been deleted). partitioner : (optional) A function that determines to which topic partition a given message needs to be written. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . store : (optional) KeyValue state store definition. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType . timestampExtractor : (optional) A function that extracts the event time from a consumed record. Any of string object : Refer to #/definitions/TimestampExtractorDefinitionWithImplicitStoreType . topic (string, required) : The name of the Kafka topic for this globalTable. valueType (string, required) : The value type of the globalTable. GlobalTableDefinitionAsJoinTarget (object) : Reference to a globalTable in a join operation. Cannot contain additional properties. keyType (string) : (optional) The key type of the globalTable. partitioner : (optional) A function that determines to which topic partition a given message needs to be written. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . store : (optional) KeyValue state store definition. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType . topic (string, required) : The name of the Kafka topic for this globalTable. valueType (string) : (optional) The value type of the globalTable. GroupByKeyOperation (object) : Operation to group all messages with the same key together. Cannot contain additional properties. name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the grouped stream. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"groupByKey\"] . GroupByOperation (object) : Operation to group all messages with together based on a keying function. Cannot contain additional properties. mapper : Function to map records to a key they can be grouped on. Any of string object : Refer to #/definitions/KeyValueMapperDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the grouped stream or table. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"groupBy\"] . InitializerDefinition (object) : Defines a initializer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the initializer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the initializer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the initializer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the initializer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the initializer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the initializer. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"initializer\"] . InitializerDefinitionWithImplicitStoreType (object) : Defines a initializer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the initializer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the initializer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the initializer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the initializer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the initializer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the initializer. Only required for function types, which are not pre-defined. JoinWithGlobalTableOperation (object) : Operation to join with a table. Cannot contain additional properties. globalTable : A reference to the globalTable, or an inline definition of the globalTable to join with. Any of string object : Refer to #/definitions/GlobalTableDefinitionAsJoinTarget . mapper : A function that maps the key value from the stream to the primary key type of the globalTable. Any of string object : Refer to #/definitions/KeyValueMapperDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"join\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . JoinWithStreamOperation (object) : Operation to join with a stream. Cannot contain additional properties. grace : (optional) The window grace period (the time to admit out-of-order events after the end of the window). Any of integer string name (string) : (optional) The name of the operation processor. otherStore : Materialized view of the joined stream. Any of string object : Refer to #/definitions/WindowStateStoreDefinitionWithImplicitStoreType . stream : A reference to the stream, or an inline definition of the stream to join with. Any of string object : Refer to #/definitions/StreamDefinitionAsJoinTarget . thisStore : Materialized view of the source stream. Any of string object : Refer to #/definitions/WindowStateStoreDefinitionWithImplicitStoreType . timeDifference : The maximum time difference for a join over two streams on the same key. Any of integer string type : The type of the operation. Must be one of: [\"join\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . JoinWithTableOperation (object) : Operation to join with a table. Cannot contain additional properties. foreignKeyExtractor : (optional) A function that can translate the join table value to a primary key. Any of string object : Refer to #/definitions/ForeignKeyExtractorDefinitionWithImplicitStoreType . grace : (optional) The window grace period (the time to admit out-of-order events after the end of the window). Any of integer string name (string) : (optional) The name of the operation processor. otherPartitioner : (optional) A function that partitions the records on the join table. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . partitioner : (optional) A function that partitions the records on the primary table. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . store : (optional) Materialized view of the joined table (only used for Table-Table joins). Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . table : A reference to the table, or an inline definition of the table to join with. Any of string object : Refer to #/definitions/TableDefinitionAsJoinTarget . type : The type of the operation. Must be one of: [\"join\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . KeyTransformerDefinition (object) : Defines a key transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the key transformer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the key transformer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the key transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the key transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the key transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the key transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the key transformer uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"keyTransformer\"] . KeyTransformerDefinitionWithImplicitStoreType (object) : Defines a key transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the key transformer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the key transformer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the key transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the key transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the key transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the key transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the key transformer uses. Only required if the function wants to use a state store. Items (string) KeyValueMapperDefinition (object) : Defines a keyvalue mapper function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue mapper. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the keyvalue mapper. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue mapper. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue mapper. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue mapper. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue mapper. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"keyValueMapper\"] . KeyValueMapperDefinitionWithImplicitStoreType (object) : Defines a keyvalue mapper function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue mapper. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the keyvalue mapper. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue mapper. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue mapper. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue mapper. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue mapper. Only required for function types, which are not pre-defined. KeyValuePrinterDefinition (object) : Defines a keyvalue printer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue printer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the keyvalue printer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue printer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue printer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue printer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue printer. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"keyValuePrinter\"] . KeyValuePrinterDefinitionWithImplicitStoreType (object) : Defines a keyvalue printer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue printer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the keyvalue printer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue printer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue printer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue printer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue printer. Only required for function types, which are not pre-defined. KeyValueStateStoreDefinition (object) : Definition of a keyValue state store. Cannot contain additional properties. caching (boolean) : (optional) \"true\" if changed to the keyValue store need to be buffered and periodically released, \"false\" to emit all changes directly. historyRetention : (optional) (Versioned only) The duration for which old record versions are available for query (cannot be negative). Any of integer string keyType (string) : (optional) The key type of the keyValue store. logging (boolean) : (optional) \"true\" if a changelog topic should be set up on Kafka for this keyValue store, \"false\" otherwise. name (string) : (optional) The name of the keyValue store. If this field is not defined, then the name is derived from the context. persistent (boolean) : (optional) \"true\" if this keyValue store needs to be stored on disk, \"false\" otherwise. segmentInterval : (optional) Size of segments for storing old record versions (must be positive). Old record versions for the same key in a single segment are stored (updated and accessed) together. The only impact of this parameter is performance. If segments are large and a workload results in many record versions for the same key being collected in a single segment, performance may degrade as a result. On the other hand, historical reads (which access older segments) and out-of-order writes may slow down if there are too many segments. Any of integer string timestamped (boolean) : (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise. type : The type of the state store. Must be one of: [\"keyValue\"] . valueType (string) : (optional) The value type of the keyValue store. versioned (boolean) : (optional) \"true\" if elements in the store are versioned, \"false\" otherwise. KeyValueStateStoreDefinitionWithImplicitStoreType (object) : Definition of a keyValue state store. Cannot contain additional properties. caching (boolean) : (optional) \"true\" if changed to the keyValue store need to be buffered and periodically released, \"false\" to emit all changes directly. historyRetention : (optional) (Versioned only) The duration for which old record versions are available for query (cannot be negative). Any of integer string keyType (string) : (optional) The key type of the keyValue store. logging (boolean) : (optional) \"true\" if a changelog topic should be set up on Kafka for this keyValue store, \"false\" otherwise. name (string) : (optional) The name of the keyValue store. If this field is not defined, then the name is derived from the context. persistent (boolean) : (optional) \"true\" if this keyValue store needs to be stored on disk, \"false\" otherwise. segmentInterval : (optional) Size of segments for storing old record versions (must be positive). Old record versions for the same key in a single segment are stored (updated and accessed) together. The only impact of this parameter is performance. If segments are large and a workload results in many record versions for the same key being collected in a single segment, performance may degrade as a result. On the other hand, historical reads (which access older segments) and out-of-order writes may slow down if there are too many segments. Any of integer string timestamped (boolean) : (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise. type : The type of the state store. Must be one of: [\"keyValue\"] . valueType (string) : (optional) The value type of the keyValue store. versioned (boolean) : (optional) \"true\" if elements in the store are versioned, \"false\" otherwise. KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType (object) : Definition of a keyValue state store. Cannot contain additional properties. caching (boolean) : (optional) \"true\" if changed to the keyValue store need to be buffered and periodically released, \"false\" to emit all changes directly. historyRetention : (optional) (Versioned only) The duration for which old record versions are available for query (cannot be negative). Any of integer string logging (boolean) : (optional) \"true\" if a changelog topic should be set up on Kafka for this keyValue store, \"false\" otherwise. persistent (boolean) : (optional) \"true\" if this keyValue store needs to be stored on disk, \"false\" otherwise. segmentInterval : (optional) Size of segments for storing old record versions (must be positive). Old record versions for the same key in a single segment are stored (updated and accessed) together. The only impact of this parameter is performance. If segments are large and a workload results in many record versions for the same key being collected in a single segment, performance may degrade as a result. On the other hand, historical reads (which access older segments) and out-of-order writes may slow down if there are too many segments. Any of integer string timestamped (boolean) : (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise. type : The type of the state store. Must be one of: [\"keyValue\"] . versioned (boolean) : (optional) \"true\" if elements in the store are versioned, \"false\" otherwise. KeyValueToKeyValueListTransformerDefinition (object) : Defines a keyvalue-to-keyvaluelist transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue-to-keyvaluelist transformer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the keyvalue-to-keyvaluelist transformer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue-to-keyvaluelist transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue-to-keyvaluelist transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue-to-keyvaluelist transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue-to-keyvaluelist transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the keyvalue-to-keyvaluelist transformer uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"keyValueToKeyValueListTransformer\"] . KeyValueToKeyValueListTransformerDefinitionWithImplicitStoreType (object) : Defines a keyvalue-to-keyvaluelist transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue-to-keyvaluelist transformer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the keyvalue-to-keyvaluelist transformer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue-to-keyvaluelist transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue-to-keyvaluelist transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue-to-keyvaluelist transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue-to-keyvaluelist transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the keyvalue-to-keyvaluelist transformer uses. Only required if the function wants to use a state store. Items (string) KeyValueToValueListTransformerDefinition (object) : Defines a keyvalue-to-valuelist transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue-to-valuelist transformer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the keyvalue-to-valuelist transformer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue-to-valuelist transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue-to-valuelist transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue-to-valuelist transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue-to-valuelist transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the keyvalue-to-valuelist transformer uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"keyValueToValueListTransformer\"] . KeyValueToValueListTransformerDefinitionWithImplicitStoreType (object) : Defines a keyvalue-to-valuelist transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue-to-valuelist transformer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the keyvalue-to-valuelist transformer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue-to-valuelist transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue-to-valuelist transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue-to-valuelist transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue-to-valuelist transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the keyvalue-to-valuelist transformer uses. Only required if the function wants to use a state store. Items (string) KeyValueTransformerDefinition (object) : Defines a keyvalue transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue transformer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the keyvalue transformer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the keyvalue transformer uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"keyValueTransformer\"] . KeyValueTransformerDefinitionWithImplicitStoreType (object) : Defines a keyvalue transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue transformer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the keyvalue transformer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the keyvalue transformer uses. Only required if the function wants to use a state store. Items (string) LeftJoinWithGlobalTableOperation (object) : Operation to leftJoin with a globalTable. Cannot contain additional properties. globalTable : A reference to the globalTable, or an inline definition of the globalTable to join with. Any of string object : Refer to #/definitions/GlobalTableDefinitionAsJoinTarget . mapper : A function that maps the key value from the stream with the primary key of the globalTable. Any of string object : Refer to #/definitions/KeyValueMapperDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"leftJoin\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . LeftJoinWithStreamOperation (object) : Operation to leftJoin with a stream. Cannot contain additional properties. grace : (optional) The window grace period (the time to admit out-of-order events after the end of the window). Any of integer string name (string) : (optional) The name of the operation processor. otherStore : Materialized view of the leftJoined stream. Any of string object : Refer to #/definitions/WindowStateStoreDefinitionWithImplicitStoreType . stream : A reference to the stream, or an inline definition of the stream to leftJoin with. Any of string object : Refer to #/definitions/StreamDefinitionAsJoinTarget . thisStore : Materialized view of the source stream. Any of string object : Refer to #/definitions/WindowStateStoreDefinitionWithImplicitStoreType . timeDifference : The maximum time difference for a leftJoin over two streams on the same key. Any of integer string type : The type of the operation. Must be one of: [\"leftJoin\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . LeftJoinWithTableOperation (object) : Operation to leftJoin with a table. Cannot contain additional properties. foreignKeyExtractor : (optional) A function that can translate the join table value to a primary key. Any of string object : Refer to #/definitions/ForeignKeyExtractorDefinitionWithImplicitStoreType . grace : (optional) The window grace period (the time to admit out-of-order events after the end of the window). Any of integer string name (string) : (optional) The name of the operation processor. otherPartitioner : (optional) A function that partitions the records on the join table. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . partitioner : (optional) A function that partitions the records on the primary table. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . store : (optional) Materialized view of the leftJoined table (only used for Table-Table joins). Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . table : A reference to the table, or an inline definition of the table to join with. Any of string object : Refer to #/definitions/TableDefinitionAsJoinTarget . type : The type of the operation. Must be one of: [\"leftJoin\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . MergeOperation (object) : A merge operation to join two Streams. Cannot contain additional properties. name (string) : (optional) The name of the operation processor. stream : The stream to merge with. Any of string object : Refer to #/definitions/StreamDefinition . type : The type of the operation. Must be one of: [\"merge\"] . MergerDefinition (object) : Defines a merger function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the merger. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the merger. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the merger. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the merger. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the merger. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the merger. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"merger\"] . MergerDefinitionWithImplicitStoreType (object) : Defines a merger function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the merger. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the merger. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the merger. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the merger. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the merger. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the merger. Only required for function types, which are not pre-defined. MetadataTransformerDefinition (object) : Defines a metadata transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the metadata transformer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the metadata transformer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the metadata transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the metadata transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the metadata transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the metadata transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the metadata transformer uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"metadataTransformer\"] . MetadataTransformerDefinitionWithImplicitStoreType (object) : Defines a metadata transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the metadata transformer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the metadata transformer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the metadata transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the metadata transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the metadata transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the metadata transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the metadata transformer uses. Only required if the function wants to use a state store. Items (string) OuterJoinWithStreamOperation (object) : Operation to outerJoin with a stream. Cannot contain additional properties. grace : (optional) The window grace period (the time to admit out-of-order events after the end of the window). Any of integer string name (string) : (optional) The name of the operation processor. otherStore : Materialized view of the outerJoined stream. Any of string object : Refer to #/definitions/WindowStateStoreDefinitionWithImplicitStoreType . stream : A reference to the stream, or an inline definition of the stream to outerJoin with. Any of string object : Refer to #/definitions/StreamDefinitionAsJoinTarget . thisStore : Materialized view of the source stream. Any of string object : Refer to #/definitions/WindowStateStoreDefinitionWithImplicitStoreType . timeDifference : The maximum time difference for an outerJoin over two streams on the same key. Any of integer string type : The type of the operation. Must be one of: [\"outerJoin\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . OuterJoinWithTableOperation (object) : Operation to outerJoin with a table. Cannot contain additional properties. name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the outerJoined table. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . table : A reference to the table, or an inline definition of the table to outerJoin with. Any of string object : Refer to #/definitions/TableDefinitionAsJoinTarget . type : The type of the operation. Must be one of: [\"outerJoin\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . ParameterDefinition (object) : Defines a parameter for a user function. Cannot contain additional properties. defaultValue (string) : (optional) The default value for the parameter. name (string, required) : The name of the parameter. type (string, required) : The type of the parameter. PeekOperation (object) : Operation to peek into a stream, without modifying the stream contents. Cannot contain additional properties. forEach : A function that gets called for every message in the stream. Any of string object : Refer to #/definitions/ForEachActionDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"peek\"] . PipelineDefinition (object) : Defines a pipeline through a source, a series of operations to perform on it and a sink operation to close the stream with. Cannot contain additional properties. as (string) : (optional) The name to register the pipeline result under, which can be used as source by follow-up pipelines. branch (array) : (optional) Defines a single branch, consisting of a condition and a pipeline to execute for messages that fulfil the predicate. Items (object) : Refer to #/definitions/BranchDefinitionWithPipeline . forEach : (optional) A function that gets called for every message in the stream. Any of string object : Refer to #/definitions/ForEachActionDefinitionWithImplicitStoreType . from : Pipeline source. Any of string object : Refer to #/definitions/TopicDefinitionSource . name (string) : (optional) The name of the operation processor. print (object) : (optional) The specification of where to print messages to. Refer to #/definitions/PrintOperation . to : (optional) Ends the pipeline by sending all messages to a stream, table or globalTable, or to an inline defined output topic and optional partitioner. Any of string object : Refer to #/definitions/ToTopicDefinition . toTopicNameExtractor : (optional) Ends the pipeline by sending all messages to a topic provided by a pre-defined topic name extractor function, or to a topic provided by an inline defined topic name extractor and optional partitioner. Any of string object : Refer to #/definitions/ToTopicNameExtractorDefinition . via (array) : (optional) A series of operations performed on the input stream. Items Any of object : Refer to #/definitions/AggregateOperation . object : Refer to #/definitions/CogroupOperation . object : Refer to #/definitions/ConvertKeyOperation . object : Refer to #/definitions/ConvertKeyValueOperation . object : Refer to #/definitions/ConvertValueOperation . object : Refer to #/definitions/CountOperation . object : Refer to #/definitions/FilterNotOperation . object : Refer to #/definitions/FilterOperation . object : Refer to #/definitions/GroupByKeyOperation . object : Refer to #/definitions/GroupByOperation . object : Refer to #/definitions/JoinWithGlobalTableOperation . object : Refer to #/definitions/JoinWithStreamOperation . object : Refer to #/definitions/JoinWithTableOperation . object : Refer to #/definitions/LeftJoinWithGlobalTableOperation . object : Refer to #/definitions/LeftJoinWithStreamOperation . object : Refer to #/definitions/LeftJoinWithTableOperation . object : Refer to #/definitions/MergeOperation . object : Refer to #/definitions/OuterJoinWithStreamOperation . object : Refer to #/definitions/OuterJoinWithTableOperation . object : Refer to #/definitions/PeekOperation . object : Refer to #/definitions/ReduceOperationWithAdderAndSubtractor . object : Refer to #/definitions/ReduceOperationWithReducer . object : Refer to #/definitions/RepartitionOperation . object : Refer to #/definitions/SuppressOperationUntilTimeLimit . object : Refer to #/definitions/SuppressOperationUntilWindowCloses . object : Refer to #/definitions/ToStreamOperation . object : Refer to #/definitions/ToTableOperation . object : Refer to #/definitions/TransformKeyOperation . object : Refer to #/definitions/TransformKeyValueOperation . object : Refer to #/definitions/TransformKeyValueToKeyValueListOperation . object : Refer to #/definitions/TransformKeyValueToValueListOperation . object : Refer to #/definitions/TransformMetadataOperation . object : Refer to #/definitions/TransformValueOperation . object : Refer to #/definitions/WindowBySessionOperation . object : Refer to #/definitions/WindowByTimeOperationWithHoppingWindow . object : Refer to #/definitions/WindowByTimeOperationWithSlidingWindow . object : Refer to #/definitions/WindowByTimeOperationWithTumblingWindow . PredicateDefinition (object) : Defines a predicate function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the predicate. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the predicate. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the predicate. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the predicate. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the predicate. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the predicate. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the predicate uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"predicate\"] . PredicateDefinitionWithImplicitStoreType (object) : Defines a predicate function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the predicate. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the predicate. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the predicate. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the predicate. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the predicate. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the predicate. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the predicate uses. Only required if the function wants to use a state store. Items (string) PrintOperation (object) : Operation to print the contents of a pipeline on the screen or to write them to a file. Cannot contain additional properties. filename (string) : (optional) The filename to output records to. If nothing is specified, then messages will be printed on stdout. label (string) : (optional) A label to attach to the output records. mapper : (optional) A function to convert record into a string for output. Any of string object : Refer to #/definitions/KeyValuePrinterDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. ProducerDefinition (object) : Definition of a Producer that regularly generates messages for a topic. Cannot contain additional properties. batchSize (integer) : (optional) The size of batches. condition : (optional) A function that validates the generator's result message. Returns \"true\" when the message may be produced on the topic, \"false\" otherwise. Any of string object : Refer to #/definitions/PredicateDefinitionWithImplicitStoreType . count (integer) : (optional) The number of messages to produce. generator : The function that generates records. Any of string object : Refer to #/definitions/GeneratorDefinitionWithImplicitStoreType . interval : (optional) The interval with which the generator is called. Any of integer string to : The topic to produce to. Any of string object : Refer to #/definitions/TopicDefinition . until : (optional) A predicate that returns true to indicate producing should stop. Any of string object : Refer to #/definitions/PredicateDefinitionWithImplicitStoreType . ReduceOperationWithAdderAndSubtractor (object) : Operation to reduce a series of records into a single aggregate result. Cannot contain additional properties. adder : A function that adds a record to the aggregate result. Any of string object : Refer to #/definitions/ReducerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the aggregation. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . subtractor : A function that removes a record from the aggregate result. Any of string object : Refer to #/definitions/ReducerDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"reduce\"] . ReduceOperationWithReducer (object) : Operation to reduce a series of records into a single aggregate result. Cannot contain additional properties. name (string) : (optional) The name of the operation processor. reducer : A function that computes a new aggregate result. Any of string object : Refer to #/definitions/ReducerDefinitionWithImplicitStoreType . store : (optional) Materialized view of the aggregation. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinition . object : Refer to #/definitions/SessionStateStoreDefinition . object : Refer to #/definitions/WindowStateStoreDefinition . type : The type of the operation. Must be one of: [\"reduce\"] . ReducerDefinition (object) : Defines a reducer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the reducer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the reducer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the reducer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the reducer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the reducer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the reducer. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"reducer\"] . ReducerDefinitionWithImplicitStoreType (object) : Defines a reducer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the reducer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the reducer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the reducer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the reducer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the reducer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the reducer. Only required for function types, which are not pre-defined. RepartitionOperation (object) : Operation to (re)partition a stream. Cannot contain additional properties. name (string) : (optional) The name of the operation processor. numberOfPartitions (integer) : (optional) The target number of partitions. partitioner : (optional) A function that partitions stream records. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"repartition\"] . SessionStateStoreDefinition (object) : Definition of a session state store. Cannot contain additional properties. caching (boolean) : (optional) \"true\" if changed to the session store need to be buffered and periodically released, \"false\" to emit all changes directly. keyType (string) : (optional) The key type of the session store. logging (boolean) : (optional) \"true\" if a changelog topic should be set up on Kafka for this session store, \"false\" otherwise. name (string) : (optional) The name of the session store. If this field is not defined, then the name is derived from the context. persistent (boolean) : (optional) \"true\" if this session store needs to be stored on disk, \"false\" otherwise. retention : (optional) The duration for which elements in the session store are retained. Any of integer string timestamped (boolean) : (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise. type : The type of the state store. Must be one of: [\"session\"] . valueType (string) : (optional) The value type of the session store. StreamDefinition (object) : Contains a definition of a Stream, which can be referenced by producers and pipelines. Cannot contain additional properties. keyType (string, required) : The key type of the stream. offsetResetPolicy (string) : (optional) Policy that determines what to do when there is no initial offset in Kafka, or if the current offset does not exist any more on the server (e.g. because that data has been deleted). partitioner : (optional) A function that determines to which topic partition a given message needs to be written. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . timestampExtractor : (optional) A function extracts the event time from a consumed record. Any of string object : Refer to #/definitions/TimestampExtractorDefinitionWithImplicitStoreType . topic (string, required) : The name of the Kafka topic for this stream. valueType (string, required) : The value type of the stream. StreamDefinitionAsJoinTarget (object) : Reference to a Stream in a join or merge operation. Cannot contain additional properties. keyType (string) : (optional) The key type of the stream. topic (string, required) : The name of the Kafka topic for this stream. valueType (string) : (optional) The value type of the stream. StreamPartitionerDefinition (object) : Defines a stream partitioner function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the stream partitioner. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the stream partitioner. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the stream partitioner. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the stream partitioner. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the stream partitioner. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the stream partitioner. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"streamPartitioner\"] . StreamPartitionerDefinitionWithImplicitStoreType (object) : Defines a stream partitioner function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the stream partitioner. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the stream partitioner. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the stream partitioner. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the stream partitioner. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the stream partitioner. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the stream partitioner. Only required for function types, which are not pre-defined. StringOrInlinePredicateDefinitionWithImplicitStoreType (object) : Defines the condition under which messages get sent down this branch. Cannot contain additional properties. if : (optional) Defines the condition under which messages get sent down this branch. Any of string object : Refer to #/definitions/PredicateDefinitionWithImplicitStoreType . SuppressOperationUntilTimeLimit (object) : Operation to suppress messages in the source stream until a time limit is reached. Cannot contain additional properties. bufferFullStrategy : (optional) What to do when the buffer is full. Must be one of: [\"emitEarlyWhenFull\", \"shutdownWhenFull\"] . duration : The duration for which messages are suppressed. Any of integer string maxBytes (string) : (optional) The maximum number of bytes in the buffer. maxRecords (string) : (optional) The maximum number of records in the buffer. name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"suppress\"] . until : The until of the Operation to suppress messages in the source stream until a certain limit is reached. Must be one of: [\"timeLimit\"] . SuppressOperationUntilWindowCloses (object) : Operation to suppress messages in the source stream until a window limit is reached. Cannot contain additional properties. bufferFullStrategy : (optional) What to do when the buffer is full. Must be one of: [\"emitEarlyWhenFull\", \"shutdownWhenFull\"] . maxBytes (string) : (optional) The maximum number of bytes in the buffer. maxRecords (string) : (optional) The maximum number of records in the buffer. name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"suppress\"] . until : The until of the Operation to suppress messages in the source stream until a certain limit is reached. Must be one of: [\"windowCloses\"] . TableDefinition (object) : Contains a definition of a table, which can be referenced by producers and pipelines. Cannot contain additional properties. keyType (string, required) : The key type of the table. offsetResetPolicy (string) : (optional) The policy that determines what to do when there is no initial consumer offset in Kafka, or if the message at the committed consumer offset does not exist (e.g. because that data has been deleted). partitioner : (optional) A function that determines to which topic partition a given message needs to be written. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . store : (optional) KeyValue state store definition. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType . timestampExtractor : (optional) A function that extracts the event time from a consumed record. Any of string object : Refer to #/definitions/TimestampExtractorDefinitionWithImplicitStoreType . topic (string, required) : The name of the Kafka topic for this table. valueType (string, required) : The value type of the table. TableDefinitionAsJoinTarget (object) : Reference to a table in a join operation. Cannot contain additional properties. keyType (string) : (optional) The key type of the table. partitioner : (optional) A function that determines to which topic partition a given message needs to be written. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . store : (optional) KeyValue state store definition. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType . topic (string, required) : The name of the Kafka topic for this table. valueType (string) : (optional) The value type of the table. TimestampExtractorDefinition (object) : Defines a timestamp extractor function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the timestamp extractor. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the timestamp extractor. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the timestamp extractor. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the timestamp extractor. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the timestamp extractor. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the timestamp extractor. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"timestampExtractor\"] . TimestampExtractorDefinitionWithImplicitStoreType (object) : Defines a timestamp extractor function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the timestamp extractor. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the timestamp extractor. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the timestamp extractor. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the timestamp extractor. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the timestamp extractor. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the timestamp extractor. Only required for function types, which are not pre-defined. ToStreamOperation (object) : Convert a Table into a Stream, optionally through a custom key transformer. Cannot contain additional properties. mapper : (optional) A function that computes the output key for every record. Any of string object : Refer to #/definitions/KeyTransformerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"toStream\"] . ToTableOperation (object) : Convert a Stream into a Table. Cannot contain additional properties. name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the result table. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"toTable\"] . ToTopicDefinition (object) : Writes out pipeline messages to a topic. Cannot contain additional properties. keyType (string) : (optional) The key type of the topic. partitioner : (optional) A function that partitions the records in the output topic. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . topic (string, required) : The name of the Kafka topic. valueType (string) : (optional) The value type of the topic. ToTopicNameExtractorDefinition (object) : Writes out pipeline messages to a topic as given by a topic name extractor. Cannot contain additional properties. partitioner : (optional) A function that partitions the records in the output topic. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . topicNameExtractor : Reference to a pre-defined topic name extractor, or an inline definition of a topic name extractor. Any of string object : Refer to #/definitions/TopicNameExtractorDefinitionWithImplicitStoreType . TopicDefinition (object) : Contains a definition of a Kafka topic, to be used by producers and pipelines. Cannot contain additional properties. keyType (string) : (optional) The key type of the topic. topic (string, required) : The name of the Kafka topic. valueType (string) : (optional) The value type of the topic. TopicDefinitionSource (object) : Contains a definition of a Kafka topic, to be used by producers and pipelines. Cannot contain additional properties. keyType (string, required) : The key type of the topic. offsetResetPolicy (string) : (optional) Policy that determines what to do when there is no initial offset in Kafka, or if the current offset does not exist any more on the server (e.g. because that data has been deleted). partitioner : (optional) A function that determines to which topic partition a given message needs to be written. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . timestampExtractor : (optional) A function extracts the event time from a consumed record. Any of string object : Refer to #/definitions/TimestampExtractorDefinitionWithImplicitStoreType . topic (string, required) : The name of the Kafka topic. valueType (string, required) : The value type of the topic. TopicNameExtractorDefinition (object) : Defines a topic name extractor function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the topic name extractor. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the topic name extractor. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the topic name extractor. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the topic name extractor. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the topic name extractor. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the topic name extractor. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"topicNameExtractor\"] . TopicNameExtractorDefinitionWithImplicitStoreType (object) : Defines a topic name extractor function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the topic name extractor. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the topic name extractor. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the topic name extractor. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the topic name extractor. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the topic name extractor. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the topic name extractor. Only required for function types, which are not pre-defined. TransformKeyOperation (object) : Convert the key of every record in the stream to another key. Cannot contain additional properties. mapper : A function that computes a new key for each record. Any of string object : Refer to #/definitions/KeyTransformerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"transformKey\", \"mapKey\", \"selectKey\"] . TransformKeyValueOperation (object) : Convert the key/value of every record in the stream to another key/value. Cannot contain additional properties. mapper : A function that computes a new key/value for each record. Any of string object : Refer to #/definitions/KeyValueTransformerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"map\", \"transformKeyValue\"] . TransformKeyValueToKeyValueListOperation (object) : Convert a stream by transforming every record into a list of derived records. Cannot contain additional properties. mapper : A function that converts every record of a stream to a list of output records. Any of string object : Refer to #/definitions/KeyValueToKeyValueListTransformerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"transformKeyValueToKeyValueList\", \"flatMap\"] . TransformKeyValueToValueListOperation (object) : Convert every record in the stream to a list of output records with the same key. Cannot contain additional properties. mapper : A function that converts every key/value into a list of result values, each of which will be combined with the original key to form a new message in the output stream. Any of string object : Refer to #/definitions/KeyValueToValueListTransformerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"transformKeyValueToValueList\", \"flatMapValues\"] . TransformMetadataOperation (object) : Convert the metadata of every record in the stream. Cannot contain additional properties. mapper : A function that converts the metadata (Kafka headers, timestamp) of every record in the stream. Any of string object : Refer to #/definitions/MetadataTransformerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"transformMetadata\"] . TransformValueOperation (object) : Convert the value of every record in the stream to another value. Cannot contain additional properties. mapper : A function that converts the value of every record into another value. Any of string object : Refer to #/definitions/ValueTransformerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the transformed table (only applies to tables, ignored for streams). Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"mapValue\", \"transformValue\", \"mapValues\"] . ValueJoinerDefinition (object) : Defines a value joiner function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the value joiner. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the value joiner. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the value joiner. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the value joiner. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the value joiner. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the value joiner. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"valueJoiner\"] . ValueJoinerDefinitionWithImplicitStoreType (object) : Defines a value joiner function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the value joiner. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the value joiner. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the value joiner. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the value joiner. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the value joiner. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the value joiner. Only required for function types, which are not pre-defined. ValueTransformerDefinition (object) : Defines a value transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the value transformer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the value transformer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the value transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the value transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the value transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the value transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the value transformer uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"valueTransformer\"] . ValueTransformerDefinitionWithImplicitStoreType (object) : Defines a value transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the value transformer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the value transformer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the value transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the value transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the value transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the value transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the value transformer uses. Only required if the function wants to use a state store. Items (string) WindowBySessionOperation (object) : Operation to window messages by session, configured by an inactivity gap. Cannot contain additional properties. grace : (optional) (Tumbling, Hopping) The grace period, during which out-of-order records can still be processed. Any of integer string inactivityGap : The inactivity gap, below which two messages are considered to be of the same session. Any of integer string name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"windowBySession\"] . WindowByTimeOperationWithHoppingWindow (object) : Operation to window records based on time criteria. Cannot contain additional properties. advanceBy : The amount of time to increase time windows by. Any of integer string duration : The duration of time windows. Any of integer string grace : (optional) The grace period, during which out-of-order records can still be processed. Any of integer string name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"windowByTime\"] . windowType : The windowType of the time window. Must be one of: [\"hopping\"] . WindowByTimeOperationWithSlidingWindow (object) : Operation to window records based on time criteria. Cannot contain additional properties. grace : (optional) The grace period, during which out-of-order records can still be processed. Any of integer string name (string) : (optional) The name of the operation processor. timeDifference : The maximum amount of time difference between two records. Any of integer string type : The type of the operation. Must be one of: [\"windowByTime\"] . windowType : The windowType of the time window. Must be one of: [\"sliding\"] . WindowByTimeOperationWithTumblingWindow (object) : Operation to window records based on time criteria. Cannot contain additional properties. duration : The duration of time windows. Any of integer string grace : (optional) The grace period, during which out-of-order records can still be processed. Any of integer string name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"windowByTime\"] . windowType : The windowType of the time window. Must be one of: [\"tumbling\"] . WindowStateStoreDefinition (object) : Definition of a window state store. Cannot contain additional properties. caching (boolean) : (optional) \"true\" if changed to the window store need to be buffered and periodically released, \"false\" to emit all changes directly. keyType (string) : (optional) The key type of the window store. logging (boolean) : (optional) \"true\" if a changelog topic should be set up on Kafka for this window store, \"false\" otherwise. name (string) : (optional) The name of the window store. If this field is not defined, then the name is derived from the context. persistent (boolean) : (optional) \"true\" if this window store needs to be stored on disk, \"false\" otherwise. retainDuplicates (boolean) : (optional) Whether or not to retain duplicates. retention : (optional) The duration for which elements in the window store are retained. Any of integer string timestamped (boolean) : (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise. type : The type of the state store. Must be one of: [\"window\"] . valueType (string) : (optional) The value type of the window store. windowSize : (optional) Size of the windows (cannot be negative). Any of integer string WindowStateStoreDefinitionWithImplicitStoreType (object) : Definition of a window state store. Cannot contain additional properties. caching (boolean) : (optional) \"true\" if changed to the window store need to be buffered and periodically released, \"false\" to emit all changes directly. keyType (string) : (optional) The key type of the window store. logging (boolean) : (optional) \"true\" if a changelog topic should be set up on Kafka for this window store, \"false\" otherwise. name (string) : (optional) The name of the window store. If this field is not defined, then the name is derived from the context. persistent (boolean) : (optional) \"true\" if this window store needs to be stored on disk, \"false\" otherwise. retainDuplicates (boolean) : (optional) Whether or not to retain duplicates. retention : (optional) The duration for which elements in the window store are retained. Any of integer string timestamped (boolean) : (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise. type : The type of the state store. Must be one of: [\"window\"] . valueType (string) : (optional) The value type of the window store. windowSize : (optional) Size of the windows (cannot be negative). Any of integer string","title":"TopologyDefinition"},{"location":"ksml-language-spec/#topologydefinition","text":"KSML definition","title":"TopologyDefinition"},{"location":"ksml-language-spec/#properties","text":"description (string) : (optional) The description of this KSML definition. functions (object) : (optional) Functions that can be referenced in producers and pipelines. ^[a-zA-Z0-9_]+$ Any of object : Refer to #/definitions/AggregatorDefinition . object : Refer to #/definitions/ForEachActionDefinition . object : Refer to #/definitions/ForeignKeyExtractorDefinition . object : Refer to #/definitions/GeneratorDefinition . object : Refer to #/definitions/GenericFunctionDefinitionWithImplicitStoreType . object : Refer to #/definitions/InitializerDefinition . object : Refer to #/definitions/KeyTransformerDefinition . object : Refer to #/definitions/KeyValueMapperDefinition . object : Refer to #/definitions/KeyValuePrinterDefinition . object : Refer to #/definitions/KeyValueToKeyValueListTransformerDefinition . object : Refer to #/definitions/KeyValueToValueListTransformerDefinition . object : Refer to #/definitions/KeyValueTransformerDefinition . object : Refer to #/definitions/MergerDefinition . object : Refer to #/definitions/MetadataTransformerDefinition . object : Refer to #/definitions/PredicateDefinition . object : Refer to #/definitions/ReducerDefinition . object : Refer to #/definitions/StreamPartitionerDefinition . object : Refer to #/definitions/TimestampExtractorDefinition . object : Refer to #/definitions/TopicNameExtractorDefinition . object : Refer to #/definitions/ValueJoinerDefinition . object : Refer to #/definitions/ValueTransformerDefinition . globalTables (object) : (optional) GlobalTables that can be referenced in producers and pipelines. ^[a-zA-Z0-9_]+$ (object) : Refer to #/definitions/GlobalTableDefinition . name (string) : (optional) The name of this KSML definition. pipelines (object) : (optional) Collection of named pipelines. ^[a-zA-Z0-9_]+$ (object) : Refer to #/definitions/PipelineDefinition . producers (object) : (optional) Collection of named producers. ^[a-zA-Z0-9_]+$ (object) : Refer to #/definitions/ProducerDefinition . stores (object) : (optional) State stores that can be referenced in producers and pipelines. ^[a-zA-Z0-9_]+$ Any of object : Refer to #/definitions/KeyValueStateStoreDefinition . object : Refer to #/definitions/SessionStateStoreDefinition . object : Refer to #/definitions/WindowStateStoreDefinition . streams (object) : (optional) Streams that can be referenced in producers and pipelines. ^[a-zA-Z0-9_]+$ (object) : Refer to #/definitions/StreamDefinition . tables (object) : (optional) Tables that can be referenced in producers and pipelines. ^[a-zA-Z0-9_]+$ (object) : Refer to #/definitions/TableDefinition . version (string) : (optional) The version of this KSML definition.","title":"Properties"},{"location":"ksml-language-spec/#definitions","text":"AggregateOperation (object) : An aggregate operation. Cannot contain additional properties. adder : (optional) (GroupedTable) A function that adds a record to the aggregation result. Any of string object : Refer to #/definitions/AggregatorDefinitionWithImplicitStoreType . aggregator : (optional) (GroupedStream, SessionWindowedStream, TimeWindowedStream) The aggregator function, which combines a value with the previous aggregation result and outputs a new aggregation result. Any of string object : Refer to #/definitions/AggregatorDefinitionWithImplicitStoreType . initializer : The initializer function, which generates an initial value for every set of aggregated records. Any of string object : Refer to #/definitions/InitializerDefinitionWithImplicitStoreType . merger : (optional) (SessionWindowedStream, SessionWindowedCogroupedStream) A function that combines two aggregation results. Any of string object : Refer to #/definitions/MergerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the result aggregation. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinition . object : Refer to #/definitions/SessionStateStoreDefinition . object : Refer to #/definitions/WindowStateStoreDefinition . subtractor : (optional) (GroupedTable) A function that removes a record from the aggregation result. Any of string object : Refer to #/definitions/AggregatorDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"aggregate\"] . AggregatorDefinition (object) : Defines a aggregator function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the aggregator. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the aggregator. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the aggregator. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the aggregator. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the aggregator. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the aggregator. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"aggregator\"] . AggregatorDefinitionWithImplicitStoreType (object) : Defines a aggregator function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the aggregator. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the aggregator. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the aggregator. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the aggregator. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the aggregator. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the aggregator. Only required for function types, which are not pre-defined. BranchDefinitionWithPipeline (object) : Defines a branch with sub-pipeline in a BranchOperation. Cannot contain additional properties. as (string) : (optional) The name to register the pipeline result under, which can be used as source by follow-up pipelines. branch (array) : (optional) Defines a single branch, consisting of a condition and a pipeline to execute for messages that fulfil the predicate. Items (object) : Refer to #/definitions/StringOrInlinePredicateDefinitionWithImplicitStoreType . forEach : (optional) A function that gets called for every message in the stream. Any of string object : Refer to #/definitions/ForEachActionDefinitionWithImplicitStoreType . if : (optional) Defines the condition under which messages get sent down this branch. Any of string object : Refer to #/definitions/PredicateDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. print (object) : (optional) The specification of where to print messages to. Refer to #/definitions/PrintOperation . to : (optional) Ends the pipeline by sending all messages to a stream, table or globalTable, or to an inline defined output topic and optional partitioner. Any of string object : Refer to #/definitions/ToTopicDefinition . toTopicNameExtractor : (optional) Ends the pipeline by sending all messages to a topic provided by a pre-defined topic name extractor function, or to a topic provided by an inline defined topic name extractor and optional partitioner. Any of string object : Refer to #/definitions/ToTopicNameExtractorDefinition . via (array) : (optional) A series of operations performed on the input stream. Items Any of object : Refer to #/definitions/AggregateOperation . object : Refer to #/definitions/CogroupOperation . object : Refer to #/definitions/ConvertKeyOperation . object : Refer to #/definitions/ConvertKeyValueOperation . object : Refer to #/definitions/ConvertValueOperation . object : Refer to #/definitions/CountOperation . object : Refer to #/definitions/FilterNotOperation . object : Refer to #/definitions/FilterOperation . object : Refer to #/definitions/GroupByKeyOperation . object : Refer to #/definitions/GroupByOperation . object : Refer to #/definitions/JoinWithGlobalTableOperation . object : Refer to #/definitions/JoinWithStreamOperation . object : Refer to #/definitions/JoinWithTableOperation . object : Refer to #/definitions/LeftJoinWithGlobalTableOperation . object : Refer to #/definitions/LeftJoinWithStreamOperation . object : Refer to #/definitions/LeftJoinWithTableOperation . object : Refer to #/definitions/MergeOperation . object : Refer to #/definitions/OuterJoinWithStreamOperation . object : Refer to #/definitions/OuterJoinWithTableOperation . object : Refer to #/definitions/PeekOperation . object : Refer to #/definitions/ReduceOperationWithAdderAndSubtractor . object : Refer to #/definitions/ReduceOperationWithReducer . object : Refer to #/definitions/RepartitionOperation . object : Refer to #/definitions/SuppressOperationUntilTimeLimit . object : Refer to #/definitions/SuppressOperationUntilWindowCloses . object : Refer to #/definitions/ToStreamOperation . object : Refer to #/definitions/ToTableOperation . object : Refer to #/definitions/TransformKeyOperation . object : Refer to #/definitions/TransformKeyValueOperation . object : Refer to #/definitions/TransformKeyValueToKeyValueListOperation . object : Refer to #/definitions/TransformKeyValueToValueListOperation . object : Refer to #/definitions/TransformMetadataOperation . object : Refer to #/definitions/TransformValueOperation . object : Refer to #/definitions/WindowBySessionOperation . object : Refer to #/definitions/WindowByTimeOperationWithHoppingWindow . object : Refer to #/definitions/WindowByTimeOperationWithSlidingWindow . object : Refer to #/definitions/WindowByTimeOperationWithTumblingWindow . CogroupOperation (object) : A cogroup operation. Cannot contain additional properties. aggregator : (GroupedStream, SessionWindowedStream, TimeWindowedStream) The aggregator function, which combines a value with the previous aggregation result and outputs a new aggregation result. Any of string object : Refer to #/definitions/AggregatorDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"cogroup\"] . ConvertKeyOperation (object) : An operation to convert the stream key type to another type. Conversion is only syntactic, eg. from Avro to XML. Cannot contain additional properties. into (string, required) : The type to convert the stream key into. name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"convertKey\"] . ConvertKeyValueOperation (object) : An operation to convert the stream key and value types to other types. Conversion is only syntactic, eg. from Avro to XML. Cannot contain additional properties. into (string, required) : The tuple type to convert the stream key/value into. name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"convertKeyValue\"] . ConvertValueOperation (object) : An operation to convert the stream value type to another type. Conversion is only syntactic, eg. from Avro to XML. Cannot contain additional properties. into (string, required) : The type to convert the stream value into. name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"convertValue\"] . CountOperation (object) : Count the number of times a key is seen in a given window. Cannot contain additional properties. name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the count operation's result. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinition . object : Refer to #/definitions/SessionStateStoreDefinition . object : Refer to #/definitions/WindowStateStoreDefinition . type : The type of the operation. Must be one of: [\"count\"] . FilterNotOperation (object) : Filter records based on the inverse result of a predicate function. Cannot contain additional properties. if : A function that returns \"false\" when records are accepted, \"true\" otherwise. Any of string object : Refer to #/definitions/PredicateDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the filtered table (only applies to tables, ignored for streams). Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"filterNot\"] . FilterOperation (object) : Filter records based on a predicate function. Cannot contain additional properties. if : A function that returns \"true\" when records are accepted, \"false\" otherwise. Any of string object : Refer to #/definitions/PredicateDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the filtered table (only applies to tables, ignored for streams). Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"filter\"] . ForEachActionDefinition (object) : Defines a foreach action function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the foreach action. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the foreach action. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the foreach action. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the foreach action. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the foreach action. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the foreach action. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the foreach action uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"forEach\"] . ForEachActionDefinitionWithImplicitStoreType (object) : Defines a foreach action function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the foreach action. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the foreach action. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the foreach action. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the foreach action. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the foreach action. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the foreach action. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the foreach action uses. Only required if the function wants to use a state store. Items (string) ForeignKeyExtractorDefinition (object) : Defines a foreign key extractor function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the foreign key extractor. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the foreign key extractor. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the foreign key extractor. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the foreign key extractor. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the foreign key extractor. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the foreign key extractor. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"foreignKeyExtractor\"] . ForeignKeyExtractorDefinitionWithImplicitStoreType (object) : Defines a foreign key extractor function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the foreign key extractor. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the foreign key extractor. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the foreign key extractor. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the foreign key extractor. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the foreign key extractor. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the foreign key extractor. Only required for function types, which are not pre-defined. GeneratorDefinition (object) : Defines a message generator function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the message generator. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the message generator. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the message generator. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the message generator. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the message generator. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the message generator. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"generator\"] . GeneratorDefinitionWithImplicitStoreType (object) : Defines a message generator function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the message generator. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the message generator. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the message generator. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the message generator. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the message generator. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the message generator. Only required for function types, which are not pre-defined. GenericFunctionDefinitionWithImplicitStoreType (object) : Defines a generic function function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the generic function. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the generic function. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the generic function. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the generic function. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the generic function. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the generic function. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"generic\"] . GlobalTableDefinition (object) : Contains a definition of a globalTable, which can be referenced by producers and pipelines. Cannot contain additional properties. keyType (string, required) : The key type of the globalTable. offsetResetPolicy (string) : (optional) The policy that determines what to do when there is no initial consumer offset in Kafka, or if the message at the committed consumer offset does not exist (e.g. because that data has been deleted). partitioner : (optional) A function that determines to which topic partition a given message needs to be written. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . store : (optional) KeyValue state store definition. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType . timestampExtractor : (optional) A function that extracts the event time from a consumed record. Any of string object : Refer to #/definitions/TimestampExtractorDefinitionWithImplicitStoreType . topic (string, required) : The name of the Kafka topic for this globalTable. valueType (string, required) : The value type of the globalTable. GlobalTableDefinitionAsJoinTarget (object) : Reference to a globalTable in a join operation. Cannot contain additional properties. keyType (string) : (optional) The key type of the globalTable. partitioner : (optional) A function that determines to which topic partition a given message needs to be written. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . store : (optional) KeyValue state store definition. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType . topic (string, required) : The name of the Kafka topic for this globalTable. valueType (string) : (optional) The value type of the globalTable. GroupByKeyOperation (object) : Operation to group all messages with the same key together. Cannot contain additional properties. name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the grouped stream. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"groupByKey\"] . GroupByOperation (object) : Operation to group all messages with together based on a keying function. Cannot contain additional properties. mapper : Function to map records to a key they can be grouped on. Any of string object : Refer to #/definitions/KeyValueMapperDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the grouped stream or table. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"groupBy\"] . InitializerDefinition (object) : Defines a initializer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the initializer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the initializer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the initializer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the initializer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the initializer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the initializer. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"initializer\"] . InitializerDefinitionWithImplicitStoreType (object) : Defines a initializer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the initializer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the initializer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the initializer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the initializer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the initializer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the initializer. Only required for function types, which are not pre-defined. JoinWithGlobalTableOperation (object) : Operation to join with a table. Cannot contain additional properties. globalTable : A reference to the globalTable, or an inline definition of the globalTable to join with. Any of string object : Refer to #/definitions/GlobalTableDefinitionAsJoinTarget . mapper : A function that maps the key value from the stream to the primary key type of the globalTable. Any of string object : Refer to #/definitions/KeyValueMapperDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"join\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . JoinWithStreamOperation (object) : Operation to join with a stream. Cannot contain additional properties. grace : (optional) The window grace period (the time to admit out-of-order events after the end of the window). Any of integer string name (string) : (optional) The name of the operation processor. otherStore : Materialized view of the joined stream. Any of string object : Refer to #/definitions/WindowStateStoreDefinitionWithImplicitStoreType . stream : A reference to the stream, or an inline definition of the stream to join with. Any of string object : Refer to #/definitions/StreamDefinitionAsJoinTarget . thisStore : Materialized view of the source stream. Any of string object : Refer to #/definitions/WindowStateStoreDefinitionWithImplicitStoreType . timeDifference : The maximum time difference for a join over two streams on the same key. Any of integer string type : The type of the operation. Must be one of: [\"join\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . JoinWithTableOperation (object) : Operation to join with a table. Cannot contain additional properties. foreignKeyExtractor : (optional) A function that can translate the join table value to a primary key. Any of string object : Refer to #/definitions/ForeignKeyExtractorDefinitionWithImplicitStoreType . grace : (optional) The window grace period (the time to admit out-of-order events after the end of the window). Any of integer string name (string) : (optional) The name of the operation processor. otherPartitioner : (optional) A function that partitions the records on the join table. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . partitioner : (optional) A function that partitions the records on the primary table. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . store : (optional) Materialized view of the joined table (only used for Table-Table joins). Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . table : A reference to the table, or an inline definition of the table to join with. Any of string object : Refer to #/definitions/TableDefinitionAsJoinTarget . type : The type of the operation. Must be one of: [\"join\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . KeyTransformerDefinition (object) : Defines a key transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the key transformer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the key transformer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the key transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the key transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the key transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the key transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the key transformer uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"keyTransformer\"] . KeyTransformerDefinitionWithImplicitStoreType (object) : Defines a key transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the key transformer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the key transformer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the key transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the key transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the key transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the key transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the key transformer uses. Only required if the function wants to use a state store. Items (string) KeyValueMapperDefinition (object) : Defines a keyvalue mapper function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue mapper. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the keyvalue mapper. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue mapper. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue mapper. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue mapper. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue mapper. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"keyValueMapper\"] . KeyValueMapperDefinitionWithImplicitStoreType (object) : Defines a keyvalue mapper function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue mapper. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the keyvalue mapper. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue mapper. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue mapper. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue mapper. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue mapper. Only required for function types, which are not pre-defined. KeyValuePrinterDefinition (object) : Defines a keyvalue printer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue printer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the keyvalue printer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue printer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue printer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue printer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue printer. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"keyValuePrinter\"] . KeyValuePrinterDefinitionWithImplicitStoreType (object) : Defines a keyvalue printer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue printer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the keyvalue printer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue printer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue printer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue printer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue printer. Only required for function types, which are not pre-defined. KeyValueStateStoreDefinition (object) : Definition of a keyValue state store. Cannot contain additional properties. caching (boolean) : (optional) \"true\" if changed to the keyValue store need to be buffered and periodically released, \"false\" to emit all changes directly. historyRetention : (optional) (Versioned only) The duration for which old record versions are available for query (cannot be negative). Any of integer string keyType (string) : (optional) The key type of the keyValue store. logging (boolean) : (optional) \"true\" if a changelog topic should be set up on Kafka for this keyValue store, \"false\" otherwise. name (string) : (optional) The name of the keyValue store. If this field is not defined, then the name is derived from the context. persistent (boolean) : (optional) \"true\" if this keyValue store needs to be stored on disk, \"false\" otherwise. segmentInterval : (optional) Size of segments for storing old record versions (must be positive). Old record versions for the same key in a single segment are stored (updated and accessed) together. The only impact of this parameter is performance. If segments are large and a workload results in many record versions for the same key being collected in a single segment, performance may degrade as a result. On the other hand, historical reads (which access older segments) and out-of-order writes may slow down if there are too many segments. Any of integer string timestamped (boolean) : (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise. type : The type of the state store. Must be one of: [\"keyValue\"] . valueType (string) : (optional) The value type of the keyValue store. versioned (boolean) : (optional) \"true\" if elements in the store are versioned, \"false\" otherwise. KeyValueStateStoreDefinitionWithImplicitStoreType (object) : Definition of a keyValue state store. Cannot contain additional properties. caching (boolean) : (optional) \"true\" if changed to the keyValue store need to be buffered and periodically released, \"false\" to emit all changes directly. historyRetention : (optional) (Versioned only) The duration for which old record versions are available for query (cannot be negative). Any of integer string keyType (string) : (optional) The key type of the keyValue store. logging (boolean) : (optional) \"true\" if a changelog topic should be set up on Kafka for this keyValue store, \"false\" otherwise. name (string) : (optional) The name of the keyValue store. If this field is not defined, then the name is derived from the context. persistent (boolean) : (optional) \"true\" if this keyValue store needs to be stored on disk, \"false\" otherwise. segmentInterval : (optional) Size of segments for storing old record versions (must be positive). Old record versions for the same key in a single segment are stored (updated and accessed) together. The only impact of this parameter is performance. If segments are large and a workload results in many record versions for the same key being collected in a single segment, performance may degrade as a result. On the other hand, historical reads (which access older segments) and out-of-order writes may slow down if there are too many segments. Any of integer string timestamped (boolean) : (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise. type : The type of the state store. Must be one of: [\"keyValue\"] . valueType (string) : (optional) The value type of the keyValue store. versioned (boolean) : (optional) \"true\" if elements in the store are versioned, \"false\" otherwise. KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType (object) : Definition of a keyValue state store. Cannot contain additional properties. caching (boolean) : (optional) \"true\" if changed to the keyValue store need to be buffered and periodically released, \"false\" to emit all changes directly. historyRetention : (optional) (Versioned only) The duration for which old record versions are available for query (cannot be negative). Any of integer string logging (boolean) : (optional) \"true\" if a changelog topic should be set up on Kafka for this keyValue store, \"false\" otherwise. persistent (boolean) : (optional) \"true\" if this keyValue store needs to be stored on disk, \"false\" otherwise. segmentInterval : (optional) Size of segments for storing old record versions (must be positive). Old record versions for the same key in a single segment are stored (updated and accessed) together. The only impact of this parameter is performance. If segments are large and a workload results in many record versions for the same key being collected in a single segment, performance may degrade as a result. On the other hand, historical reads (which access older segments) and out-of-order writes may slow down if there are too many segments. Any of integer string timestamped (boolean) : (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise. type : The type of the state store. Must be one of: [\"keyValue\"] . versioned (boolean) : (optional) \"true\" if elements in the store are versioned, \"false\" otherwise. KeyValueToKeyValueListTransformerDefinition (object) : Defines a keyvalue-to-keyvaluelist transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue-to-keyvaluelist transformer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the keyvalue-to-keyvaluelist transformer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue-to-keyvaluelist transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue-to-keyvaluelist transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue-to-keyvaluelist transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue-to-keyvaluelist transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the keyvalue-to-keyvaluelist transformer uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"keyValueToKeyValueListTransformer\"] . KeyValueToKeyValueListTransformerDefinitionWithImplicitStoreType (object) : Defines a keyvalue-to-keyvaluelist transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue-to-keyvaluelist transformer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the keyvalue-to-keyvaluelist transformer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue-to-keyvaluelist transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue-to-keyvaluelist transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue-to-keyvaluelist transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue-to-keyvaluelist transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the keyvalue-to-keyvaluelist transformer uses. Only required if the function wants to use a state store. Items (string) KeyValueToValueListTransformerDefinition (object) : Defines a keyvalue-to-valuelist transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue-to-valuelist transformer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the keyvalue-to-valuelist transformer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue-to-valuelist transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue-to-valuelist transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue-to-valuelist transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue-to-valuelist transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the keyvalue-to-valuelist transformer uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"keyValueToValueListTransformer\"] . KeyValueToValueListTransformerDefinitionWithImplicitStoreType (object) : Defines a keyvalue-to-valuelist transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue-to-valuelist transformer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the keyvalue-to-valuelist transformer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue-to-valuelist transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue-to-valuelist transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue-to-valuelist transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue-to-valuelist transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the keyvalue-to-valuelist transformer uses. Only required if the function wants to use a state store. Items (string) KeyValueTransformerDefinition (object) : Defines a keyvalue transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue transformer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the keyvalue transformer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the keyvalue transformer uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"keyValueTransformer\"] . KeyValueTransformerDefinitionWithImplicitStoreType (object) : Defines a keyvalue transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue transformer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the keyvalue transformer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the keyvalue transformer uses. Only required if the function wants to use a state store. Items (string) LeftJoinWithGlobalTableOperation (object) : Operation to leftJoin with a globalTable. Cannot contain additional properties. globalTable : A reference to the globalTable, or an inline definition of the globalTable to join with. Any of string object : Refer to #/definitions/GlobalTableDefinitionAsJoinTarget . mapper : A function that maps the key value from the stream with the primary key of the globalTable. Any of string object : Refer to #/definitions/KeyValueMapperDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"leftJoin\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . LeftJoinWithStreamOperation (object) : Operation to leftJoin with a stream. Cannot contain additional properties. grace : (optional) The window grace period (the time to admit out-of-order events after the end of the window). Any of integer string name (string) : (optional) The name of the operation processor. otherStore : Materialized view of the leftJoined stream. Any of string object : Refer to #/definitions/WindowStateStoreDefinitionWithImplicitStoreType . stream : A reference to the stream, or an inline definition of the stream to leftJoin with. Any of string object : Refer to #/definitions/StreamDefinitionAsJoinTarget . thisStore : Materialized view of the source stream. Any of string object : Refer to #/definitions/WindowStateStoreDefinitionWithImplicitStoreType . timeDifference : The maximum time difference for a leftJoin over two streams on the same key. Any of integer string type : The type of the operation. Must be one of: [\"leftJoin\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . LeftJoinWithTableOperation (object) : Operation to leftJoin with a table. Cannot contain additional properties. foreignKeyExtractor : (optional) A function that can translate the join table value to a primary key. Any of string object : Refer to #/definitions/ForeignKeyExtractorDefinitionWithImplicitStoreType . grace : (optional) The window grace period (the time to admit out-of-order events after the end of the window). Any of integer string name (string) : (optional) The name of the operation processor. otherPartitioner : (optional) A function that partitions the records on the join table. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . partitioner : (optional) A function that partitions the records on the primary table. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . store : (optional) Materialized view of the leftJoined table (only used for Table-Table joins). Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . table : A reference to the table, or an inline definition of the table to join with. Any of string object : Refer to #/definitions/TableDefinitionAsJoinTarget . type : The type of the operation. Must be one of: [\"leftJoin\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . MergeOperation (object) : A merge operation to join two Streams. Cannot contain additional properties. name (string) : (optional) The name of the operation processor. stream : The stream to merge with. Any of string object : Refer to #/definitions/StreamDefinition . type : The type of the operation. Must be one of: [\"merge\"] . MergerDefinition (object) : Defines a merger function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the merger. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the merger. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the merger. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the merger. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the merger. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the merger. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"merger\"] . MergerDefinitionWithImplicitStoreType (object) : Defines a merger function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the merger. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the merger. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the merger. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the merger. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the merger. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the merger. Only required for function types, which are not pre-defined. MetadataTransformerDefinition (object) : Defines a metadata transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the metadata transformer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the metadata transformer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the metadata transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the metadata transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the metadata transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the metadata transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the metadata transformer uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"metadataTransformer\"] . MetadataTransformerDefinitionWithImplicitStoreType (object) : Defines a metadata transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the metadata transformer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the metadata transformer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the metadata transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the metadata transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the metadata transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the metadata transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the metadata transformer uses. Only required if the function wants to use a state store. Items (string) OuterJoinWithStreamOperation (object) : Operation to outerJoin with a stream. Cannot contain additional properties. grace : (optional) The window grace period (the time to admit out-of-order events after the end of the window). Any of integer string name (string) : (optional) The name of the operation processor. otherStore : Materialized view of the outerJoined stream. Any of string object : Refer to #/definitions/WindowStateStoreDefinitionWithImplicitStoreType . stream : A reference to the stream, or an inline definition of the stream to outerJoin with. Any of string object : Refer to #/definitions/StreamDefinitionAsJoinTarget . thisStore : Materialized view of the source stream. Any of string object : Refer to #/definitions/WindowStateStoreDefinitionWithImplicitStoreType . timeDifference : The maximum time difference for an outerJoin over two streams on the same key. Any of integer string type : The type of the operation. Must be one of: [\"outerJoin\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . OuterJoinWithTableOperation (object) : Operation to outerJoin with a table. Cannot contain additional properties. name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the outerJoined table. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . table : A reference to the table, or an inline definition of the table to outerJoin with. Any of string object : Refer to #/definitions/TableDefinitionAsJoinTarget . type : The type of the operation. Must be one of: [\"outerJoin\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . ParameterDefinition (object) : Defines a parameter for a user function. Cannot contain additional properties. defaultValue (string) : (optional) The default value for the parameter. name (string, required) : The name of the parameter. type (string, required) : The type of the parameter. PeekOperation (object) : Operation to peek into a stream, without modifying the stream contents. Cannot contain additional properties. forEach : A function that gets called for every message in the stream. Any of string object : Refer to #/definitions/ForEachActionDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"peek\"] . PipelineDefinition (object) : Defines a pipeline through a source, a series of operations to perform on it and a sink operation to close the stream with. Cannot contain additional properties. as (string) : (optional) The name to register the pipeline result under, which can be used as source by follow-up pipelines. branch (array) : (optional) Defines a single branch, consisting of a condition and a pipeline to execute for messages that fulfil the predicate. Items (object) : Refer to #/definitions/BranchDefinitionWithPipeline . forEach : (optional) A function that gets called for every message in the stream. Any of string object : Refer to #/definitions/ForEachActionDefinitionWithImplicitStoreType . from : Pipeline source. Any of string object : Refer to #/definitions/TopicDefinitionSource . name (string) : (optional) The name of the operation processor. print (object) : (optional) The specification of where to print messages to. Refer to #/definitions/PrintOperation . to : (optional) Ends the pipeline by sending all messages to a stream, table or globalTable, or to an inline defined output topic and optional partitioner. Any of string object : Refer to #/definitions/ToTopicDefinition . toTopicNameExtractor : (optional) Ends the pipeline by sending all messages to a topic provided by a pre-defined topic name extractor function, or to a topic provided by an inline defined topic name extractor and optional partitioner. Any of string object : Refer to #/definitions/ToTopicNameExtractorDefinition . via (array) : (optional) A series of operations performed on the input stream. Items Any of object : Refer to #/definitions/AggregateOperation . object : Refer to #/definitions/CogroupOperation . object : Refer to #/definitions/ConvertKeyOperation . object : Refer to #/definitions/ConvertKeyValueOperation . object : Refer to #/definitions/ConvertValueOperation . object : Refer to #/definitions/CountOperation . object : Refer to #/definitions/FilterNotOperation . object : Refer to #/definitions/FilterOperation . object : Refer to #/definitions/GroupByKeyOperation . object : Refer to #/definitions/GroupByOperation . object : Refer to #/definitions/JoinWithGlobalTableOperation . object : Refer to #/definitions/JoinWithStreamOperation . object : Refer to #/definitions/JoinWithTableOperation . object : Refer to #/definitions/LeftJoinWithGlobalTableOperation . object : Refer to #/definitions/LeftJoinWithStreamOperation . object : Refer to #/definitions/LeftJoinWithTableOperation . object : Refer to #/definitions/MergeOperation . object : Refer to #/definitions/OuterJoinWithStreamOperation . object : Refer to #/definitions/OuterJoinWithTableOperation . object : Refer to #/definitions/PeekOperation . object : Refer to #/definitions/ReduceOperationWithAdderAndSubtractor . object : Refer to #/definitions/ReduceOperationWithReducer . object : Refer to #/definitions/RepartitionOperation . object : Refer to #/definitions/SuppressOperationUntilTimeLimit . object : Refer to #/definitions/SuppressOperationUntilWindowCloses . object : Refer to #/definitions/ToStreamOperation . object : Refer to #/definitions/ToTableOperation . object : Refer to #/definitions/TransformKeyOperation . object : Refer to #/definitions/TransformKeyValueOperation . object : Refer to #/definitions/TransformKeyValueToKeyValueListOperation . object : Refer to #/definitions/TransformKeyValueToValueListOperation . object : Refer to #/definitions/TransformMetadataOperation . object : Refer to #/definitions/TransformValueOperation . object : Refer to #/definitions/WindowBySessionOperation . object : Refer to #/definitions/WindowByTimeOperationWithHoppingWindow . object : Refer to #/definitions/WindowByTimeOperationWithSlidingWindow . object : Refer to #/definitions/WindowByTimeOperationWithTumblingWindow . PredicateDefinition (object) : Defines a predicate function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the predicate. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the predicate. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the predicate. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the predicate. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the predicate. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the predicate. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the predicate uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"predicate\"] . PredicateDefinitionWithImplicitStoreType (object) : Defines a predicate function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the predicate. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the predicate. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the predicate. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the predicate. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the predicate. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the predicate. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the predicate uses. Only required if the function wants to use a state store. Items (string) PrintOperation (object) : Operation to print the contents of a pipeline on the screen or to write them to a file. Cannot contain additional properties. filename (string) : (optional) The filename to output records to. If nothing is specified, then messages will be printed on stdout. label (string) : (optional) A label to attach to the output records. mapper : (optional) A function to convert record into a string for output. Any of string object : Refer to #/definitions/KeyValuePrinterDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. ProducerDefinition (object) : Definition of a Producer that regularly generates messages for a topic. Cannot contain additional properties. batchSize (integer) : (optional) The size of batches. condition : (optional) A function that validates the generator's result message. Returns \"true\" when the message may be produced on the topic, \"false\" otherwise. Any of string object : Refer to #/definitions/PredicateDefinitionWithImplicitStoreType . count (integer) : (optional) The number of messages to produce. generator : The function that generates records. Any of string object : Refer to #/definitions/GeneratorDefinitionWithImplicitStoreType . interval : (optional) The interval with which the generator is called. Any of integer string to : The topic to produce to. Any of string object : Refer to #/definitions/TopicDefinition . until : (optional) A predicate that returns true to indicate producing should stop. Any of string object : Refer to #/definitions/PredicateDefinitionWithImplicitStoreType . ReduceOperationWithAdderAndSubtractor (object) : Operation to reduce a series of records into a single aggregate result. Cannot contain additional properties. adder : A function that adds a record to the aggregate result. Any of string object : Refer to #/definitions/ReducerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the aggregation. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . subtractor : A function that removes a record from the aggregate result. Any of string object : Refer to #/definitions/ReducerDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"reduce\"] . ReduceOperationWithReducer (object) : Operation to reduce a series of records into a single aggregate result. Cannot contain additional properties. name (string) : (optional) The name of the operation processor. reducer : A function that computes a new aggregate result. Any of string object : Refer to #/definitions/ReducerDefinitionWithImplicitStoreType . store : (optional) Materialized view of the aggregation. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinition . object : Refer to #/definitions/SessionStateStoreDefinition . object : Refer to #/definitions/WindowStateStoreDefinition . type : The type of the operation. Must be one of: [\"reduce\"] . ReducerDefinition (object) : Defines a reducer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the reducer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the reducer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the reducer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the reducer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the reducer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the reducer. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"reducer\"] . ReducerDefinitionWithImplicitStoreType (object) : Defines a reducer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the reducer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the reducer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the reducer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the reducer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the reducer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the reducer. Only required for function types, which are not pre-defined. RepartitionOperation (object) : Operation to (re)partition a stream. Cannot contain additional properties. name (string) : (optional) The name of the operation processor. numberOfPartitions (integer) : (optional) The target number of partitions. partitioner : (optional) A function that partitions stream records. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"repartition\"] . SessionStateStoreDefinition (object) : Definition of a session state store. Cannot contain additional properties. caching (boolean) : (optional) \"true\" if changed to the session store need to be buffered and periodically released, \"false\" to emit all changes directly. keyType (string) : (optional) The key type of the session store. logging (boolean) : (optional) \"true\" if a changelog topic should be set up on Kafka for this session store, \"false\" otherwise. name (string) : (optional) The name of the session store. If this field is not defined, then the name is derived from the context. persistent (boolean) : (optional) \"true\" if this session store needs to be stored on disk, \"false\" otherwise. retention : (optional) The duration for which elements in the session store are retained. Any of integer string timestamped (boolean) : (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise. type : The type of the state store. Must be one of: [\"session\"] . valueType (string) : (optional) The value type of the session store. StreamDefinition (object) : Contains a definition of a Stream, which can be referenced by producers and pipelines. Cannot contain additional properties. keyType (string, required) : The key type of the stream. offsetResetPolicy (string) : (optional) Policy that determines what to do when there is no initial offset in Kafka, or if the current offset does not exist any more on the server (e.g. because that data has been deleted). partitioner : (optional) A function that determines to which topic partition a given message needs to be written. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . timestampExtractor : (optional) A function extracts the event time from a consumed record. Any of string object : Refer to #/definitions/TimestampExtractorDefinitionWithImplicitStoreType . topic (string, required) : The name of the Kafka topic for this stream. valueType (string, required) : The value type of the stream. StreamDefinitionAsJoinTarget (object) : Reference to a Stream in a join or merge operation. Cannot contain additional properties. keyType (string) : (optional) The key type of the stream. topic (string, required) : The name of the Kafka topic for this stream. valueType (string) : (optional) The value type of the stream. StreamPartitionerDefinition (object) : Defines a stream partitioner function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the stream partitioner. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the stream partitioner. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the stream partitioner. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the stream partitioner. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the stream partitioner. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the stream partitioner. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"streamPartitioner\"] . StreamPartitionerDefinitionWithImplicitStoreType (object) : Defines a stream partitioner function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the stream partitioner. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the stream partitioner. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the stream partitioner. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the stream partitioner. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the stream partitioner. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the stream partitioner. Only required for function types, which are not pre-defined. StringOrInlinePredicateDefinitionWithImplicitStoreType (object) : Defines the condition under which messages get sent down this branch. Cannot contain additional properties. if : (optional) Defines the condition under which messages get sent down this branch. Any of string object : Refer to #/definitions/PredicateDefinitionWithImplicitStoreType . SuppressOperationUntilTimeLimit (object) : Operation to suppress messages in the source stream until a time limit is reached. Cannot contain additional properties. bufferFullStrategy : (optional) What to do when the buffer is full. Must be one of: [\"emitEarlyWhenFull\", \"shutdownWhenFull\"] . duration : The duration for which messages are suppressed. Any of integer string maxBytes (string) : (optional) The maximum number of bytes in the buffer. maxRecords (string) : (optional) The maximum number of records in the buffer. name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"suppress\"] . until : The until of the Operation to suppress messages in the source stream until a certain limit is reached. Must be one of: [\"timeLimit\"] . SuppressOperationUntilWindowCloses (object) : Operation to suppress messages in the source stream until a window limit is reached. Cannot contain additional properties. bufferFullStrategy : (optional) What to do when the buffer is full. Must be one of: [\"emitEarlyWhenFull\", \"shutdownWhenFull\"] . maxBytes (string) : (optional) The maximum number of bytes in the buffer. maxRecords (string) : (optional) The maximum number of records in the buffer. name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"suppress\"] . until : The until of the Operation to suppress messages in the source stream until a certain limit is reached. Must be one of: [\"windowCloses\"] . TableDefinition (object) : Contains a definition of a table, which can be referenced by producers and pipelines. Cannot contain additional properties. keyType (string, required) : The key type of the table. offsetResetPolicy (string) : (optional) The policy that determines what to do when there is no initial consumer offset in Kafka, or if the message at the committed consumer offset does not exist (e.g. because that data has been deleted). partitioner : (optional) A function that determines to which topic partition a given message needs to be written. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . store : (optional) KeyValue state store definition. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType . timestampExtractor : (optional) A function that extracts the event time from a consumed record. Any of string object : Refer to #/definitions/TimestampExtractorDefinitionWithImplicitStoreType . topic (string, required) : The name of the Kafka topic for this table. valueType (string, required) : The value type of the table. TableDefinitionAsJoinTarget (object) : Reference to a table in a join operation. Cannot contain additional properties. keyType (string) : (optional) The key type of the table. partitioner : (optional) A function that determines to which topic partition a given message needs to be written. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . store : (optional) KeyValue state store definition. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType . topic (string, required) : The name of the Kafka topic for this table. valueType (string) : (optional) The value type of the table. TimestampExtractorDefinition (object) : Defines a timestamp extractor function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the timestamp extractor. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the timestamp extractor. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the timestamp extractor. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the timestamp extractor. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the timestamp extractor. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the timestamp extractor. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"timestampExtractor\"] . TimestampExtractorDefinitionWithImplicitStoreType (object) : Defines a timestamp extractor function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the timestamp extractor. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the timestamp extractor. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the timestamp extractor. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the timestamp extractor. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the timestamp extractor. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the timestamp extractor. Only required for function types, which are not pre-defined. ToStreamOperation (object) : Convert a Table into a Stream, optionally through a custom key transformer. Cannot contain additional properties. mapper : (optional) A function that computes the output key for every record. Any of string object : Refer to #/definitions/KeyTransformerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"toStream\"] . ToTableOperation (object) : Convert a Stream into a Table. Cannot contain additional properties. name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the result table. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"toTable\"] . ToTopicDefinition (object) : Writes out pipeline messages to a topic. Cannot contain additional properties. keyType (string) : (optional) The key type of the topic. partitioner : (optional) A function that partitions the records in the output topic. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . topic (string, required) : The name of the Kafka topic. valueType (string) : (optional) The value type of the topic. ToTopicNameExtractorDefinition (object) : Writes out pipeline messages to a topic as given by a topic name extractor. Cannot contain additional properties. partitioner : (optional) A function that partitions the records in the output topic. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . topicNameExtractor : Reference to a pre-defined topic name extractor, or an inline definition of a topic name extractor. Any of string object : Refer to #/definitions/TopicNameExtractorDefinitionWithImplicitStoreType . TopicDefinition (object) : Contains a definition of a Kafka topic, to be used by producers and pipelines. Cannot contain additional properties. keyType (string) : (optional) The key type of the topic. topic (string, required) : The name of the Kafka topic. valueType (string) : (optional) The value type of the topic. TopicDefinitionSource (object) : Contains a definition of a Kafka topic, to be used by producers and pipelines. Cannot contain additional properties. keyType (string, required) : The key type of the topic. offsetResetPolicy (string) : (optional) Policy that determines what to do when there is no initial offset in Kafka, or if the current offset does not exist any more on the server (e.g. because that data has been deleted). partitioner : (optional) A function that determines to which topic partition a given message needs to be written. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . timestampExtractor : (optional) A function extracts the event time from a consumed record. Any of string object : Refer to #/definitions/TimestampExtractorDefinitionWithImplicitStoreType . topic (string, required) : The name of the Kafka topic. valueType (string, required) : The value type of the topic. TopicNameExtractorDefinition (object) : Defines a topic name extractor function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the topic name extractor. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the topic name extractor. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the topic name extractor. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the topic name extractor. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the topic name extractor. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the topic name extractor. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"topicNameExtractor\"] . TopicNameExtractorDefinitionWithImplicitStoreType (object) : Defines a topic name extractor function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the topic name extractor. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the topic name extractor. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the topic name extractor. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the topic name extractor. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the topic name extractor. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the topic name extractor. Only required for function types, which are not pre-defined. TransformKeyOperation (object) : Convert the key of every record in the stream to another key. Cannot contain additional properties. mapper : A function that computes a new key for each record. Any of string object : Refer to #/definitions/KeyTransformerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"transformKey\", \"mapKey\", \"selectKey\"] . TransformKeyValueOperation (object) : Convert the key/value of every record in the stream to another key/value. Cannot contain additional properties. mapper : A function that computes a new key/value for each record. Any of string object : Refer to #/definitions/KeyValueTransformerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"map\", \"transformKeyValue\"] . TransformKeyValueToKeyValueListOperation (object) : Convert a stream by transforming every record into a list of derived records. Cannot contain additional properties. mapper : A function that converts every record of a stream to a list of output records. Any of string object : Refer to #/definitions/KeyValueToKeyValueListTransformerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"transformKeyValueToKeyValueList\", \"flatMap\"] . TransformKeyValueToValueListOperation (object) : Convert every record in the stream to a list of output records with the same key. Cannot contain additional properties. mapper : A function that converts every key/value into a list of result values, each of which will be combined with the original key to form a new message in the output stream. Any of string object : Refer to #/definitions/KeyValueToValueListTransformerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"transformKeyValueToValueList\", \"flatMapValues\"] . TransformMetadataOperation (object) : Convert the metadata of every record in the stream. Cannot contain additional properties. mapper : A function that converts the metadata (Kafka headers, timestamp) of every record in the stream. Any of string object : Refer to #/definitions/MetadataTransformerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"transformMetadata\"] . TransformValueOperation (object) : Convert the value of every record in the stream to another value. Cannot contain additional properties. mapper : A function that converts the value of every record into another value. Any of string object : Refer to #/definitions/ValueTransformerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the transformed table (only applies to tables, ignored for streams). Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"mapValue\", \"transformValue\", \"mapValues\"] . ValueJoinerDefinition (object) : Defines a value joiner function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the value joiner. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the value joiner. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the value joiner. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the value joiner. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the value joiner. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the value joiner. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"valueJoiner\"] . ValueJoinerDefinitionWithImplicitStoreType (object) : Defines a value joiner function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the value joiner. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the value joiner. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the value joiner. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the value joiner. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the value joiner. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the value joiner. Only required for function types, which are not pre-defined. ValueTransformerDefinition (object) : Defines a value transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the value transformer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the value transformer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the value transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the value transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the value transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the value transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the value transformer uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"valueTransformer\"] . ValueTransformerDefinitionWithImplicitStoreType (object) : Defines a value transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the value transformer. Any of boolean integer number string expression : (optional) The (multiline) expression returned by the value transformer. Used as an alternative for 'return' statements in the code. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the value transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the value transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the value transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the value transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the value transformer uses. Only required if the function wants to use a state store. Items (string) WindowBySessionOperation (object) : Operation to window messages by session, configured by an inactivity gap. Cannot contain additional properties. grace : (optional) (Tumbling, Hopping) The grace period, during which out-of-order records can still be processed. Any of integer string inactivityGap : The inactivity gap, below which two messages are considered to be of the same session. Any of integer string name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"windowBySession\"] . WindowByTimeOperationWithHoppingWindow (object) : Operation to window records based on time criteria. Cannot contain additional properties. advanceBy : The amount of time to increase time windows by. Any of integer string duration : The duration of time windows. Any of integer string grace : (optional) The grace period, during which out-of-order records can still be processed. Any of integer string name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"windowByTime\"] . windowType : The windowType of the time window. Must be one of: [\"hopping\"] . WindowByTimeOperationWithSlidingWindow (object) : Operation to window records based on time criteria. Cannot contain additional properties. grace : (optional) The grace period, during which out-of-order records can still be processed. Any of integer string name (string) : (optional) The name of the operation processor. timeDifference : The maximum amount of time difference between two records. Any of integer string type : The type of the operation. Must be one of: [\"windowByTime\"] . windowType : The windowType of the time window. Must be one of: [\"sliding\"] . WindowByTimeOperationWithTumblingWindow (object) : Operation to window records based on time criteria. Cannot contain additional properties. duration : The duration of time windows. Any of integer string grace : (optional) The grace period, during which out-of-order records can still be processed. Any of integer string name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"windowByTime\"] . windowType : The windowType of the time window. Must be one of: [\"tumbling\"] . WindowStateStoreDefinition (object) : Definition of a window state store. Cannot contain additional properties. caching (boolean) : (optional) \"true\" if changed to the window store need to be buffered and periodically released, \"false\" to emit all changes directly. keyType (string) : (optional) The key type of the window store. logging (boolean) : (optional) \"true\" if a changelog topic should be set up on Kafka for this window store, \"false\" otherwise. name (string) : (optional) The name of the window store. If this field is not defined, then the name is derived from the context. persistent (boolean) : (optional) \"true\" if this window store needs to be stored on disk, \"false\" otherwise. retainDuplicates (boolean) : (optional) Whether or not to retain duplicates. retention : (optional) The duration for which elements in the window store are retained. Any of integer string timestamped (boolean) : (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise. type : The type of the state store. Must be one of: [\"window\"] . valueType (string) : (optional) The value type of the window store. windowSize : (optional) Size of the windows (cannot be negative). Any of integer string WindowStateStoreDefinitionWithImplicitStoreType (object) : Definition of a window state store. Cannot contain additional properties. caching (boolean) : (optional) \"true\" if changed to the window store need to be buffered and periodically released, \"false\" to emit all changes directly. keyType (string) : (optional) The key type of the window store. logging (boolean) : (optional) \"true\" if a changelog topic should be set up on Kafka for this window store, \"false\" otherwise. name (string) : (optional) The name of the window store. If this field is not defined, then the name is derived from the context. persistent (boolean) : (optional) \"true\" if this window store needs to be stored on disk, \"false\" otherwise. retainDuplicates (boolean) : (optional) Whether or not to retain duplicates. retention : (optional) The duration for which elements in the window store are retained. Any of integer string timestamped (boolean) : (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise. type : The type of the state store. Must be one of: [\"window\"] . valueType (string) : (optional) The value type of the window store. windowSize : (optional) Size of the windows (cannot be negative). Any of integer string","title":"Definitions"},{"location":"notations/","text":"Notations Table of Contents Introduction Avro CSV JSON SOAP XML Introduction KSML is able to express its internal data types in a number of external representations. Internally these are called notations . The different notations are described below. AVRO Avro types are supported through the \"avro\" prefix in types. The notation is avro:schema , where schema is the schema fqdn, or just the schema name itself. On Kafka topics, Avro types are serialized in binary format. Internally they are represented as structs. Examples: avro:SensorData avro:io.axual.ksml.example.SensorData Note: when referencing an AVRO schema, you have to ensure that the respective schema file can be found in the KSML working directory and has the .avsc file extension. CSV Comma-separated values are supported through the \"csv\" prefix in types. The notation is csv:schema , where schema is the schema fqdn, or just the schema name itself. On Kafka topics, CSV types are serialized as string . Internally they are represented as structs. Examples: csv:SensorData csv:io.axual.ksml.example.SensorData Note: when referencing an CSV schema, you have to ensure that the respective schema file can be found in the KSML working directory and has the .csv file extension. JSON JSON types are supported through the \"json\" prefix in types. The notation is json:schema , where schema is the schema fqdn, or just the schema name itself. On Kafka topics, JSON types are serialized as string . Internally they are represented as structs or lists. Examples: json:SensorData json:io.axual.ksml.example.SensorData If you want to use JSON without a schema, you can leave out the colon and schema name: json Note: when referencing an JSON schema, you have to ensure that the respective schema file can be found in the KSML working directory and has the .json file extension. SOAP SOAP is supported through built-in serializers and deserializers. The representation on Kafka will always be string . Internally SOAP objects are structs with their own schema. Field names are derived from the SOAP standards. XML XML is supported through built-in serializers and deserializers. The representation on Kafka will always be string . Internally XML objects are structs. Examples: xml:SensorData xml:io.axual.ksml.example.SensorData Note: when referencing an XML schema, you have to ensure that the respective schema file can be found in the KSML working directory and has the .xsd file extension.","title":"Notations"},{"location":"notations/#notations","text":"","title":"Notations"},{"location":"notations/#table-of-contents","text":"Introduction Avro CSV JSON SOAP XML","title":"Table of Contents"},{"location":"notations/#introduction","text":"KSML is able to express its internal data types in a number of external representations. Internally these are called notations . The different notations are described below.","title":"Introduction"},{"location":"notations/#avro","text":"Avro types are supported through the \"avro\" prefix in types. The notation is avro:schema , where schema is the schema fqdn, or just the schema name itself. On Kafka topics, Avro types are serialized in binary format. Internally they are represented as structs. Examples: avro:SensorData avro:io.axual.ksml.example.SensorData Note: when referencing an AVRO schema, you have to ensure that the respective schema file can be found in the KSML working directory and has the .avsc file extension.","title":"AVRO"},{"location":"notations/#csv","text":"Comma-separated values are supported through the \"csv\" prefix in types. The notation is csv:schema , where schema is the schema fqdn, or just the schema name itself. On Kafka topics, CSV types are serialized as string . Internally they are represented as structs. Examples: csv:SensorData csv:io.axual.ksml.example.SensorData Note: when referencing an CSV schema, you have to ensure that the respective schema file can be found in the KSML working directory and has the .csv file extension.","title":"CSV"},{"location":"notations/#json","text":"JSON types are supported through the \"json\" prefix in types. The notation is json:schema , where schema is the schema fqdn, or just the schema name itself. On Kafka topics, JSON types are serialized as string . Internally they are represented as structs or lists. Examples: json:SensorData json:io.axual.ksml.example.SensorData If you want to use JSON without a schema, you can leave out the colon and schema name: json Note: when referencing an JSON schema, you have to ensure that the respective schema file can be found in the KSML working directory and has the .json file extension.","title":"JSON"},{"location":"notations/#soap","text":"SOAP is supported through built-in serializers and deserializers. The representation on Kafka will always be string . Internally SOAP objects are structs with their own schema. Field names are derived from the SOAP standards.","title":"SOAP"},{"location":"notations/#xml","text":"XML is supported through built-in serializers and deserializers. The representation on Kafka will always be string . Internally XML objects are structs. Examples: xml:SensorData xml:io.axual.ksml.example.SensorData Note: when referencing an XML schema, you have to ensure that the respective schema file can be found in the KSML working directory and has the .xsd file extension.","title":"XML"},{"location":"operations/","text":"Operations Table of Contents Introduction Operations aggregate cogroup convertKey convertKeyValue convertValue count filter filterNot flatMap flatMapValues groupBy groupByKey join leftJoin map mapKey mapValue mapValues merge outerJoin peek reduce repartition selectKey suppress toStream transformKey transformKeyValue transformKeyValueToKeyValueList transformKeyValueToValueList transformValue windowBySession windowByTime Sink Operations as branch forEach print to toTopicNameExtractor Introduction Pipelines in KSML have a beginning, a middle and (optionally) an end. Operations form the middle part of pipelines. They are modeled as separate YAML entities, where each operation takes input from the previous operation and applies its own logic. The returned stream then serves as input for the next operation. Transform Operations Transformations are operations that take an input stream and convert it to an output stream. This section lists all supported transformations. Each one states the type of stream it returns. Parameter Value Type Description name string The name of the operation. Note that not all combinations of output/input streams are supported by Kafka Streams. The user that writes the KSML definition needs to make sure that streams that result from one operation can actually serve as input to the next. KSML does type checking and will exit with an error when operations that can not be chained together are listed after another in the KSML definition. aggregate This operation aggregates multiple values into a single one by repeatedly calling an aggregator function. It can operate on a range of stream types. Stream Type Returns Parameter Value Type Required Description KGroupedStream <K,V> KTable <K,VR> store Store configuration No An optional Store configuration, should be of type keyValue . initializer Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type VR . aggregator Inline or reference Yes An Aggregator function, which takes a key of type K , a value of type V and aggregatedValue of type VR . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type VR . KGroupedTable <K,V> KTable <K,VR> store Store configuration No An optional Store configuration, should be of type keyValue . initializer Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type VR . adder Inline or reference Yes An Aggregator function, which takes a key of type K , a value of type V and aggregatedValue of type VR . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type VR . subtractor Inline or reference Yes An Aggregator function, which takes a key of type K , a value of type V and aggregatedValue of type VR . It should remove the key/value from the previously calculated aggregateValue and return a new aggregate value of type VR . SessionWindowedKStream <K,V> KTable <Windowed<K>,VR> store Store configuration No An optional Store configuration, should be of type session . initializer Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type VR . aggregator Inline or reference Yes An Aggregator function, which takes a key of type K , a value of type V and aggregatedValue of type VR . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type VR . merger Inline or reference Yes A Merger function, which takes a key of type K , and two values value1 and value2 of type V . The return value is the merged result, also of type V . [TimeWindowedKStreamObject] <K,V> KTable <Windowed<K>,VR> store Store configuration No An optional Store configuration, should be of type window . initializer Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type VR . aggregator Inline or reference Yes An Aggregator function, which takes a key of type K , a value of type V and aggregatedValue of type VR . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type VR . [CogroupedKStream] <K,V> KTable <K,VR> store Store configuration No An optional Store configuration, should be of type keyValue . initializer Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type VR . SessionWindowedCogroupedKStream <K,V> KTable <Windowed<K>,VR> store Store configuration No An optional Store configuration, should be of type session . initializer Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type VR . merger Inline or reference Yes A Merger function, which takes a key of type K , and two values value1 and value2 of type V . The return value is the merged result, also of type V . TimeWindowedCogroupedKStream <K,V> KTable <Windowed<K>,VR> store Store configuration No An optional Store configuration, should be of type window . initializer Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type VR . Example: from: input_stream via: - type: groupBy mapper: my_mapper_function - type: aggregate initializer: expression: 0 aggregator: expression: aggregatedValue + value - type: toStream to: output_stream cogroup This operation cogroups multiple values into a single one by repeatedly calling an aggregator function. It can operate on a range of stream types. Stream Type Returns Parameter Value Type Required Description KGroupedStream <K,V> [CogroupedKStream] <K,VR> aggregator Inline or reference Yes An Aggregator function, which takes a key of type K , a value of type V and aggregatedValue of type VR . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type VR . [CogroupedKStream] <K,V> n/a n/a n/a n/a This method is currently not supported in KSML. Example: from: input_stream via: - type: groupBy mapper: my_mapper_function - type: cogroup aggregator: expression: aggregatedValue + value - type: toStream to: output_stream Note: this operation was added to KSML for completion purposes, but is not considered ready or fully functional. Feel free to experiment, but don't rely on this in production. Syntax changes may occur in future KSML releases. convertKey This built-in operation takes a message and converts the key into a given type. Stream Type Returns Parameter Value Type Description KStream <K,V> KStream <KR,V> into string The type to convert the key into. Conversion to KR is done by KSML. Example: from: topic: input_stream keyType: string valueType: string via: - type: convertKey into: json to: output_stream convertKeyValue This built-in operation takes a message and converts the key and value into a given type. Stream Type Returns Parameter Value Type Description KStream <K,V> KStream <KR,VR> into string The type to convert the key and value into. Conversion of key into KR and value into VR is done by KSML. Example: from: topic: input_stream keyType: string valueType: string via: - type: convertKeyValue into: (json,xml) to: output_stream convertValue This built-in operation takes a message and converts the value into a given type. Stream Type Returns Parameter Value Type Description KStream <K,V> KStream <K,VR> into string The type to convert the value into. Conversion of value into VR is done by KSML. Example: from: topic: input_stream keyType: string valueType: string via: - type: convertValue into: xml to: output_stream count This operation counts the number of messages and returns a table multiple values into a single one by repeatedly calling an aggregator function. It can operate on a range of stream types. Stream Type Returns Parameter Value Type Required Description KGroupedStream <K,V> KTable <K,Long> store Store configuration No An optional Store configuration, should be of type keyValue . KGroupedTable <K,V> KTable <K,Long> store Store configuration No An optional Store configuration, should be of type keyValue . SessionWindowedKStream <K,V> KTable <Windowed<K>,Long> store Store configuration No An optional Store configuration, should be of type session . [TimeWindowedKStreamObject] <K,V> KTable <Windowed<K>,Long> store Store configuration No An optional Store configuration, should be of type window . Example: from: input_stream via: - type: groupBy mapper: my_mapper_function - type: count - type: toStream to: output_stream filter Filter all incoming messages according to some predicate. The predicate function is called for every message. Only when the predicate returns true , then the message will be sent to the output stream. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <K,V> if Yes Inline or reference A Predicate function, which returns True if the message can pass the filter, False otherwise. KTable <K,V> KTable <K,V> store Store configuration No An optional Store configuration, should be of type keyValue . if Yes Inline or reference A Predicate function, which returns True if the message can pass the filter, False otherwise. Example: from: input_stream via: - type: filter if: expression: key.startswith('a') to: output_stream filterNot This operation works exactly like filter , but negates all predicates before applying them. That means messages for which the predicate returns False are accepted, while those that the predicate returns True for are filtered out. See filter for details on how to implement. flatMap This operation takes a message and transforms it into zero, one or more new messages, which may have different key and value types than the source. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <KR,VR> mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V . The return type should be a list of type [(KR,VR)] containing a list of transformed key and value pairs. Example: from: input_stream via: - type: flatMap mapper: expression: [ (key,value), (key,value) ] # duplicate all incoming messages to: output_stream flatMapValues This operation takes a message and transforms it into zero, one or more new values, which may have different value types than the source. Every entry in the result list is combined with the source key and produced on the output stream. Stream Type Returns Parameter Value Type Description KStream <K,V> KStream <K,VR> mapper Inline or reference A KeyValueMapper function, which takes a key of type K and a value of type V . The return type should be a list of type [VR] containing a list of transformed value s. Example: from: input_stream via: - type: flatMapValues mapper: expression: [ value+1, value+2, value+3 ] # creates 3 new messages [key,VR] for every input message to: output_stream groupBy Group the records of a stream by value resulting from a KeyValueMapper. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KGroupedStream <KR,V> store Store configuration No An optional Store configuration, should be of type keyValue . mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V and returns a value of type KR to group the stream by. KTable <K,V> KGroupedTable <KR,V> store Store configuration No An optional Store configuration, should be of type keyValue . mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V and returns a value of type KR to group the stream by. Example: from: input_stream via: - type: groupBy mapper: expression: value[\"some_field\"] resultType: string - type: aggregate initializer: expression: 0 aggregator: expression: value1+value2 - type: toStream to: output_stream groupByKey Group the records of a stream by the stream's key. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KGroupedStream <K,V> store Store configuration No An optional Store configuration, should be of type keyValue . Example: from: input_stream via: - type: groupByKey - type: aggregate initializer: expression: 0 aggregator: expression: value1+value2 - type: toStream to: output_stream join Join records of this stream with another stream's records using inner join. The join is computed on the records' key with join predicate thisStream.key == otherStream.key . If both streams are not tables, then their timestamps need to be close enough as defined by timeDifference. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <K,VR> store Store configuration No An optional Store configuration, should be of type window . stream string Yes The name of the stream to join with. The stream should be of key type K and value type VR . valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a key of type K , and two values value1 and value2 of type V . The return value is the joined value of type VR . timeDifference duration Yes The maximum allowed between two joined records. grace duration No A grace period during with out-of-order to-be-joined records may still arrive. KStream <K,V> KStream <K,VR> table string Yes The name of the table to join with. The table should be of key type K and value type VO . valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a value1 of type V from the source table and a value2 of type VO from the join table. The return value is the joined value of type VR . grace duration No A grace period during with out-of-order to-be-joined records may still arrive. KStream <K,V> KStream <K,VR> globalTable string Yes The name of the global table to join with. The global table should be of key type GK and value type GV . mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V . The return value is the key of type GK of the records from the GlobalTable to join with. valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a key of type K , and two values value1 and value2 of type V . The return value is the joined value of type VR . KTable <K,V> KTable <K,VR> store Store configuration No The Store configuration, should be of type keyValue . table string Yes The name of the table to join with. The table should be of key type K and value type VO . foreignKeyExtractor Inline or reference No A [ForeignKeyExtractor] function, which takes a value of type V , which needs to be converted into the key type KO of the table to join with. valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a value1 of type V from the source table and a value2 of type VO from the join table. The return value is the joined value of type VR . partitioner Inline or reference No A [Partitioner] function, which partitions the records on the primary stream. otherPartitioner Inline or reference No A [Partitioner] function, which partitions the records on the join table. Example: from: input_stream via: - type: join stream: second_stream valueJoiner: my_key_value_mapper timeDifference: 1m to: output_stream leftJoin Join records of this stream with another stream's records using left join. The join is computed on the records' key with join predicate thisStream.key == otherStream.key . If both streams are not tables, then their timestamps need to be close enough as defined by timeDifference. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <K,VR> store Store configuration No An optional Store configuration, should be of type window . stream string Yes The name of the stream to join with. The stream should be of key type K and value type VR . valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a key of type K , and two values value1 and value2 of type V . The return value is the joined value of type VR . timeDifference duration Yes The maximum allowed between two joined records. grace duration No A grace period during with out-of-order to-be-joined records may still arrive. KStream <K,V> KStream <K,VR> table string Yes The name of the table to join with. The table should be of key type K and value type VO . valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a value1 of type V from the source table and a value2 of type VO from the join table. The return value is the joined value of type VR . grace duration No A grace period during with out-of-order to-be-joined records may still arrive. KStream <K,V> KStream <K,VR> globalTable string Yes The name of the global table to join with. The global table should be of key type GK and value type GV . mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V . The return value is the key of type GK of the records from the GlobalTable to join with. valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a key of type K , and two values value1 and value2 of type V . The return value is the joined value of type VR . KTable <K,V> KTable <K,VR> store Store configuration No The Store configuration, should be of type keyValue . table string Yes The name of the table to join with. The table should be of key type K and value type VO . foreignKeyExtractor Inline or reference No A [ForeignKeyExtractor] function, which takes a value of type V , which needs to be converted into the key type KO of the table to join with. valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a value1 of type V from the source table and a value2 of type VO from the join table. The return value is the joined value of type VR . partitioner Inline or reference No A [Partitioner] function, which partitions the records on the primary stream. otherPartitioner Inline or reference No A [Partitioner] function, which partitions the records on the join table. Example: from: input_stream via: - type: leftJoin stream: second_stream valueJoiner: my_key_value_mapper timeDifference: 1m to: output_stream map This operation takes a message and transforms the key and value into a new key and value, which can each have a different type than the source message key and value. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <KR,VR> mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V . The return type should be a tuple of type (KR,VR) containing the transformed key and value . Example: from: input_stream via: - type: map mapper: expression: (str(key), str(value)) # convert key and value from source type to string to: output_stream mapKey This is an alias for selectKey . Example: from: input_stream via: - type: mapKey mapper: expression: str(key) # convert key from source type to string to: output_stream mapValue This is an alias for mapValues . Example: from: input_stream via: - type: mapValue mapper: expression: str(value) # convert value from source type to String to: output_stream mapValues This operation takes a message and transforms its value to a new value, which may have different value type than the source. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <K,VR> mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V . The return type should be a value of type VR . Example: from: input_stream via: - type: mapValues mapper: expression: str(value) # convert value from source type to String to: output_stream merge Merge this stream and the given stream into one larger stream. There is no ordering guarantee between records from this stream and records from the provided stream in the merged stream. Relative order is preserved within each input stream though (ie, records within one input stream are processed in order). Stream Type Returns Parameter Value Type Description KStream <K,V> KStream <K,V> stream string The name of the stream to merge with. Example: from: input_stream via: - type: merge stream: second_stream to: output_stream outerJoin Join records of this stream with another stream's records using outer join. The join is computed on the records' key with join predicate thisStream.key == otherStream.key . If both streams are not tables, then their timestamps need to be close enough as defined by timeDifference. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <K,VR> store Store configuration No An optional Store configuration, should be of type window . stream string Yes The name of the stream to join with. The stream should be of key type K and value type VR . valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a key of type K , and two values value1 and value2 of type V . The return value is the joined value of type VR . timeDifference duration Yes The maximum allowed between two joined records. grace duration No A grace period during with out-of-order to-be-joined records may still arrive. KTable <K,V> KStream <K,VR> store Store configuration No An optional Store configuration, should be of type keyValue . table string Yes The name of the table to join with. The table should be of key type K and value type VO . valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a value1 of type V from the source table and a value2 of type VO from the join table. The return value is the joined value of type VR . Example: from: input_stream via: - type: outerJoin stream: second_stream valueJoiner: my_key_value_mapper timeDifference: 1m to: output_stream peek Perform an action on each record of a stream. This is a stateless record-by-record operation. Peek is a non-terminal operation that triggers a side effect (such as logging or statistics collection) and returns an unchanged stream. Stream Type Returns Parameter Value Type Description KStream <K,V> KStream <K,V> forEach Inline or reference The [ForEach] function that will be called for every message, receiving arguments key of type K and value of type V . Example: from: input_stream via: - type: peek forEach: print_key_and_value to: output_stream reduce Combine the values of records in this stream by the grouped key. Records with null key or value are ignored. Combining implies that the type of the aggregate result is the same as the type of the input value, similar to aggregate(Initializer, Aggregator) . Stream Type Returns Parameter Value Type Required Description KGroupedStream <K,V> KTable <K,V> store Store configuration No An optional Store configuration, should be of type keyValue . reducer Inline or reference Yes A Reducer function, which takes a key of type K , a value of type V and aggregatedValue of type V . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type V . KGroupedTable <K,V> KTable <K,V> store Store configuration No An optional Store configuration, should be of type keyValue . adder Inline or reference Yes A Reducer function, which takes a key of type K , a value of type V and aggregatedValue of type V . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type V . subtractor Inline or reference Yes A Reducer function, which takes a key of type K , a value of type V and aggregatedValue of type V . It should remove the key/value from the previously calculated aggregateValue and return a new aggregate value of type V . SessionWindowedKStream <K,V> KTable <Windowed<K>,V> store Store configuration No An optional Store configuration, should be of type session . reducer Inline or reference Yes A Reducer function, which takes a key of type K , a value of type V and aggregatedValue of type V . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type V . [TimeWindowedKStreamObject] <K,V> KTable <Windowed<K>,V> store Store configuration No An optional Store configuration, should be of type window . reducer Inline or reference Yes A Reducer function, which takes a key of type K , a value of type V and aggregatedValue of type V . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type V . Example: from: input_stream via: - type: groupBy mapper: my_mapper_function - type: aggregate initializer: expression: 0 aggregator: expression: aggregatedValue + value - type: toStream to: output_stream Example: [ yaml ] ---- from: input_stream via: - type: groupBy mapper: my_mapper_function - type: reduce reducer: expression: value1+value2 - type: toStream to: output_stream repartition Materialize this stream to an auto-generated repartition topic with a given number of partitions, using a custom partitioner. Similar to auto-repartitioning, the topic will be created with infinite retention time and data will be automatically purged. The topic will be named as \"${applicationId}- -repartition\". Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <K,V> numberOfPartitions integer No The number of partitions of the repartitioned topic. partitioner Inline or reference No A custom [Partitioner] function to partition records. Example: from: input_stream via: - type: repartition name: my_partitioner numberOfPartitions: 3 partitioner: my_own_partitioner - type: peek forEach: print_key_and_value - type: toStream to: output_stream selectKey This operation takes a message and transforms the key into a new key, which may have a different type. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <KR,V> mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V . The return value is the key of resulting stream, which is of type KR . Example: from: input_stream via: - type: selectKey mapper: expression: str(key) # convert key from source type to string to: output_stream suppress Suppress some updates from this changelog stream, determined by the supplied Suppressed configuration. When windowCloses is selected and no further restrictions are provided, then this is interpreted as Suppressed.untilWindowCloses(unbounded()) . Stream Type Returns Parameter Value Type Required Description KTable <K,V> KTable <K,V> until string Yes This value can either be timeLimit or windowCloses . Note that timeLimit suppression works on any stream, while windowCloses suppression works only on Windowed streams. For the latter, see [windowByTime] or [windowBySession]. duration string No The Duration to suppress updates (only when until == timeLimit ) maxBytes int No The maximum number of bytes to suppress updates maxRecords int No The maximum number of records to suppress updates bufferFullStrategy string No Can be one of emitEarlyWhenFull , shutdownWhenFull Example: from: input_table via: - type: suppress until: timeLimit duration: 30s maxBytes: 128000 maxRecords: 10000 bufferFullStrategy: emitEarlyWhenFull - type: peek forEach: print_key_and_value - type: toStream to: output_stream toStream Convert a KTable into a KStream object. Stream Type Returns Parameter Value Type Required Description KTable <K,V> KStream <KR,V> mapper Inline or reference No A KeyValueMapper function, which takes a key of type K and a value of type V . The return value is the key of resulting stream, which is of type KR . If no mapper is provided, then keys remain unchanged. Example: from: input_table via: - type: toStream to: output_stream transformKey This is an alias for selectKey . Example: from: input_stream via: - type: transformKey mapper: expression: str(key) # convert key from source type to string to: output_stream transformKeyValue This is an alias for map . Example: from: input_stream via: - type: transformKeyValue mapper: expression: (str(key), str(value)) # convert key and value from source type to string to: output_stream transformKeyValueToKeyValueList This is an alias for flatMap . Example: from: input_stream via: - type: transformKeyValueToKeyValueList mapper: expression: [ (key,value), (key,value) ] # duplicate all incoming messages to: output_stream transformKeyValueToValueList This is an alias for flapMapValues . Example: from: input_stream via: - type: transformKeyValueToValueList mapper: expression: [ value+1, value+2, value+3 ] # creates 3 new messages [key,VR] for every input message to: output_stream transformMetadata This operation takes a message and transforms its value to a new value, which may have different value type than the source. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <K,VR> mapper Inline or reference Yes A [MetadataTransformer] function that converts the metadata (Kafka headers, timestamp) of every record in the stream. It gets a metadata object as input and should return the same type, but potentially with modified fields. Example: from: input_stream via: - type: transformValue mapper: expression: str(value) # convert value from source type to String to: output_stream transformValue This is an alias for mapValues . Example: from: input_stream via: - type: transformValue mapper: expression: str(value) # convert value from source type to String to: output_stream windowBySession Create a new windowed KStream instance that can be used to perform windowed aggregations. For more details on the different types of windows, please refer to [WindowTypes]|[this page]. Stream Type Returns Parameter Value Type Required Description KGroupedStream <K,V> SessionWindowedKStream <K,V> inactivityGap Duration Yes The maximum inactivity gap with which keys are grouped. grace Duration No The grace duration allowing for out-of-order messages to still be associated with the right session. [CogroupedKStream] <K,V> SessionWindowedCogroupedKStream <K,V> inactivityGap Duration Yes The maximum inactivity gap with which keys are grouped. grace Duration No The grace duration allowing for out-of-order messages to still be associated with the right session. Example: from: input_stream via: - type: groupBy mapper: my_mapper_function - type: windowedBySession inactivityGap: 1h grace: 5m - type: reduce reducer: my_reducer_function - type: toStream to: output_stream windowByTime Create a new windowed KStream instance that can be used to perform windowed aggregations. For more details on the different types of windows, please refer to [WindowTypes]|[this page]. Stream Type Returns Parameter Value Type Description KGroupedStream <K,V> TimeWindowedKStream <K,V> windowType string Fixed value sliding . timeDifference Duration The time difference parameter for the [SlidingWindows] object. grace Duration (Optional) The grace parameter for the [SlidingWindows] object. KGroupedStream <K,V> TimeWindowedKStream <K,V> windowType string Fixed value hopping . advanceBy Duration The amount by which each window is advanced. If this value is not specified, then it will be equal to duration , which gives tumbling windows. If you make this value smaller than duration you will get hopping windows. grace Duration (Optional) The grace parameter for the [TimeWindows] object. KGroupedStream <K,V> TimeWindowedKStream <K,V> windowType string Fixed value tumbling . duration Duration The duration parameter for the [TimeWindows] object. grace Duration (Optional) The grace parameter for the [TimeWindows] object. [CogroupedKStream] <K,V> TimeWindowedCogroupedKStream <K,V> windowType string Fixed value sliding . timeDifference Duration The time difference parameter for the [SlidingWindows] object. grace Duration (Optional) The grace parameter for the [SlidingWindows] object. [CogroupedKStream] <K,V> TimeWindowedCogroupedKStream <K,V> windowType string Fixed value hopping . advanceBy Duration The amount by which each window is advanced. If this value is not specified, then it will be equal to duration , which gives tumbling windows. If you make this value smaller than duration you will get hopping windows. grace Duration (Optional) The grace parameter for the [TimeWindows] object. [CogroupedKStream] <K,V> TimeWindowedCogroupedKStream <K,V> windowType string Fixed value tumbling . duration Duration The duration parameter for the [TimeWindows] object. grace Duration (Optional) The grace parameter for the [TimeWindows] object. Example: from: input_stream via: - type: groupBy mapper: my_mapper_function - type: windowedBy windowType: time duration: 1h advanceBy: 15m grace: 5m - type: reduce reducer: my_reducer_function - type: toStream to: output_stream Sink Operations as Pipelines closed of with as can be referred by other pipelines as their starting reference. This allows for a common part of processing logic to be placed in its own pipeline in KSML, serving as an intermediate result. Applies to Value Type Required Description Any pipeline <K,V> string Yes The name under which the pipeline result can be referenced by other pipelines. Example: pipelines: first: from: some_source_topic via: - type: ... as: first_pipeline second: from: first_pipeline via: - type: ... to: ... Here, the first pipeline ends by sending its output to a stream internally called first_pipeline . This stream is used as input for the second pipeline. branch Branches out messages from the input stream into several branches based on predicates. Each branch is defined as a list item below the branch operation. Branch predicates are defined using the if keyword. Messages are only processed by one of the branches, namely the first one for which the predicate returns true . Applies to Value Type Required Description KStream <K,V> List of branch definitions Yes See for description of branch definitions below. Branches in KSML are nested pipelines, which are parsed without the requirement of a source attribute. Each branch accepts the following parameters: Branch element Value Type Required Description if Inline Predicate or reference No The Predicate function that determines if the message is sent down this branch, or is passed on to the next branch in line. Inline All pipeline parameters, see [Pipeline] Yes The inlined pipeline describes the topology of the specific branch. Example: from: some_source_topic branch: - if: expression: value['color'] == 'blue' to: ksml_sensordata_blue - if: expression: value['color'] == 'red' to: ksml_sensordata_red - forEach: code: | print('Unknown color sensor: '+value[\"color\"]) In this example, the first two branches are entered if the respective predicate matches (the color attribute of value matches a certain color). If the predicate returns false , then the next predicate/branch is tried. Only the last branch in the list can be a sink operation. forEach This sends each message to a custom defined function. This function is expected to handle each message as its final step. The function does not (need to) return anything. Applies to Value Type Description KStream <K,V> Inline or reference The [ForEach] function that is called for every record on the source stream. Its arguments are key of type K and value of type V . Examples: forEach: my_foreach_function forEach: code: print(value) print This sends each message to a custom defined print function. This function is expected to handle each message as the final in the pipeline. The function does not (need to) return anything. As target, you can specify a filename. If none is specified, then all messages are printed to stdout. Applies to Parameter Value Type Required Description KStream <K,V> filename string No The filename to output records to. If nothing is specified, then messages will be printed on stdout. label string No A label to attach to every output record. mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V . The return value should be of type string and is sent to the specified file or stdout. Examples: from: source via: - type: ... print: filename: file.txt mapper: expression: \"record value: \" + str(value) to Messages are sent directly to a named Stream . Applies to Value Type Required Description KStream <K,V> Inline [Topic] or reference to a stream, table or global table Yes The name of a defined stream . Examples: to: my_target_topic from: source via: - type: ... to: topic: my_target_topic keyType: someType valueType: someOtherType partitioner: expression: hash_of(key) toTopicNameExtractor Messages are passed onto a user function, which returns the name of the topic that message needs to be sent to. This operation acts as a Sink and is always the last operation in a pipeline . Applies to Value Type Required Description KStream <K,V> Inline or reference Yes The [TopicNameExtractor] function that is called for every message and returns the topic name to which the message shall be written. Examples: toTopicNameExtractor: my_extractor_function toTopicNameExtractor: code: | if key == 'sensor1': return 'ksml_sensordata_sensor1' elif key == 'sensor2': return 'ksml_sensordata_sensor2' else: return 'ksml_sensordata_sensor0'","title":"Operations"},{"location":"operations/#operations","text":"","title":"Operations"},{"location":"operations/#table-of-contents","text":"Introduction Operations aggregate cogroup convertKey convertKeyValue convertValue count filter filterNot flatMap flatMapValues groupBy groupByKey join leftJoin map mapKey mapValue mapValues merge outerJoin peek reduce repartition selectKey suppress toStream transformKey transformKeyValue transformKeyValueToKeyValueList transformKeyValueToValueList transformValue windowBySession windowByTime Sink Operations as branch forEach print to toTopicNameExtractor","title":"Table of Contents"},{"location":"operations/#introduction","text":"Pipelines in KSML have a beginning, a middle and (optionally) an end. Operations form the middle part of pipelines. They are modeled as separate YAML entities, where each operation takes input from the previous operation and applies its own logic. The returned stream then serves as input for the next operation.","title":"Introduction"},{"location":"operations/#transform-operations","text":"Transformations are operations that take an input stream and convert it to an output stream. This section lists all supported transformations. Each one states the type of stream it returns. Parameter Value Type Description name string The name of the operation. Note that not all combinations of output/input streams are supported by Kafka Streams. The user that writes the KSML definition needs to make sure that streams that result from one operation can actually serve as input to the next. KSML does type checking and will exit with an error when operations that can not be chained together are listed after another in the KSML definition.","title":"Transform Operations"},{"location":"operations/#aggregate","text":"This operation aggregates multiple values into a single one by repeatedly calling an aggregator function. It can operate on a range of stream types. Stream Type Returns Parameter Value Type Required Description KGroupedStream <K,V> KTable <K,VR> store Store configuration No An optional Store configuration, should be of type keyValue . initializer Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type VR . aggregator Inline or reference Yes An Aggregator function, which takes a key of type K , a value of type V and aggregatedValue of type VR . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type VR . KGroupedTable <K,V> KTable <K,VR> store Store configuration No An optional Store configuration, should be of type keyValue . initializer Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type VR . adder Inline or reference Yes An Aggregator function, which takes a key of type K , a value of type V and aggregatedValue of type VR . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type VR . subtractor Inline or reference Yes An Aggregator function, which takes a key of type K , a value of type V and aggregatedValue of type VR . It should remove the key/value from the previously calculated aggregateValue and return a new aggregate value of type VR . SessionWindowedKStream <K,V> KTable <Windowed<K>,VR> store Store configuration No An optional Store configuration, should be of type session . initializer Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type VR . aggregator Inline or reference Yes An Aggregator function, which takes a key of type K , a value of type V and aggregatedValue of type VR . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type VR . merger Inline or reference Yes A Merger function, which takes a key of type K , and two values value1 and value2 of type V . The return value is the merged result, also of type V . [TimeWindowedKStreamObject] <K,V> KTable <Windowed<K>,VR> store Store configuration No An optional Store configuration, should be of type window . initializer Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type VR . aggregator Inline or reference Yes An Aggregator function, which takes a key of type K , a value of type V and aggregatedValue of type VR . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type VR . [CogroupedKStream] <K,V> KTable <K,VR> store Store configuration No An optional Store configuration, should be of type keyValue . initializer Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type VR . SessionWindowedCogroupedKStream <K,V> KTable <Windowed<K>,VR> store Store configuration No An optional Store configuration, should be of type session . initializer Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type VR . merger Inline or reference Yes A Merger function, which takes a key of type K , and two values value1 and value2 of type V . The return value is the merged result, also of type V . TimeWindowedCogroupedKStream <K,V> KTable <Windowed<K>,VR> store Store configuration No An optional Store configuration, should be of type window . initializer Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type VR . Example: from: input_stream via: - type: groupBy mapper: my_mapper_function - type: aggregate initializer: expression: 0 aggregator: expression: aggregatedValue + value - type: toStream to: output_stream","title":"aggregate"},{"location":"operations/#cogroup","text":"This operation cogroups multiple values into a single one by repeatedly calling an aggregator function. It can operate on a range of stream types. Stream Type Returns Parameter Value Type Required Description KGroupedStream <K,V> [CogroupedKStream] <K,VR> aggregator Inline or reference Yes An Aggregator function, which takes a key of type K , a value of type V and aggregatedValue of type VR . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type VR . [CogroupedKStream] <K,V> n/a n/a n/a n/a This method is currently not supported in KSML. Example: from: input_stream via: - type: groupBy mapper: my_mapper_function - type: cogroup aggregator: expression: aggregatedValue + value - type: toStream to: output_stream Note: this operation was added to KSML for completion purposes, but is not considered ready or fully functional. Feel free to experiment, but don't rely on this in production. Syntax changes may occur in future KSML releases.","title":"cogroup"},{"location":"operations/#convertkey","text":"This built-in operation takes a message and converts the key into a given type. Stream Type Returns Parameter Value Type Description KStream <K,V> KStream <KR,V> into string The type to convert the key into. Conversion to KR is done by KSML. Example: from: topic: input_stream keyType: string valueType: string via: - type: convertKey into: json to: output_stream","title":"convertKey"},{"location":"operations/#convertkeyvalue","text":"This built-in operation takes a message and converts the key and value into a given type. Stream Type Returns Parameter Value Type Description KStream <K,V> KStream <KR,VR> into string The type to convert the key and value into. Conversion of key into KR and value into VR is done by KSML. Example: from: topic: input_stream keyType: string valueType: string via: - type: convertKeyValue into: (json,xml) to: output_stream","title":"convertKeyValue"},{"location":"operations/#convertvalue","text":"This built-in operation takes a message and converts the value into a given type. Stream Type Returns Parameter Value Type Description KStream <K,V> KStream <K,VR> into string The type to convert the value into. Conversion of value into VR is done by KSML. Example: from: topic: input_stream keyType: string valueType: string via: - type: convertValue into: xml to: output_stream","title":"convertValue"},{"location":"operations/#count","text":"This operation counts the number of messages and returns a table multiple values into a single one by repeatedly calling an aggregator function. It can operate on a range of stream types. Stream Type Returns Parameter Value Type Required Description KGroupedStream <K,V> KTable <K,Long> store Store configuration No An optional Store configuration, should be of type keyValue . KGroupedTable <K,V> KTable <K,Long> store Store configuration No An optional Store configuration, should be of type keyValue . SessionWindowedKStream <K,V> KTable <Windowed<K>,Long> store Store configuration No An optional Store configuration, should be of type session . [TimeWindowedKStreamObject] <K,V> KTable <Windowed<K>,Long> store Store configuration No An optional Store configuration, should be of type window . Example: from: input_stream via: - type: groupBy mapper: my_mapper_function - type: count - type: toStream to: output_stream","title":"count"},{"location":"operations/#filter","text":"Filter all incoming messages according to some predicate. The predicate function is called for every message. Only when the predicate returns true , then the message will be sent to the output stream. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <K,V> if Yes Inline or reference A Predicate function, which returns True if the message can pass the filter, False otherwise. KTable <K,V> KTable <K,V> store Store configuration No An optional Store configuration, should be of type keyValue . if Yes Inline or reference A Predicate function, which returns True if the message can pass the filter, False otherwise. Example: from: input_stream via: - type: filter if: expression: key.startswith('a') to: output_stream","title":"filter"},{"location":"operations/#filternot","text":"This operation works exactly like filter , but negates all predicates before applying them. That means messages for which the predicate returns False are accepted, while those that the predicate returns True for are filtered out. See filter for details on how to implement.","title":"filterNot"},{"location":"operations/#flatmap","text":"This operation takes a message and transforms it into zero, one or more new messages, which may have different key and value types than the source. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <KR,VR> mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V . The return type should be a list of type [(KR,VR)] containing a list of transformed key and value pairs. Example: from: input_stream via: - type: flatMap mapper: expression: [ (key,value), (key,value) ] # duplicate all incoming messages to: output_stream","title":"flatMap"},{"location":"operations/#flatmapvalues","text":"This operation takes a message and transforms it into zero, one or more new values, which may have different value types than the source. Every entry in the result list is combined with the source key and produced on the output stream. Stream Type Returns Parameter Value Type Description KStream <K,V> KStream <K,VR> mapper Inline or reference A KeyValueMapper function, which takes a key of type K and a value of type V . The return type should be a list of type [VR] containing a list of transformed value s. Example: from: input_stream via: - type: flatMapValues mapper: expression: [ value+1, value+2, value+3 ] # creates 3 new messages [key,VR] for every input message to: output_stream","title":"flatMapValues"},{"location":"operations/#groupby","text":"Group the records of a stream by value resulting from a KeyValueMapper. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KGroupedStream <KR,V> store Store configuration No An optional Store configuration, should be of type keyValue . mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V and returns a value of type KR to group the stream by. KTable <K,V> KGroupedTable <KR,V> store Store configuration No An optional Store configuration, should be of type keyValue . mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V and returns a value of type KR to group the stream by. Example: from: input_stream via: - type: groupBy mapper: expression: value[\"some_field\"] resultType: string - type: aggregate initializer: expression: 0 aggregator: expression: value1+value2 - type: toStream to: output_stream","title":"groupBy"},{"location":"operations/#groupbykey","text":"Group the records of a stream by the stream's key. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KGroupedStream <K,V> store Store configuration No An optional Store configuration, should be of type keyValue . Example: from: input_stream via: - type: groupByKey - type: aggregate initializer: expression: 0 aggregator: expression: value1+value2 - type: toStream to: output_stream","title":"groupByKey"},{"location":"operations/#join","text":"Join records of this stream with another stream's records using inner join. The join is computed on the records' key with join predicate thisStream.key == otherStream.key . If both streams are not tables, then their timestamps need to be close enough as defined by timeDifference. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <K,VR> store Store configuration No An optional Store configuration, should be of type window . stream string Yes The name of the stream to join with. The stream should be of key type K and value type VR . valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a key of type K , and two values value1 and value2 of type V . The return value is the joined value of type VR . timeDifference duration Yes The maximum allowed between two joined records. grace duration No A grace period during with out-of-order to-be-joined records may still arrive. KStream <K,V> KStream <K,VR> table string Yes The name of the table to join with. The table should be of key type K and value type VO . valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a value1 of type V from the source table and a value2 of type VO from the join table. The return value is the joined value of type VR . grace duration No A grace period during with out-of-order to-be-joined records may still arrive. KStream <K,V> KStream <K,VR> globalTable string Yes The name of the global table to join with. The global table should be of key type GK and value type GV . mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V . The return value is the key of type GK of the records from the GlobalTable to join with. valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a key of type K , and two values value1 and value2 of type V . The return value is the joined value of type VR . KTable <K,V> KTable <K,VR> store Store configuration No The Store configuration, should be of type keyValue . table string Yes The name of the table to join with. The table should be of key type K and value type VO . foreignKeyExtractor Inline or reference No A [ForeignKeyExtractor] function, which takes a value of type V , which needs to be converted into the key type KO of the table to join with. valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a value1 of type V from the source table and a value2 of type VO from the join table. The return value is the joined value of type VR . partitioner Inline or reference No A [Partitioner] function, which partitions the records on the primary stream. otherPartitioner Inline or reference No A [Partitioner] function, which partitions the records on the join table. Example: from: input_stream via: - type: join stream: second_stream valueJoiner: my_key_value_mapper timeDifference: 1m to: output_stream","title":"join"},{"location":"operations/#leftjoin","text":"Join records of this stream with another stream's records using left join. The join is computed on the records' key with join predicate thisStream.key == otherStream.key . If both streams are not tables, then their timestamps need to be close enough as defined by timeDifference. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <K,VR> store Store configuration No An optional Store configuration, should be of type window . stream string Yes The name of the stream to join with. The stream should be of key type K and value type VR . valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a key of type K , and two values value1 and value2 of type V . The return value is the joined value of type VR . timeDifference duration Yes The maximum allowed between two joined records. grace duration No A grace period during with out-of-order to-be-joined records may still arrive. KStream <K,V> KStream <K,VR> table string Yes The name of the table to join with. The table should be of key type K and value type VO . valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a value1 of type V from the source table and a value2 of type VO from the join table. The return value is the joined value of type VR . grace duration No A grace period during with out-of-order to-be-joined records may still arrive. KStream <K,V> KStream <K,VR> globalTable string Yes The name of the global table to join with. The global table should be of key type GK and value type GV . mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V . The return value is the key of type GK of the records from the GlobalTable to join with. valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a key of type K , and two values value1 and value2 of type V . The return value is the joined value of type VR . KTable <K,V> KTable <K,VR> store Store configuration No The Store configuration, should be of type keyValue . table string Yes The name of the table to join with. The table should be of key type K and value type VO . foreignKeyExtractor Inline or reference No A [ForeignKeyExtractor] function, which takes a value of type V , which needs to be converted into the key type KO of the table to join with. valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a value1 of type V from the source table and a value2 of type VO from the join table. The return value is the joined value of type VR . partitioner Inline or reference No A [Partitioner] function, which partitions the records on the primary stream. otherPartitioner Inline or reference No A [Partitioner] function, which partitions the records on the join table. Example: from: input_stream via: - type: leftJoin stream: second_stream valueJoiner: my_key_value_mapper timeDifference: 1m to: output_stream","title":"leftJoin"},{"location":"operations/#map","text":"This operation takes a message and transforms the key and value into a new key and value, which can each have a different type than the source message key and value. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <KR,VR> mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V . The return type should be a tuple of type (KR,VR) containing the transformed key and value . Example: from: input_stream via: - type: map mapper: expression: (str(key), str(value)) # convert key and value from source type to string to: output_stream","title":"map"},{"location":"operations/#mapkey","text":"This is an alias for selectKey . Example: from: input_stream via: - type: mapKey mapper: expression: str(key) # convert key from source type to string to: output_stream","title":"mapKey"},{"location":"operations/#mapvalue","text":"This is an alias for mapValues . Example: from: input_stream via: - type: mapValue mapper: expression: str(value) # convert value from source type to String to: output_stream","title":"mapValue"},{"location":"operations/#mapvalues","text":"This operation takes a message and transforms its value to a new value, which may have different value type than the source. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <K,VR> mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V . The return type should be a value of type VR . Example: from: input_stream via: - type: mapValues mapper: expression: str(value) # convert value from source type to String to: output_stream","title":"mapValues"},{"location":"operations/#merge","text":"Merge this stream and the given stream into one larger stream. There is no ordering guarantee between records from this stream and records from the provided stream in the merged stream. Relative order is preserved within each input stream though (ie, records within one input stream are processed in order). Stream Type Returns Parameter Value Type Description KStream <K,V> KStream <K,V> stream string The name of the stream to merge with. Example: from: input_stream via: - type: merge stream: second_stream to: output_stream","title":"merge"},{"location":"operations/#outerjoin","text":"Join records of this stream with another stream's records using outer join. The join is computed on the records' key with join predicate thisStream.key == otherStream.key . If both streams are not tables, then their timestamps need to be close enough as defined by timeDifference. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <K,VR> store Store configuration No An optional Store configuration, should be of type window . stream string Yes The name of the stream to join with. The stream should be of key type K and value type VR . valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a key of type K , and two values value1 and value2 of type V . The return value is the joined value of type VR . timeDifference duration Yes The maximum allowed between two joined records. grace duration No A grace period during with out-of-order to-be-joined records may still arrive. KTable <K,V> KStream <K,VR> store Store configuration No An optional Store configuration, should be of type keyValue . table string Yes The name of the table to join with. The table should be of key type K and value type VO . valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a value1 of type V from the source table and a value2 of type VO from the join table. The return value is the joined value of type VR . Example: from: input_stream via: - type: outerJoin stream: second_stream valueJoiner: my_key_value_mapper timeDifference: 1m to: output_stream","title":"outerJoin"},{"location":"operations/#peek","text":"Perform an action on each record of a stream. This is a stateless record-by-record operation. Peek is a non-terminal operation that triggers a side effect (such as logging or statistics collection) and returns an unchanged stream. Stream Type Returns Parameter Value Type Description KStream <K,V> KStream <K,V> forEach Inline or reference The [ForEach] function that will be called for every message, receiving arguments key of type K and value of type V . Example: from: input_stream via: - type: peek forEach: print_key_and_value to: output_stream","title":"peek"},{"location":"operations/#reduce","text":"Combine the values of records in this stream by the grouped key. Records with null key or value are ignored. Combining implies that the type of the aggregate result is the same as the type of the input value, similar to aggregate(Initializer, Aggregator) . Stream Type Returns Parameter Value Type Required Description KGroupedStream <K,V> KTable <K,V> store Store configuration No An optional Store configuration, should be of type keyValue . reducer Inline or reference Yes A Reducer function, which takes a key of type K , a value of type V and aggregatedValue of type V . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type V . KGroupedTable <K,V> KTable <K,V> store Store configuration No An optional Store configuration, should be of type keyValue . adder Inline or reference Yes A Reducer function, which takes a key of type K , a value of type V and aggregatedValue of type V . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type V . subtractor Inline or reference Yes A Reducer function, which takes a key of type K , a value of type V and aggregatedValue of type V . It should remove the key/value from the previously calculated aggregateValue and return a new aggregate value of type V . SessionWindowedKStream <K,V> KTable <Windowed<K>,V> store Store configuration No An optional Store configuration, should be of type session . reducer Inline or reference Yes A Reducer function, which takes a key of type K , a value of type V and aggregatedValue of type V . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type V . [TimeWindowedKStreamObject] <K,V> KTable <Windowed<K>,V> store Store configuration No An optional Store configuration, should be of type window . reducer Inline or reference Yes A Reducer function, which takes a key of type K , a value of type V and aggregatedValue of type V . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type V . Example: from: input_stream via: - type: groupBy mapper: my_mapper_function - type: aggregate initializer: expression: 0 aggregator: expression: aggregatedValue + value - type: toStream to: output_stream Example: [ yaml ] ---- from: input_stream via: - type: groupBy mapper: my_mapper_function - type: reduce reducer: expression: value1+value2 - type: toStream to: output_stream","title":"reduce"},{"location":"operations/#repartition","text":"Materialize this stream to an auto-generated repartition topic with a given number of partitions, using a custom partitioner. Similar to auto-repartitioning, the topic will be created with infinite retention time and data will be automatically purged. The topic will be named as \"${applicationId}- -repartition\". Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <K,V> numberOfPartitions integer No The number of partitions of the repartitioned topic. partitioner Inline or reference No A custom [Partitioner] function to partition records. Example: from: input_stream via: - type: repartition name: my_partitioner numberOfPartitions: 3 partitioner: my_own_partitioner - type: peek forEach: print_key_and_value - type: toStream to: output_stream","title":"repartition"},{"location":"operations/#selectkey","text":"This operation takes a message and transforms the key into a new key, which may have a different type. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <KR,V> mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V . The return value is the key of resulting stream, which is of type KR . Example: from: input_stream via: - type: selectKey mapper: expression: str(key) # convert key from source type to string to: output_stream","title":"selectKey"},{"location":"operations/#suppress","text":"Suppress some updates from this changelog stream, determined by the supplied Suppressed configuration. When windowCloses is selected and no further restrictions are provided, then this is interpreted as Suppressed.untilWindowCloses(unbounded()) . Stream Type Returns Parameter Value Type Required Description KTable <K,V> KTable <K,V> until string Yes This value can either be timeLimit or windowCloses . Note that timeLimit suppression works on any stream, while windowCloses suppression works only on Windowed streams. For the latter, see [windowByTime] or [windowBySession]. duration string No The Duration to suppress updates (only when until == timeLimit ) maxBytes int No The maximum number of bytes to suppress updates maxRecords int No The maximum number of records to suppress updates bufferFullStrategy string No Can be one of emitEarlyWhenFull , shutdownWhenFull Example: from: input_table via: - type: suppress until: timeLimit duration: 30s maxBytes: 128000 maxRecords: 10000 bufferFullStrategy: emitEarlyWhenFull - type: peek forEach: print_key_and_value - type: toStream to: output_stream","title":"suppress"},{"location":"operations/#tostream","text":"Convert a KTable into a KStream object. Stream Type Returns Parameter Value Type Required Description KTable <K,V> KStream <KR,V> mapper Inline or reference No A KeyValueMapper function, which takes a key of type K and a value of type V . The return value is the key of resulting stream, which is of type KR . If no mapper is provided, then keys remain unchanged. Example: from: input_table via: - type: toStream to: output_stream","title":"toStream"},{"location":"operations/#transformkey","text":"This is an alias for selectKey . Example: from: input_stream via: - type: transformKey mapper: expression: str(key) # convert key from source type to string to: output_stream","title":"transformKey"},{"location":"operations/#transformkeyvalue","text":"This is an alias for map . Example: from: input_stream via: - type: transformKeyValue mapper: expression: (str(key), str(value)) # convert key and value from source type to string to: output_stream","title":"transformKeyValue"},{"location":"operations/#transformkeyvaluetokeyvaluelist","text":"This is an alias for flatMap . Example: from: input_stream via: - type: transformKeyValueToKeyValueList mapper: expression: [ (key,value), (key,value) ] # duplicate all incoming messages to: output_stream","title":"transformKeyValueToKeyValueList"},{"location":"operations/#transformkeyvaluetovaluelist","text":"This is an alias for flapMapValues . Example: from: input_stream via: - type: transformKeyValueToValueList mapper: expression: [ value+1, value+2, value+3 ] # creates 3 new messages [key,VR] for every input message to: output_stream","title":"transformKeyValueToValueList"},{"location":"operations/#transformmetadata","text":"This operation takes a message and transforms its value to a new value, which may have different value type than the source. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <K,VR> mapper Inline or reference Yes A [MetadataTransformer] function that converts the metadata (Kafka headers, timestamp) of every record in the stream. It gets a metadata object as input and should return the same type, but potentially with modified fields. Example: from: input_stream via: - type: transformValue mapper: expression: str(value) # convert value from source type to String to: output_stream","title":"transformMetadata"},{"location":"operations/#transformvalue","text":"This is an alias for mapValues . Example: from: input_stream via: - type: transformValue mapper: expression: str(value) # convert value from source type to String to: output_stream","title":"transformValue"},{"location":"operations/#windowbysession","text":"Create a new windowed KStream instance that can be used to perform windowed aggregations. For more details on the different types of windows, please refer to [WindowTypes]|[this page]. Stream Type Returns Parameter Value Type Required Description KGroupedStream <K,V> SessionWindowedKStream <K,V> inactivityGap Duration Yes The maximum inactivity gap with which keys are grouped. grace Duration No The grace duration allowing for out-of-order messages to still be associated with the right session. [CogroupedKStream] <K,V> SessionWindowedCogroupedKStream <K,V> inactivityGap Duration Yes The maximum inactivity gap with which keys are grouped. grace Duration No The grace duration allowing for out-of-order messages to still be associated with the right session. Example: from: input_stream via: - type: groupBy mapper: my_mapper_function - type: windowedBySession inactivityGap: 1h grace: 5m - type: reduce reducer: my_reducer_function - type: toStream to: output_stream","title":"windowBySession"},{"location":"operations/#windowbytime","text":"Create a new windowed KStream instance that can be used to perform windowed aggregations. For more details on the different types of windows, please refer to [WindowTypes]|[this page]. Stream Type Returns Parameter Value Type Description KGroupedStream <K,V> TimeWindowedKStream <K,V> windowType string Fixed value sliding . timeDifference Duration The time difference parameter for the [SlidingWindows] object. grace Duration (Optional) The grace parameter for the [SlidingWindows] object. KGroupedStream <K,V> TimeWindowedKStream <K,V> windowType string Fixed value hopping . advanceBy Duration The amount by which each window is advanced. If this value is not specified, then it will be equal to duration , which gives tumbling windows. If you make this value smaller than duration you will get hopping windows. grace Duration (Optional) The grace parameter for the [TimeWindows] object. KGroupedStream <K,V> TimeWindowedKStream <K,V> windowType string Fixed value tumbling . duration Duration The duration parameter for the [TimeWindows] object. grace Duration (Optional) The grace parameter for the [TimeWindows] object. [CogroupedKStream] <K,V> TimeWindowedCogroupedKStream <K,V> windowType string Fixed value sliding . timeDifference Duration The time difference parameter for the [SlidingWindows] object. grace Duration (Optional) The grace parameter for the [SlidingWindows] object. [CogroupedKStream] <K,V> TimeWindowedCogroupedKStream <K,V> windowType string Fixed value hopping . advanceBy Duration The amount by which each window is advanced. If this value is not specified, then it will be equal to duration , which gives tumbling windows. If you make this value smaller than duration you will get hopping windows. grace Duration (Optional) The grace parameter for the [TimeWindows] object. [CogroupedKStream] <K,V> TimeWindowedCogroupedKStream <K,V> windowType string Fixed value tumbling . duration Duration The duration parameter for the [TimeWindows] object. grace Duration (Optional) The grace parameter for the [TimeWindows] object. Example: from: input_stream via: - type: groupBy mapper: my_mapper_function - type: windowedBy windowType: time duration: 1h advanceBy: 15m grace: 5m - type: reduce reducer: my_reducer_function - type: toStream to: output_stream","title":"windowByTime"},{"location":"operations/#sink-operations","text":"","title":"Sink Operations"},{"location":"operations/#as","text":"Pipelines closed of with as can be referred by other pipelines as their starting reference. This allows for a common part of processing logic to be placed in its own pipeline in KSML, serving as an intermediate result. Applies to Value Type Required Description Any pipeline <K,V> string Yes The name under which the pipeline result can be referenced by other pipelines. Example: pipelines: first: from: some_source_topic via: - type: ... as: first_pipeline second: from: first_pipeline via: - type: ... to: ... Here, the first pipeline ends by sending its output to a stream internally called first_pipeline . This stream is used as input for the second pipeline.","title":"as"},{"location":"operations/#branch","text":"Branches out messages from the input stream into several branches based on predicates. Each branch is defined as a list item below the branch operation. Branch predicates are defined using the if keyword. Messages are only processed by one of the branches, namely the first one for which the predicate returns true . Applies to Value Type Required Description KStream <K,V> List of branch definitions Yes See for description of branch definitions below. Branches in KSML are nested pipelines, which are parsed without the requirement of a source attribute. Each branch accepts the following parameters: Branch element Value Type Required Description if Inline Predicate or reference No The Predicate function that determines if the message is sent down this branch, or is passed on to the next branch in line. Inline All pipeline parameters, see [Pipeline] Yes The inlined pipeline describes the topology of the specific branch. Example: from: some_source_topic branch: - if: expression: value['color'] == 'blue' to: ksml_sensordata_blue - if: expression: value['color'] == 'red' to: ksml_sensordata_red - forEach: code: | print('Unknown color sensor: '+value[\"color\"]) In this example, the first two branches are entered if the respective predicate matches (the color attribute of value matches a certain color). If the predicate returns false , then the next predicate/branch is tried. Only the last branch in the list can be a sink operation.","title":"branch"},{"location":"operations/#foreach","text":"This sends each message to a custom defined function. This function is expected to handle each message as its final step. The function does not (need to) return anything. Applies to Value Type Description KStream <K,V> Inline or reference The [ForEach] function that is called for every record on the source stream. Its arguments are key of type K and value of type V . Examples: forEach: my_foreach_function forEach: code: print(value)","title":"forEach"},{"location":"operations/#print","text":"This sends each message to a custom defined print function. This function is expected to handle each message as the final in the pipeline. The function does not (need to) return anything. As target, you can specify a filename. If none is specified, then all messages are printed to stdout. Applies to Parameter Value Type Required Description KStream <K,V> filename string No The filename to output records to. If nothing is specified, then messages will be printed on stdout. label string No A label to attach to every output record. mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V . The return value should be of type string and is sent to the specified file or stdout. Examples: from: source via: - type: ... print: filename: file.txt mapper: expression: \"record value: \" + str(value)","title":"print"},{"location":"operations/#to","text":"Messages are sent directly to a named Stream . Applies to Value Type Required Description KStream <K,V> Inline [Topic] or reference to a stream, table or global table Yes The name of a defined stream . Examples: to: my_target_topic from: source via: - type: ... to: topic: my_target_topic keyType: someType valueType: someOtherType partitioner: expression: hash_of(key)","title":"to"},{"location":"operations/#totopicnameextractor","text":"Messages are passed onto a user function, which returns the name of the topic that message needs to be sent to. This operation acts as a Sink and is always the last operation in a pipeline . Applies to Value Type Required Description KStream <K,V> Inline or reference Yes The [TopicNameExtractor] function that is called for every message and returns the topic name to which the message shall be written. Examples: toTopicNameExtractor: my_extractor_function toTopicNameExtractor: code: | if key == 'sensor1': return 'ksml_sensordata_sensor1' elif key == 'sensor2': return 'ksml_sensordata_sensor2' else: return 'ksml_sensordata_sensor0'","title":"toTopicNameExtractor"},{"location":"pipelines/","text":"Pipeline Table of Contents Introduction Definition Elements Source Operations Sink Introduction Pipelines form the heart of KSML streams logic. They take one or more input streams and apply processing logic to them. Output is passed on from operation to operation. Definition Pipelines are contained in the pipelines section. Each pipeline has a name, which is the name of the YAML tag. As an example, the following defines a pipeline called copy_pipeline , which consumes messages from some_input_stream and outputs the same messages to some_output_stream . pipelines: copy_pipeline: from: some_input_stream to: some_output_stream Elements All pipelines contain three elements: Source The source of a pipeline is marked with the from keyword. The value of the YAML node is the name of a defined Stream , Table or GlobalTable . See Streams for more information. Operations Once a source was selected, a list of operations can be applied to the input. The list is started through the keyword via , below which a list of operations is defined. Example: pipelines: copy_pipeline: from: some_input_stream via: - type: peek forEach: my_peek_function - type: transformKeyValue mapper: my_transform_function to: some_output_stream Each operation must have a type field, which indicates the type of operations applied to the input. See Operations for a full list of operations that can be applied. Sinks After all transformation operations are applied, a pipeline can define a sink to which all messages are sent. There are four sink types in KSML: Sink type Description as Allows the pipeline result to be saved under an internal name, which can later be referenced. Pipelines defined after this point may refer to this name in their from statement. branch This statement allows the pipeline to be split up in several branches. Each branch filters messages with an if statement. Messages will be processed only by the first branch of which the if statement is true. forEach Sends every message to a function, without expecting any return type. Because there is no return type, the pipeline always stops after this statement. print Prints out every message according to a given output specification. to Sends all output messages to a specific target. This target can be a pre-defined stream , table or globalTable , or an inline-defined topic. toTopicNameExtractor Messages are passed onto a user function, which returns the name of the topic that message needs to be sent to. For more information, see the respective documentation on pipeline definitions in the definitions section of the KSML language spec . Duration Some pipeline operations require specifying durations. Durations can be expressed as strings with the following syntax: ###x where # is a positive number between 0 and 999999 and x is an optional letter from the following table: Letter Description none Duration in milliseconds s Duration in seconds m Duration in minutes h Duration in hours d Duration in days w Duration in weeks Examples: 100 ==> hundred milliseconds 30s ==> thirty seconds 8h ==> eight hours 2w ==> two weeks Note that durations are not a data type that can used as key or value on a Kafka topic.","title":"Pipeline"},{"location":"pipelines/#pipeline","text":"","title":"Pipeline"},{"location":"pipelines/#table-of-contents","text":"Introduction Definition Elements Source Operations Sink","title":"Table of Contents"},{"location":"pipelines/#introduction","text":"Pipelines form the heart of KSML streams logic. They take one or more input streams and apply processing logic to them. Output is passed on from operation to operation.","title":"Introduction"},{"location":"pipelines/#definition","text":"Pipelines are contained in the pipelines section. Each pipeline has a name, which is the name of the YAML tag. As an example, the following defines a pipeline called copy_pipeline , which consumes messages from some_input_stream and outputs the same messages to some_output_stream . pipelines: copy_pipeline: from: some_input_stream to: some_output_stream","title":"Definition"},{"location":"pipelines/#elements","text":"All pipelines contain three elements:","title":"Elements"},{"location":"pipelines/#source","text":"The source of a pipeline is marked with the from keyword. The value of the YAML node is the name of a defined Stream , Table or GlobalTable . See Streams for more information.","title":"Source"},{"location":"pipelines/#operations","text":"Once a source was selected, a list of operations can be applied to the input. The list is started through the keyword via , below which a list of operations is defined. Example: pipelines: copy_pipeline: from: some_input_stream via: - type: peek forEach: my_peek_function - type: transformKeyValue mapper: my_transform_function to: some_output_stream Each operation must have a type field, which indicates the type of operations applied to the input. See Operations for a full list of operations that can be applied.","title":"Operations"},{"location":"pipelines/#sinks","text":"After all transformation operations are applied, a pipeline can define a sink to which all messages are sent. There are four sink types in KSML: Sink type Description as Allows the pipeline result to be saved under an internal name, which can later be referenced. Pipelines defined after this point may refer to this name in their from statement. branch This statement allows the pipeline to be split up in several branches. Each branch filters messages with an if statement. Messages will be processed only by the first branch of which the if statement is true. forEach Sends every message to a function, without expecting any return type. Because there is no return type, the pipeline always stops after this statement. print Prints out every message according to a given output specification. to Sends all output messages to a specific target. This target can be a pre-defined stream , table or globalTable , or an inline-defined topic. toTopicNameExtractor Messages are passed onto a user function, which returns the name of the topic that message needs to be sent to. For more information, see the respective documentation on pipeline definitions in the definitions section of the KSML language spec .","title":"Sinks"},{"location":"pipelines/#duration","text":"Some pipeline operations require specifying durations. Durations can be expressed as strings with the following syntax: ###x where # is a positive number between 0 and 999999 and x is an optional letter from the following table: Letter Description none Duration in milliseconds s Duration in seconds m Duration in minutes h Duration in hours d Duration in days w Duration in weeks Examples: 100 ==> hundred milliseconds 30s ==> thirty seconds 8h ==> eight hours 2w ==> two weeks Note that durations are not a data type that can used as key or value on a Kafka topic.","title":"Duration"},{"location":"quick-start/","text":"Quick start Table of Contents Introduction Starting a demo setup Starting a KSML runner Next steps Introduction KSML comes with example definitions, which contain a producer that outputs SensorData messages to Kafka, and several pipelines, which each independently consume and process the produced messages. Starting a demo setup After checking out the repository, go to the KSML directory and execute the following: docker compose up -d This will start Zookeeper, Kafka and a Schema Registry in the background. It will also start the demo producer, which outputs two random messages per second on a ksml_sensordata_avro topic. You can check the valid starting of these containers using the following command: docker compose logs -f Press CTRL-C when you verified data is produced. This typically looks like this: example-producer-1 | 2024-03-06T20:24:49,480Z INFO i.a.k.r.backend.KafkaProducerRunner Calling generate_sensordata_message example-producer-1 | 2024-03-06T20:24:49,480Z INFO i.a.k.r.backend.ExecutableProducer Message: key=sensor2, value=SensorData: {\"city\":\"Utrecht\", \"color\":\"white\", \"name\":\"sensor2\", \"owner\":\"Alice\", \"timestamp\":1709756689480, \"type\":\"HUMIDITY\", \"unit\":\"%\", \"value\":\"66\"} example-producer-1 | 2024-03-06T20:24:49,481Z INFO i.a.k.r.backend.ExecutableProducer Produced message to ksml_sensordata_avro, partition 0, offset 1975 example-producer-1 | 2024-03-06T20:24:49,481Z INFO i.a.k.r.backend.KafkaProducerRunner Calling generate_sensordata_message example-producer-1 | 2024-03-06T20:24:49,481Z INFO i.a.k.r.backend.ExecutableProducer Message: key=sensor3, value=SensorData: {\"city\":\"Alkmaar\", \"color\":\"black\", \"name\":\"sensor3\", \"owner\":\"Dave\", \"timestamp\":\"1709756689481\", \"type\":\"STATE\", \"unit\":\"state\", \"value\":\"off\"} example-producer-1 | 2024-03-06T20:24:49,483Z INFO i.a.k.r.backend.Execut^Cestamp\":1709756689814, \"type\":\"AREA\", \"unit\":\"ft2\", \"value\":\"76\"} example-producer-1 | 2024-03-06T20:24:49,815Z INFO i.a.k.r.backend.ExecutableProducer Produced message to ksml_sensordata_xml, partition 0, offset 1129 example-producer-1 | 2024-03-06T20:24:49,921Z INFO i.a.k.r.backend.KafkaProducerRunner Calling generate_sensordata_message example-producer-1 | 2024-03-06T20:24:49,922Z INFO i.a.k.r.backend.ExecutableProducer Message: key=sensor6, value=SensorData: {\"city\":\"Amsterdam\", \"color\":\"yellow\", \"name\":\"sensor6\", \"owner\":\"Evan\", \"timestamp\":1709756689922, \"type\":\"AREA\", \"unit\":\"m2\", \"value\":\"245\"} example-producer-1 | 2024-03-06T20:24:49,923Z INFO i.a.k.r.backend.ExecutableProducer Produced message to ksml_sensordata_avro, partition 0, offset 1976 example-producer-1 | 2024-03-06T20:24:50,035Z INFO i.a.k.r.backend.KafkaProducerRunner Calling generate_sensordata_message example-producer-1 | 2024-03-06T20:24:50,035Z INFO i.a.k.r.backend.ExecutableProducer Message: key=sensor7, value=SensorData: {\"city\":\"Alkmaar\", \"color\":\"black\", \"name\":\"sensor7\", \"owner\":\"Dave\", \"timestamp\":\"1709756690035\", \"type\":\"TEMPERATURE\", \"unit\":\"C\", \"value\":\"0\"} Starting a KSML runner To start a container which executes the example KSML definitions, type ./examples/run.sh This will start the KSML docker container. You should see the following typical output: 2024-03-06T20:24:51,921Z INFO io.axual.ksml.runner.KSMLRunner Starting KSML Runner 1.76.0.0 ... ... ... ... 2024-03-06T20:24:57,196Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor9, value={'city': 'Alkmaar', 'color': 'yellow', 'name': 'sensor9', 'owner': 'Bob', 'timestamp': 1709749917190, 'type': 'LENGTH', 'unit': 'm', 'value': '562', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:24:57,631Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor3, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor3', 'owner': 'Bob', 'timestamp': 1709749917628, 'type': 'HUMIDITY', 'unit': 'g/m3', 'value': '23', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:24:58,082Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor6, value={'city': 'Amsterdam', 'color': 'white', 'name': 'sensor6', 'owner': 'Bob', 'timestamp': 1709749918078, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '64', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:24:58,528Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor9, value={'city': 'Amsterdam', 'color': 'black', 'name': 'sensor9', 'owner': 'Evan', 'timestamp': 1709749918524, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '87', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:24:58,970Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor1, value={'city': 'Amsterdam', 'color': 'black', 'name': 'sensor1', 'owner': 'Bob', 'timestamp': 1709749918964, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '75', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:24:59,412Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor5, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor5', 'owner': 'Bob', 'timestamp': 1709749919409, 'type': 'LENGTH', 'unit': 'm', 'value': '658', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} Next steps Check out the examples in the examples directory of the project. By modifying the file examples/ksml-runner.yaml you can select the example(s) to run. For a more elaborate introduction, you can start here or refer to the documentation .","title":"Quick start"},{"location":"quick-start/#quick-start","text":"","title":"Quick start"},{"location":"quick-start/#table-of-contents","text":"Introduction Starting a demo setup Starting a KSML runner Next steps","title":"Table of Contents"},{"location":"quick-start/#introduction","text":"KSML comes with example definitions, which contain a producer that outputs SensorData messages to Kafka, and several pipelines, which each independently consume and process the produced messages.","title":"Introduction"},{"location":"quick-start/#starting-a-demo-setup","text":"After checking out the repository, go to the KSML directory and execute the following: docker compose up -d This will start Zookeeper, Kafka and a Schema Registry in the background. It will also start the demo producer, which outputs two random messages per second on a ksml_sensordata_avro topic. You can check the valid starting of these containers using the following command: docker compose logs -f Press CTRL-C when you verified data is produced. This typically looks like this: example-producer-1 | 2024-03-06T20:24:49,480Z INFO i.a.k.r.backend.KafkaProducerRunner Calling generate_sensordata_message example-producer-1 | 2024-03-06T20:24:49,480Z INFO i.a.k.r.backend.ExecutableProducer Message: key=sensor2, value=SensorData: {\"city\":\"Utrecht\", \"color\":\"white\", \"name\":\"sensor2\", \"owner\":\"Alice\", \"timestamp\":1709756689480, \"type\":\"HUMIDITY\", \"unit\":\"%\", \"value\":\"66\"} example-producer-1 | 2024-03-06T20:24:49,481Z INFO i.a.k.r.backend.ExecutableProducer Produced message to ksml_sensordata_avro, partition 0, offset 1975 example-producer-1 | 2024-03-06T20:24:49,481Z INFO i.a.k.r.backend.KafkaProducerRunner Calling generate_sensordata_message example-producer-1 | 2024-03-06T20:24:49,481Z INFO i.a.k.r.backend.ExecutableProducer Message: key=sensor3, value=SensorData: {\"city\":\"Alkmaar\", \"color\":\"black\", \"name\":\"sensor3\", \"owner\":\"Dave\", \"timestamp\":\"1709756689481\", \"type\":\"STATE\", \"unit\":\"state\", \"value\":\"off\"} example-producer-1 | 2024-03-06T20:24:49,483Z INFO i.a.k.r.backend.Execut^Cestamp\":1709756689814, \"type\":\"AREA\", \"unit\":\"ft2\", \"value\":\"76\"} example-producer-1 | 2024-03-06T20:24:49,815Z INFO i.a.k.r.backend.ExecutableProducer Produced message to ksml_sensordata_xml, partition 0, offset 1129 example-producer-1 | 2024-03-06T20:24:49,921Z INFO i.a.k.r.backend.KafkaProducerRunner Calling generate_sensordata_message example-producer-1 | 2024-03-06T20:24:49,922Z INFO i.a.k.r.backend.ExecutableProducer Message: key=sensor6, value=SensorData: {\"city\":\"Amsterdam\", \"color\":\"yellow\", \"name\":\"sensor6\", \"owner\":\"Evan\", \"timestamp\":1709756689922, \"type\":\"AREA\", \"unit\":\"m2\", \"value\":\"245\"} example-producer-1 | 2024-03-06T20:24:49,923Z INFO i.a.k.r.backend.ExecutableProducer Produced message to ksml_sensordata_avro, partition 0, offset 1976 example-producer-1 | 2024-03-06T20:24:50,035Z INFO i.a.k.r.backend.KafkaProducerRunner Calling generate_sensordata_message example-producer-1 | 2024-03-06T20:24:50,035Z INFO i.a.k.r.backend.ExecutableProducer Message: key=sensor7, value=SensorData: {\"city\":\"Alkmaar\", \"color\":\"black\", \"name\":\"sensor7\", \"owner\":\"Dave\", \"timestamp\":\"1709756690035\", \"type\":\"TEMPERATURE\", \"unit\":\"C\", \"value\":\"0\"}","title":"Starting a demo setup"},{"location":"quick-start/#starting-a-ksml-runner","text":"To start a container which executes the example KSML definitions, type ./examples/run.sh This will start the KSML docker container. You should see the following typical output: 2024-03-06T20:24:51,921Z INFO io.axual.ksml.runner.KSMLRunner Starting KSML Runner 1.76.0.0 ... ... ... ... 2024-03-06T20:24:57,196Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor9, value={'city': 'Alkmaar', 'color': 'yellow', 'name': 'sensor9', 'owner': 'Bob', 'timestamp': 1709749917190, 'type': 'LENGTH', 'unit': 'm', 'value': '562', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:24:57,631Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor3, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor3', 'owner': 'Bob', 'timestamp': 1709749917628, 'type': 'HUMIDITY', 'unit': 'g/m3', 'value': '23', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:24:58,082Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor6, value={'city': 'Amsterdam', 'color': 'white', 'name': 'sensor6', 'owner': 'Bob', 'timestamp': 1709749918078, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '64', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:24:58,528Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor9, value={'city': 'Amsterdam', 'color': 'black', 'name': 'sensor9', 'owner': 'Evan', 'timestamp': 1709749918524, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '87', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:24:58,970Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor1, value={'city': 'Amsterdam', 'color': 'black', 'name': 'sensor1', 'owner': 'Bob', 'timestamp': 1709749918964, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '75', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:24:59,412Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor5, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor5', 'owner': 'Bob', 'timestamp': 1709749919409, 'type': 'LENGTH', 'unit': 'm', 'value': '658', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}}","title":"Starting a KSML runner"},{"location":"quick-start/#next-steps","text":"Check out the examples in the examples directory of the project. By modifying the file examples/ksml-runner.yaml you can select the example(s) to run. For a more elaborate introduction, you can start here or refer to the documentation .","title":"Next steps"},{"location":"release-notes/","text":"Release Notes Releases Release Notes Releases 1.0.4 (2024-10-18) 1.0.2 (2024-09-20) 1.0.1 (2024-07-17) 1.0.0 (2024-06-28) 0.8.0 (2024-03-08) 0.9.1 (2024-06-21) 0.9.0 (2024-06-05) 0.2.2 (2024-01-30) 0.2.1 (2023-12-20) 0.2.0 (2023-12-07) 0.1.0 (2023-03-15) 0.0.4 (2022-12-02) 0.0.3 (2021-07-30) 0.0.2 (2021-06-28) 0.0.1 (2021-04-30) 1.0.3 (2024-10-18) KSML Fix high CPU usage Upgrade to Avro 1.11.4 to fix CVE-2024-47561 1.0.2 (2024-09-20) KSML Upgrade to Kafka Streams 3.8.0 Avro Schema Registry settings no longer required if Avro not used Add missing object in KSML Json Schema Fix serialisation and list handling issues Helm charts Use liveness and readiness and startup probes to fix state issues Fix conflicting default configuration Prometheus export and ServiceMonitor 1.0.1 (2024-07-17) Topology Optimization can be applied Runtime dependencies, like LZ4 compression support, are back in the KSML image Fix parse error messages during join Fix windowed aggregation flow errors Update windowed object support in multiple operations and functions 1.0.0 (2024-06-28) Reworked parsing logic, allowing alternatives for operations and other definitions to co-exist in the KSML language specification. This allows for better syntax checking in IDEs. Lots of small fixes and completion modifications. 0.9.1 (2024-06-21) Fix failing test in GitHub Actions during release Unified build workflows 0.9.0 (2024-06-05) Collectable metrics New topology test suite Python context hardening Improved handling of Kafka tombstones Added flexibility to producers (single shot, n-shot, or user condition-based) JSON Logging support Bumped GraalVM to 23.1.2 Bumped several dependency versions Several fixes and security updates 0.8.0 (2024-03-08) Reworked all parsing logic, to allow for exporting the JSON schema of the KSML specification: docs/specification.md is now derived from internal parser logic, guaranteeing consistency and completeness. examples/ksml.json contains the JSON schema, which can be loaded into IDEs for syntax validation and completion. Improved schema handling: Better compatibility checking between schema fields. Improved support for state stores: Update to state store typing and handling. Manual state stores can be defined and referenced in pipelines. Manual state stores are also available in Python functions. State stores can be used 'side-effect-free' (e.g. no AVRO schema registration) Python function improvements: Automatic variable assignment for state stores. Every Python function can use a Java Logger, integrating Python output with KSML log output. Type inference in situations where parameters or result types can be derived from the context. Lots of small language updates: Improve readability for store types, filter operations and windowing operations Introduction of the \"as\" operation, which allows for pipeline referencing and chaining. Better data type handling: Separation of data types and KSML core, allowing for easier addition of new data types in the future. Automatic conversion of data types, removing common pipeline failure scenarios. New implementation for CSV handling. Merged the different runners into a single runner. KSML definitions can now include both producers (data generators) and pipelines (Kafka Streams topologies). Removal of Kafka and Axual backend distinctions. Configuration file updates, allowing for running multiple definitions in a single runner (each in its own namespace). Examples updated to reflect the latest definition format. Documentation updated. 0.2.2 (2024-01-30) Changes: Fix KSML java process not stopping on exception Fix stream-stream join validation and align other checks Bump logback to 1.4.12 Fix to enable Streams optimisations to be applied to topology Fix resolving admin client issues causing warning messages 0.2.1 (2023-12-20) Changes: Fixed an issue with AVRO and field validations 0.2.0 (2023-12-07) Changes: Optimized Docker build Merged KSML Runners into one module, optimize Docker builds and workflow KSML documentation updates Docker image, GraalPy venv, install and GU commands fail Update GitHub Actions Small robustness improvements Issue #72 - Fix build failures when trying to use venv and install python packages Manual state store support, Kafka client cleanups and configuration changes Update and clean up dependencies Update documentation to use new runner configurations Update to GraalVM for JDK 21 Community 0.1.0 (2023-03-15) Changes: Added XML/SOAP support Added data generator Added Automatic Type Conversion Added Schema Support for XML, Avro, JSON, Schema Added Basic Error Handling 0.0.4 (2022-12-02) Changes: Update to kafka 3.2.3 Update to Java 17 Support multiple architectures in KSML, linux/amd64 and linux/arm64 Refactored internal typing system, plus some fixes to store operations Introduce queryable state stores Add better handling of NULL keys and values from Kafka Implement schema support Added Docker multistage build Bug fix for windowed objects Store improvements Support Liberica NIK Switch from Travis CI to GitHub workflow Build snapshot Docker image on pull request merged 0.0.3 (2021-07-30) Changes: Support for Python 3 through GraalVM improved data structuring bug fixes 0.0.2 (2021-06-28) Changes: Added JSON support, Named topology and name store supported 0.0.1 (2021-04-30) Changes: First alpha release","title":"Release Notes"},{"location":"release-notes/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/#releases","text":"Release Notes Releases 1.0.4 (2024-10-18) 1.0.2 (2024-09-20) 1.0.1 (2024-07-17) 1.0.0 (2024-06-28) 0.8.0 (2024-03-08) 0.9.1 (2024-06-21) 0.9.0 (2024-06-05) 0.2.2 (2024-01-30) 0.2.1 (2023-12-20) 0.2.0 (2023-12-07) 0.1.0 (2023-03-15) 0.0.4 (2022-12-02) 0.0.3 (2021-07-30) 0.0.2 (2021-06-28) 0.0.1 (2021-04-30)","title":"Releases"},{"location":"release-notes/#103-2024-10-18","text":"KSML Fix high CPU usage Upgrade to Avro 1.11.4 to fix CVE-2024-47561","title":"1.0.3 (2024-10-18)"},{"location":"release-notes/#102-2024-09-20","text":"KSML Upgrade to Kafka Streams 3.8.0 Avro Schema Registry settings no longer required if Avro not used Add missing object in KSML Json Schema Fix serialisation and list handling issues Helm charts Use liveness and readiness and startup probes to fix state issues Fix conflicting default configuration Prometheus export and ServiceMonitor","title":"1.0.2 (2024-09-20)"},{"location":"release-notes/#101-2024-07-17","text":"Topology Optimization can be applied Runtime dependencies, like LZ4 compression support, are back in the KSML image Fix parse error messages during join Fix windowed aggregation flow errors Update windowed object support in multiple operations and functions","title":"1.0.1 (2024-07-17)"},{"location":"release-notes/#100-2024-06-28","text":"Reworked parsing logic, allowing alternatives for operations and other definitions to co-exist in the KSML language specification. This allows for better syntax checking in IDEs. Lots of small fixes and completion modifications.","title":"1.0.0 (2024-06-28)"},{"location":"release-notes/#091-2024-06-21","text":"Fix failing test in GitHub Actions during release Unified build workflows","title":"0.9.1 (2024-06-21)"},{"location":"release-notes/#090-2024-06-05","text":"Collectable metrics New topology test suite Python context hardening Improved handling of Kafka tombstones Added flexibility to producers (single shot, n-shot, or user condition-based) JSON Logging support Bumped GraalVM to 23.1.2 Bumped several dependency versions Several fixes and security updates","title":"0.9.0 (2024-06-05)"},{"location":"release-notes/#080-2024-03-08","text":"Reworked all parsing logic, to allow for exporting the JSON schema of the KSML specification: docs/specification.md is now derived from internal parser logic, guaranteeing consistency and completeness. examples/ksml.json contains the JSON schema, which can be loaded into IDEs for syntax validation and completion. Improved schema handling: Better compatibility checking between schema fields. Improved support for state stores: Update to state store typing and handling. Manual state stores can be defined and referenced in pipelines. Manual state stores are also available in Python functions. State stores can be used 'side-effect-free' (e.g. no AVRO schema registration) Python function improvements: Automatic variable assignment for state stores. Every Python function can use a Java Logger, integrating Python output with KSML log output. Type inference in situations where parameters or result types can be derived from the context. Lots of small language updates: Improve readability for store types, filter operations and windowing operations Introduction of the \"as\" operation, which allows for pipeline referencing and chaining. Better data type handling: Separation of data types and KSML core, allowing for easier addition of new data types in the future. Automatic conversion of data types, removing common pipeline failure scenarios. New implementation for CSV handling. Merged the different runners into a single runner. KSML definitions can now include both producers (data generators) and pipelines (Kafka Streams topologies). Removal of Kafka and Axual backend distinctions. Configuration file updates, allowing for running multiple definitions in a single runner (each in its own namespace). Examples updated to reflect the latest definition format. Documentation updated.","title":"0.8.0 (2024-03-08)"},{"location":"release-notes/#022-2024-01-30","text":"Changes: Fix KSML java process not stopping on exception Fix stream-stream join validation and align other checks Bump logback to 1.4.12 Fix to enable Streams optimisations to be applied to topology Fix resolving admin client issues causing warning messages","title":"0.2.2 (2024-01-30)"},{"location":"release-notes/#021-2023-12-20","text":"Changes: Fixed an issue with AVRO and field validations","title":"0.2.1 (2023-12-20)"},{"location":"release-notes/#020-2023-12-07","text":"Changes: Optimized Docker build Merged KSML Runners into one module, optimize Docker builds and workflow KSML documentation updates Docker image, GraalPy venv, install and GU commands fail Update GitHub Actions Small robustness improvements Issue #72 - Fix build failures when trying to use venv and install python packages Manual state store support, Kafka client cleanups and configuration changes Update and clean up dependencies Update documentation to use new runner configurations Update to GraalVM for JDK 21 Community","title":"0.2.0 (2023-12-07)"},{"location":"release-notes/#010-2023-03-15","text":"Changes: Added XML/SOAP support Added data generator Added Automatic Type Conversion Added Schema Support for XML, Avro, JSON, Schema Added Basic Error Handling","title":"0.1.0 (2023-03-15)"},{"location":"release-notes/#004-2022-12-02","text":"Changes: Update to kafka 3.2.3 Update to Java 17 Support multiple architectures in KSML, linux/amd64 and linux/arm64 Refactored internal typing system, plus some fixes to store operations Introduce queryable state stores Add better handling of NULL keys and values from Kafka Implement schema support Added Docker multistage build Bug fix for windowed objects Store improvements Support Liberica NIK Switch from Travis CI to GitHub workflow Build snapshot Docker image on pull request merged","title":"0.0.4 (2022-12-02)"},{"location":"release-notes/#003-2021-07-30","text":"Changes: Support for Python 3 through GraalVM improved data structuring bug fixes","title":"0.0.3 (2021-07-30)"},{"location":"release-notes/#002-2021-06-28","text":"Changes: Added JSON support, Named topology and name store supported","title":"0.0.2 (2021-06-28)"},{"location":"release-notes/#001-2021-04-30","text":"Changes: First alpha release","title":"0.0.1 (2021-04-30)"},{"location":"runners/","text":"KSML Runner Table of Contents Introduction Configuration Namespace support Starting a container Introduction KSML is built around the ksml module, which provides the core functionality for parsing KSML definition files and converting them into Kafka Streams topologies. To keep KSML lightweight and flexible, it does not execute these topologies directly. Instead, execution is handled by the ksml-runner module \u2014 a standalone Java application that loads KSML configurations and runs the corresponding Kafka Streams applications. The ksml-runner supports standard Kafka configurations and includes advanced features for working with Kafka clusters that use namespacing (e.g., tenant-aware deployments). Example runner configurations are provided below. Configuration The configuration file passed to the KSML runner is in YAML format and should contain at least the following: ksml: applicationServer: # The application server is currently only offering REST querying of state stores enabled: true # true if you want to enable REST querying of state stores host: 0.0.0.0 # by default listen on all interfaces port: 8080 # port to listen on configDirectory: /ksml/config # Location of the KSML definitions. Default is the current working directory schemaDirectory: /ksml/schemas # Location of the schema definitions. Default is the config directory storageDirectory: /ksml/data # Where the stateful data is written. Defaults is the default JVM temp directory errorHandling: # how to handle errors consume: log: true # log errors logPayload: true # log message payloads upon error loggerName: ConsumeError # logger name handler: continueOnFail # continue or stop on error process: log: true # log errors logPayload: true # log message payloads upon error loggerName: ProcessError # logger name handler: stopOnFail # continue or stop on error produce: log: true # log errors logPayload: true # log message payloads upon error loggerName: ProduceError # logger name handler: continueOnFail # continue or stop on error enableProducers: true # False to disable producers in the KSML definition enablePipelines: true # False to disable pipelines in the KSML definition definitions: # KSML definition files from the working directory namedDefinition1: definition1.yaml namedDefinition2: definition2.yaml namedDefinition3: <more here...> kafka: # Kafka streams configuration options application.id: io.ksml.example.processor bootstrap.servers: broker-1:9092,broker-2:9092 security.protocol: SSL ssl.protocol: TLSv1.3 ssl.enabled.protocols: TLSv1.3,TLSv1.2 ssl.endpoint.identification.algorithm: \"\" ssl.truststore.location: /ksml/config/truststore.jks ssl.truststore.password: password-for-truststore # Schema Registry client configuration, needed when schema registry is used schema.registry.url: http://schema-registry:8083 schema.registry.ssl.truststore.location: /ksml/config/truststore.jks schema.registry.ssl.truststore.password: password-for-truststore Using with Axual platform or other namespaced Kafka clusters A special mode for connecting to clusters that use namespaced Kafka resources is available. This mode can be activated by specifying the namespace pattern to use. This pattern will be resolved to a complete name by KSML using the provided configuration options. The following config will resolve the backing topic of a stream or table kafka: # The patterns for topics, groups and transactional ids. # Each field between the curly braces must be specified in the configuration, except the topic, # group.id and transactional.id fields, which is used to identify the place where the resource name # is used topic.pattern: \"{tenant}-{instance}-{environment}-{topic}\" group.id.pattern: \"{tenant}-{instance}-{environment}-{group.id}\" transactional.id.pattern: \"{tenant}-{instance}-{environment}-{transactional.id}\" # Additional configuration options used for resolving the pattern to values tenant: \"ksmldemo\" instance: \"dta\" environment: \"dev\" Starting a container To start a container the KSML definitions and Runner configuration files need to be available in a directory mounted inside the docker container. The default Runner configuration filename is ksml-runner.yaml . If no arguments are given, the runner will look for this file in the home directory ## -w sets the current working directory in the container docker run --rm -ti -v /path/to/local/ksml/directory:/ksml -w /ksml axual/ksml-axual:snapshot ## or docker run --rm -ti -v /path/to/local/ksml/directory:/ksml -w /ksml axual/ksml-axual:snapshot /ksml/ksml-runner.yaml or, if the runner configuration is in a different file, like my-runner.yaml . docker run --rm -ti -v /path/to/local/ksml/directory:/ksml axual/ksml-axual:latest /ksml/my-runner.yaml","title":"KSML Runner"},{"location":"runners/#ksml-runner","text":"","title":"KSML Runner"},{"location":"runners/#table-of-contents","text":"Introduction Configuration Namespace support Starting a container","title":"Table of Contents"},{"location":"runners/#introduction","text":"KSML is built around the ksml module, which provides the core functionality for parsing KSML definition files and converting them into Kafka Streams topologies. To keep KSML lightweight and flexible, it does not execute these topologies directly. Instead, execution is handled by the ksml-runner module \u2014 a standalone Java application that loads KSML configurations and runs the corresponding Kafka Streams applications. The ksml-runner supports standard Kafka configurations and includes advanced features for working with Kafka clusters that use namespacing (e.g., tenant-aware deployments). Example runner configurations are provided below.","title":"Introduction"},{"location":"runners/#configuration","text":"The configuration file passed to the KSML runner is in YAML format and should contain at least the following: ksml: applicationServer: # The application server is currently only offering REST querying of state stores enabled: true # true if you want to enable REST querying of state stores host: 0.0.0.0 # by default listen on all interfaces port: 8080 # port to listen on configDirectory: /ksml/config # Location of the KSML definitions. Default is the current working directory schemaDirectory: /ksml/schemas # Location of the schema definitions. Default is the config directory storageDirectory: /ksml/data # Where the stateful data is written. Defaults is the default JVM temp directory errorHandling: # how to handle errors consume: log: true # log errors logPayload: true # log message payloads upon error loggerName: ConsumeError # logger name handler: continueOnFail # continue or stop on error process: log: true # log errors logPayload: true # log message payloads upon error loggerName: ProcessError # logger name handler: stopOnFail # continue or stop on error produce: log: true # log errors logPayload: true # log message payloads upon error loggerName: ProduceError # logger name handler: continueOnFail # continue or stop on error enableProducers: true # False to disable producers in the KSML definition enablePipelines: true # False to disable pipelines in the KSML definition definitions: # KSML definition files from the working directory namedDefinition1: definition1.yaml namedDefinition2: definition2.yaml namedDefinition3: <more here...> kafka: # Kafka streams configuration options application.id: io.ksml.example.processor bootstrap.servers: broker-1:9092,broker-2:9092 security.protocol: SSL ssl.protocol: TLSv1.3 ssl.enabled.protocols: TLSv1.3,TLSv1.2 ssl.endpoint.identification.algorithm: \"\" ssl.truststore.location: /ksml/config/truststore.jks ssl.truststore.password: password-for-truststore # Schema Registry client configuration, needed when schema registry is used schema.registry.url: http://schema-registry:8083 schema.registry.ssl.truststore.location: /ksml/config/truststore.jks schema.registry.ssl.truststore.password: password-for-truststore","title":"Configuration"},{"location":"runners/#using-with-axual-platform-or-other-namespaced-kafka-clusters","text":"A special mode for connecting to clusters that use namespaced Kafka resources is available. This mode can be activated by specifying the namespace pattern to use. This pattern will be resolved to a complete name by KSML using the provided configuration options. The following config will resolve the backing topic of a stream or table kafka: # The patterns for topics, groups and transactional ids. # Each field between the curly braces must be specified in the configuration, except the topic, # group.id and transactional.id fields, which is used to identify the place where the resource name # is used topic.pattern: \"{tenant}-{instance}-{environment}-{topic}\" group.id.pattern: \"{tenant}-{instance}-{environment}-{group.id}\" transactional.id.pattern: \"{tenant}-{instance}-{environment}-{transactional.id}\" # Additional configuration options used for resolving the pattern to values tenant: \"ksmldemo\" instance: \"dta\" environment: \"dev\"","title":"Using with Axual platform or other namespaced Kafka clusters"},{"location":"runners/#starting-a-container","text":"To start a container the KSML definitions and Runner configuration files need to be available in a directory mounted inside the docker container. The default Runner configuration filename is ksml-runner.yaml . If no arguments are given, the runner will look for this file in the home directory ## -w sets the current working directory in the container docker run --rm -ti -v /path/to/local/ksml/directory:/ksml -w /ksml axual/ksml-axual:snapshot ## or docker run --rm -ti -v /path/to/local/ksml/directory:/ksml -w /ksml axual/ksml-axual:snapshot /ksml/ksml-runner.yaml or, if the runner configuration is in a different file, like my-runner.yaml . docker run --rm -ti -v /path/to/local/ksml/directory:/ksml axual/ksml-axual:latest /ksml/my-runner.yaml","title":"Starting a container"},{"location":"sample-tutorial-outline/","text":"Sample Tutorial: Building Your First KSML Data Pipeline This document provides an outline for a sample tutorial that demonstrates the proposed documentation structure. This tutorial would be part of the \"Beginner Tutorials\" section in the new documentation structure. Tutorial Overview This tutorial guides new users through building their first KSML data pipeline, from setup to execution. By the end, users will understand the basic components of KSML and how to create a simple but functional data processing application. Prerequisites Basic understanding of Kafka concepts (topics, messages) Docker installed for running the example environment No prior Kafka Streams or KSML experience required Tutorial Structure 1. Introduction (What You'll Build) Brief explanation of what the tutorial will cover Visual diagram of the pipeline we'll build: Source topic \u2192 Filter \u2192 Transform \u2192 Destination topic Expected outcomes and skills learned 2. Setting Up Your Environment Using Docker Compose to start a local Kafka cluster Verifying the environment is running correctly Understanding the example data we'll be working with 3. Understanding KSML Basics What is a KSML definition file? Key components: streams, functions, pipelines How KSML translates to Kafka Streams topologies 4. Creating Your First KSML Pipeline Step 1: Define Your Streams streams: input_stream: topic: tutorial_input keyType: string valueType: json output_stream: topic: tutorial_output keyType: string valueType: json Explanation of stream definitions Understanding key and value types How KSML handles serialization/deserialization Step 2: Create a Simple Function functions: log_message: type: forEach parameters: - name: message_type type: string code: log.info(\"{} message - key={}, value={}\", message_type, key, value) How functions work in KSML Python code integration Parameter passing and types Step 3: Build Your Pipeline pipelines: tutorial_pipeline: from: input_stream via: - type: filter if: expression: value.get('temperature') > 70 - type: mapValues mapper: expression: {\"sensor\": key, \"temp_fahrenheit\": value.get('temperature'), \"temp_celsius\": (value.get('temperature') - 32) * 5/9} - type: peek forEach: code: log_message(key, value, message_type=\"Processed\") to: output_stream Breaking down each operation Understanding the data flow How operations are chained together 5. Running Your Pipeline Starting the KSML runner Producing test messages to the input topic Observing the filtered and transformed output 6. Exploring and Modifying Suggestions for modifications to try Adding more operations to the pipeline Troubleshooting common issues 7. Next Steps Links to more advanced tutorials Related concepts to explore Reference documentation for operations used Teaching Approach This tutorial uses a hands-on, step-by-step approach with explanations at each stage: What : Clear description of what we're doing Why : Explanation of why this approach is useful How : Detailed instructions with code examples Result : What to expect when the code runs Understanding : Deeper explanation of what's happening behind the scenes Sample Code Snippets with Explanations Example: Explaining the Filter Operation - type: filter if: expression: value.get('temperature') > 70 What's happening here: - The filter operation examines each message and decides whether to keep it or discard it - The if parameter takes a Python expression that evaluates to a boolean (True/False) - In this case, we're only keeping messages where the temperature value is greater than 70\u00b0F - Messages that don't meet this condition are dropped from the pipeline - This is a stateless operation - it doesn't remember anything about previous messages When to use filters: - To reduce the volume of data flowing through your pipeline - To focus on specific conditions or events of interest - As an early step to eliminate irrelevant data before more expensive operations Common patterns: - Filtering by field values - Filtering based on message keys - Combining multiple conditions with logical operators (and, or, not) Conclusion This sample tutorial outline demonstrates how the new documentation structure would provide a more educational and intuitive approach for beginners. It combines practical, hands-on examples with conceptual explanations and context, helping users not just learn the syntax but understand the underlying concepts and best practices.","title":"Sample Tutorial: Building Your First KSML Data Pipeline"},{"location":"sample-tutorial-outline/#sample-tutorial-building-your-first-ksml-data-pipeline","text":"This document provides an outline for a sample tutorial that demonstrates the proposed documentation structure. This tutorial would be part of the \"Beginner Tutorials\" section in the new documentation structure.","title":"Sample Tutorial: Building Your First KSML Data Pipeline"},{"location":"sample-tutorial-outline/#tutorial-overview","text":"This tutorial guides new users through building their first KSML data pipeline, from setup to execution. By the end, users will understand the basic components of KSML and how to create a simple but functional data processing application.","title":"Tutorial Overview"},{"location":"sample-tutorial-outline/#prerequisites","text":"Basic understanding of Kafka concepts (topics, messages) Docker installed for running the example environment No prior Kafka Streams or KSML experience required","title":"Prerequisites"},{"location":"sample-tutorial-outline/#tutorial-structure","text":"","title":"Tutorial Structure"},{"location":"sample-tutorial-outline/#1-introduction-what-youll-build","text":"Brief explanation of what the tutorial will cover Visual diagram of the pipeline we'll build: Source topic \u2192 Filter \u2192 Transform \u2192 Destination topic Expected outcomes and skills learned","title":"1. Introduction (What You'll Build)"},{"location":"sample-tutorial-outline/#2-setting-up-your-environment","text":"Using Docker Compose to start a local Kafka cluster Verifying the environment is running correctly Understanding the example data we'll be working with","title":"2. Setting Up Your Environment"},{"location":"sample-tutorial-outline/#3-understanding-ksml-basics","text":"What is a KSML definition file? Key components: streams, functions, pipelines How KSML translates to Kafka Streams topologies","title":"3. Understanding KSML Basics"},{"location":"sample-tutorial-outline/#4-creating-your-first-ksml-pipeline","text":"","title":"4. Creating Your First KSML Pipeline"},{"location":"sample-tutorial-outline/#step-1-define-your-streams","text":"streams: input_stream: topic: tutorial_input keyType: string valueType: json output_stream: topic: tutorial_output keyType: string valueType: json Explanation of stream definitions Understanding key and value types How KSML handles serialization/deserialization","title":"Step 1: Define Your Streams"},{"location":"sample-tutorial-outline/#step-2-create-a-simple-function","text":"functions: log_message: type: forEach parameters: - name: message_type type: string code: log.info(\"{} message - key={}, value={}\", message_type, key, value) How functions work in KSML Python code integration Parameter passing and types","title":"Step 2: Create a Simple Function"},{"location":"sample-tutorial-outline/#step-3-build-your-pipeline","text":"pipelines: tutorial_pipeline: from: input_stream via: - type: filter if: expression: value.get('temperature') > 70 - type: mapValues mapper: expression: {\"sensor\": key, \"temp_fahrenheit\": value.get('temperature'), \"temp_celsius\": (value.get('temperature') - 32) * 5/9} - type: peek forEach: code: log_message(key, value, message_type=\"Processed\") to: output_stream Breaking down each operation Understanding the data flow How operations are chained together","title":"Step 3: Build Your Pipeline"},{"location":"sample-tutorial-outline/#5-running-your-pipeline","text":"Starting the KSML runner Producing test messages to the input topic Observing the filtered and transformed output","title":"5. Running Your Pipeline"},{"location":"sample-tutorial-outline/#6-exploring-and-modifying","text":"Suggestions for modifications to try Adding more operations to the pipeline Troubleshooting common issues","title":"6. Exploring and Modifying"},{"location":"sample-tutorial-outline/#7-next-steps","text":"Links to more advanced tutorials Related concepts to explore Reference documentation for operations used","title":"7. Next Steps"},{"location":"sample-tutorial-outline/#teaching-approach","text":"This tutorial uses a hands-on, step-by-step approach with explanations at each stage: What : Clear description of what we're doing Why : Explanation of why this approach is useful How : Detailed instructions with code examples Result : What to expect when the code runs Understanding : Deeper explanation of what's happening behind the scenes","title":"Teaching Approach"},{"location":"sample-tutorial-outline/#sample-code-snippets-with-explanations","text":"","title":"Sample Code Snippets with Explanations"},{"location":"sample-tutorial-outline/#example-explaining-the-filter-operation","text":"- type: filter if: expression: value.get('temperature') > 70 What's happening here: - The filter operation examines each message and decides whether to keep it or discard it - The if parameter takes a Python expression that evaluates to a boolean (True/False) - In this case, we're only keeping messages where the temperature value is greater than 70\u00b0F - Messages that don't meet this condition are dropped from the pipeline - This is a stateless operation - it doesn't remember anything about previous messages When to use filters: - To reduce the volume of data flowing through your pipeline - To focus on specific conditions or events of interest - As an early step to eliminate irrelevant data before more expensive operations Common patterns: - Filtering by field values - Filtering based on message keys - Combining multiple conditions with logical operators (and, or, not)","title":"Example: Explaining the Filter Operation"},{"location":"sample-tutorial-outline/#conclusion","text":"This sample tutorial outline demonstrates how the new documentation structure would provide a more educational and intuitive approach for beginners. It combines practical, hands-on examples with conceptual explanations and context, helping users not just learn the syntax but understand the underlying concepts and best practices.","title":"Conclusion"},{"location":"stores/","text":"State Stores Introduction Several stream operations use state stores to retain (intermediate) results of their calculations. State stores are typically configured with some additional parameters to limit their data storage (retention time) and/or determine when data is emitted from them to the next stream operation. stores: owner_count_store: name: owner_count retention: 3m caching: false Configuration State store configurations are defined by the following tags: Parameter Value Type Default Required Description name string none Optional The name of the state store. This field is not mandatory, but operations that use the state store configuration will require a name for their store. If the store configuration does not specify an explicit name, then the operation will default back to the operation's name, specified with its name attribute. If that name is unspecified, then an exception will be thrown. In general, it is considered good practice to always specify the store name explicitly with its definition. type string none Required The type of the state store. Possible types are keyValue , session and window . persistent boolean false Optional true if the state store should be retained on disk. See [link] for more information on how Kafka Streams maintains state store state in a state directory. When this parameter is false or undefined, the state store is (re)built up in memory during upon KSML start. timestamped boolean false Optional (Only relevant for keyValue and window stores) true if all messages in the state store need to be timestamped. This effectively changes the state store from type to . The timestamp contains the last timestamp that updated the aggregated value in the window. versioned boolean false Optional (Only relevant for keyValue stores) true if elements in the store are versioned, false otherwise keyType string none Required The key type of the state store. See Types for more information. valueType string none Required The value type of the state store. See Types for more information. caching boolean false Optional This parameter controls the internal state store caching. When true , the state store caches entries and does not emit every state change but only. When false all changes to the state store will be emitted immediately. logging boolean false Optional This parameter determines whether state changes are written out to a changelog topic, or not. When true all state store changes are produced to a changelog topic. The changelog topic is named appId-storeName-changelog . When false no changelog topic is written to. Example: stores: owner_count_store: name: owner_count retention: 3m caching: false pipelines: main: from: sensor_source via: - type: groupBy name: ksml_sensordata_grouped mapper: expression: value[\"owner\"] resultType: string - type: windowedBy windowType: time duration: 20s grace: 40s - type: aggregate store: owner_count_store # refer to the store configuration above initializer: expression: 0 resultType: long aggregator: expression: aggregatedValue+1 resultType: long Instead of referring to predefined state store configurations, you may also use an inline definition for the store: - type: aggregate store: name: owner_count retention: 3m caching: false initializer: expression: 0 resultType: long aggregator: expression: aggregatedValue+1 resultType: long","title":"State Stores"},{"location":"stores/#state-stores","text":"","title":"State Stores"},{"location":"stores/#introduction","text":"Several stream operations use state stores to retain (intermediate) results of their calculations. State stores are typically configured with some additional parameters to limit their data storage (retention time) and/or determine when data is emitted from them to the next stream operation. stores: owner_count_store: name: owner_count retention: 3m caching: false","title":"Introduction"},{"location":"stores/#configuration","text":"State store configurations are defined by the following tags: Parameter Value Type Default Required Description name string none Optional The name of the state store. This field is not mandatory, but operations that use the state store configuration will require a name for their store. If the store configuration does not specify an explicit name, then the operation will default back to the operation's name, specified with its name attribute. If that name is unspecified, then an exception will be thrown. In general, it is considered good practice to always specify the store name explicitly with its definition. type string none Required The type of the state store. Possible types are keyValue , session and window . persistent boolean false Optional true if the state store should be retained on disk. See [link] for more information on how Kafka Streams maintains state store state in a state directory. When this parameter is false or undefined, the state store is (re)built up in memory during upon KSML start. timestamped boolean false Optional (Only relevant for keyValue and window stores) true if all messages in the state store need to be timestamped. This effectively changes the state store from type to . The timestamp contains the last timestamp that updated the aggregated value in the window. versioned boolean false Optional (Only relevant for keyValue stores) true if elements in the store are versioned, false otherwise keyType string none Required The key type of the state store. See Types for more information. valueType string none Required The value type of the state store. See Types for more information. caching boolean false Optional This parameter controls the internal state store caching. When true , the state store caches entries and does not emit every state change but only. When false all changes to the state store will be emitted immediately. logging boolean false Optional This parameter determines whether state changes are written out to a changelog topic, or not. When true all state store changes are produced to a changelog topic. The changelog topic is named appId-storeName-changelog . When false no changelog topic is written to. Example: stores: owner_count_store: name: owner_count retention: 3m caching: false pipelines: main: from: sensor_source via: - type: groupBy name: ksml_sensordata_grouped mapper: expression: value[\"owner\"] resultType: string - type: windowedBy windowType: time duration: 20s grace: 40s - type: aggregate store: owner_count_store # refer to the store configuration above initializer: expression: 0 resultType: long aggregator: expression: aggregatedValue+1 resultType: long Instead of referring to predefined state store configurations, you may also use an inline definition for the store: - type: aggregate store: name: owner_count retention: 3m caching: false initializer: expression: 0 resultType: long aggregator: expression: aggregatedValue+1 resultType: long","title":"Configuration"},{"location":"streams/","text":"Streams Table of Contents Introduction Stream Table GlobalTable Introduction Every KSML definition file contains a list of declared streams. There are three types of streams supported: Type Kafka Streams equivalent Description Stream KStream KStream is an abstraction of a record stream of KeyValue pairs, i.e., each record is an independent entity/event in the real world. For example a user X might buy two items I1 and I2, and thus there might be two records , in the stream. A KStream is either defined from one or multiple Kafka topics that are consumed message by message or the result of a KStream transformation. A KTable can also be converted into a KStream. A KStream can be transformed record by record, joined with another KStream, KTable, GlobalKTable, or can be aggregated into a KTable. Table KTable KTable is an abstraction of a changelog stream from a primary-keyed table. Each record in this changelog stream is an update on the primary-keyed table with the record key as the primary key. A KTable is either defined from a single Kafka topic that is consumed message by message or the result of a KTable transformation. An aggregation of a KStream also yields a KTable. A KTable can be transformed record by record, joined with another KTable or KStream, or can be re-partitioned and aggregated into a new KTable. GlobalTable GlobalKTable GlobalKTable is an abstraction of a changelog stream from a primary-keyed table. Each record in this changelog stream is an update on the primary-keyed table with the record key as the primary key. GlobalKTable can only be used as right-hand side input for stream-table joins. In contrast to a KTable that is partitioned over all KafkaStreams instances, a GlobalKTable is fully replicated per KafkaStreams instance. Every partition of the underlying topic is consumed by each GlobalKTable, such that the full set of data is available in every KafkaStreams instance. This provides the ability to perform joins with KStream without having to repartition the input stream. All joins with the GlobalKTable require that a KeyValueMapper is provided that can map from the KeyValue of the left hand side KStream to the key of the right hand side GlobalKTable. The definitions of these stream types are done as described below. Stream Example: streams: my_stream_reference: topic: some_kafka_topic keyType: string valueType: string offsetResetPolicy: earliest timestampExtractor: my_timestamp_extractor Table Example: tables: my_table_reference: topic: some_kafka_topic keyType: string valueType: string store: <keyValue state store reference or inline definition> GlobalTable Example: globalTables: my_global_table_reference: topic: some_kafka_topic keyType: string valueType: string","title":"Streams"},{"location":"streams/#streams","text":"","title":"Streams"},{"location":"streams/#table-of-contents","text":"Introduction Stream Table GlobalTable","title":"Table of Contents"},{"location":"streams/#introduction","text":"Every KSML definition file contains a list of declared streams. There are three types of streams supported: Type Kafka Streams equivalent Description Stream KStream KStream is an abstraction of a record stream of KeyValue pairs, i.e., each record is an independent entity/event in the real world. For example a user X might buy two items I1 and I2, and thus there might be two records , in the stream. A KStream is either defined from one or multiple Kafka topics that are consumed message by message or the result of a KStream transformation. A KTable can also be converted into a KStream. A KStream can be transformed record by record, joined with another KStream, KTable, GlobalKTable, or can be aggregated into a KTable. Table KTable KTable is an abstraction of a changelog stream from a primary-keyed table. Each record in this changelog stream is an update on the primary-keyed table with the record key as the primary key. A KTable is either defined from a single Kafka topic that is consumed message by message or the result of a KTable transformation. An aggregation of a KStream also yields a KTable. A KTable can be transformed record by record, joined with another KTable or KStream, or can be re-partitioned and aggregated into a new KTable. GlobalTable GlobalKTable GlobalKTable is an abstraction of a changelog stream from a primary-keyed table. Each record in this changelog stream is an update on the primary-keyed table with the record key as the primary key. GlobalKTable can only be used as right-hand side input for stream-table joins. In contrast to a KTable that is partitioned over all KafkaStreams instances, a GlobalKTable is fully replicated per KafkaStreams instance. Every partition of the underlying topic is consumed by each GlobalKTable, such that the full set of data is available in every KafkaStreams instance. This provides the ability to perform joins with KStream without having to repartition the input stream. All joins with the GlobalKTable require that a KeyValueMapper is provided that can map from the KeyValue of the left hand side KStream to the key of the right hand side GlobalKTable. The definitions of these stream types are done as described below.","title":"Introduction"},{"location":"streams/#stream","text":"Example: streams: my_stream_reference: topic: some_kafka_topic keyType: string valueType: string offsetResetPolicy: earliest timestampExtractor: my_timestamp_extractor","title":"Stream"},{"location":"streams/#table","text":"Example: tables: my_table_reference: topic: some_kafka_topic keyType: string valueType: string store: <keyValue state store reference or inline definition>","title":"Table"},{"location":"streams/#globaltable","text":"Example: globalTables: my_global_table_reference: topic: some_kafka_topic keyType: string valueType: string","title":"GlobalTable"},{"location":"types/","text":"Types Table of Contents Introduction Notations Primitives Any Duration Enum List Struct Tuple Windowed Introduction KSML supports a wide range of simple and complex types. In addition, each data type can be read from Kafka, and written to Kafka, using Notations . More about Notations in the following paragraph. The paragraphs after will dive more into the data types that KSML supports. Notations KSML has a number of simple and complex data types. Simple data types contain a single value. For example, a long or a string type can hold only one value. Complex data types contain multiple values. For example, a list or a struct can contain zero or more other simple or complex values. For example, a struct is comparable to a key/value map, where all keys are of type string . Its contents could be represented as: { \"name\": \"Albert\", \"lastName\": \"Einstein\", \"dateOfBirth\": \"14-08-1879\", \"profession\": \"Patent clerk\", \"children\": 3, \"isAlive\": false } There are several ways in which structured objects are read from or written to Kafka topics. For instance, using AVRO, JSON or XML. Each use the same struct but translate it differently to a binary or string representation that can be stored in Kafka. To facilitate these different ways of writing, KSML supports Notations . Each notation can be seen as 'the format that data is written in' to Kafka. Below is a list of all supported notations: Notation Link Implementations AVRO https://avro.apache.org apicurio_avro, confluent_avro CSV https://en.wikipedia.org/wiki/Comma-separated_values JSON https://json.org SOAP https://en.wikipedia.org/wiki/SOAP XML https://en.wikipedia.org/wiki/XML Notations are a natural extension to data types. They can be used as: notation:datatype For example: avro:SensorData # AVRO with schema SensorData (loaded from SensorData.avsc) json # Schemaless JSON xml:PersonSchema # XML with schema PersonSchema (loaded from PersonSchema.xsd) Primitives The following native types are supported. Type Description ?, or any Any type null, or none Null type, available for variables without a value (eg. Kafka tombstone messages, or optional AVRO fields) boolean Boolean values, ie. true or false double Double precision floating point float Single precision floating point byte 8-bit integer short 16-bit integer int 32-bit integer long 64-bit long bytes Byte array string String of characters struct Key-value map, where with string keys and values of any type Any The special type ? or any can be used in places where input is uncertain. Code that deals with input of this type should always perform proper type checking before assuming any specific underlying type. Duration Some fields in the KSML spec are of type duration . These fields have a fixed format 123x , where 123 is an integer and x is any of the following: : milliseconds s : seconds m : minutes h : hours d : days w : weeks Enum Enumerations can be defined as individual types, through: enum(literal1, literal2, ...) List Lists contain elements of the same type. They are defined using: [elementType] Examples: [string] [long] [avro:SensorData] [(long,string)] Struct Structs are key-value maps, as usually found in AVRO or JSON messages. They are defined using: struct Tuple Tuples combine multiple subtypes into one. For example (1, \"text\") is a tuple containing an integer and a string element. Tuple types always have a fixed number of elements. Examples: (long, string) (avro:SensorData, string, long, string) ([string], long) Union Unions are 'either-or' types. They have their own internal structure and can be described by respective data schema. Unions are defined using: union(type1, type2, ...) Examples: union(null, string) union(avro:SensorData, long, string) Windowed Some Kafka Streams operations modify the key type from K to Windowed\\ . Kafka Streams uses the Windowed\\ type to group Kafka messages with similar keys together. The result is always a time-bound window, with a defined start and end time. KSML can convert the internal Windowed\\ type into a struct type with five fields: start : The window start timestamp (type long ) end : The window end timestamp (type long ) startTime : The window start time (type string ) endTime : The window end time (type string ) key : The key used to group items together in this window (type is the same as the original key type) However, in pipelines or topic definitions users may need to refer to this type explicitly. This is done in the following manner: notation:windowed(keytype) For example: avro:windowed(avro:SensorData) xml:windowed(long)","title":"Types"},{"location":"types/#types","text":"","title":"Types"},{"location":"types/#table-of-contents","text":"Introduction Notations Primitives Any Duration Enum List Struct Tuple Windowed","title":"Table of Contents"},{"location":"types/#introduction","text":"KSML supports a wide range of simple and complex types. In addition, each data type can be read from Kafka, and written to Kafka, using Notations . More about Notations in the following paragraph. The paragraphs after will dive more into the data types that KSML supports.","title":"Introduction"},{"location":"types/#notations","text":"KSML has a number of simple and complex data types. Simple data types contain a single value. For example, a long or a string type can hold only one value. Complex data types contain multiple values. For example, a list or a struct can contain zero or more other simple or complex values. For example, a struct is comparable to a key/value map, where all keys are of type string . Its contents could be represented as: { \"name\": \"Albert\", \"lastName\": \"Einstein\", \"dateOfBirth\": \"14-08-1879\", \"profession\": \"Patent clerk\", \"children\": 3, \"isAlive\": false } There are several ways in which structured objects are read from or written to Kafka topics. For instance, using AVRO, JSON or XML. Each use the same struct but translate it differently to a binary or string representation that can be stored in Kafka. To facilitate these different ways of writing, KSML supports Notations . Each notation can be seen as 'the format that data is written in' to Kafka. Below is a list of all supported notations: Notation Link Implementations AVRO https://avro.apache.org apicurio_avro, confluent_avro CSV https://en.wikipedia.org/wiki/Comma-separated_values JSON https://json.org SOAP https://en.wikipedia.org/wiki/SOAP XML https://en.wikipedia.org/wiki/XML Notations are a natural extension to data types. They can be used as: notation:datatype For example: avro:SensorData # AVRO with schema SensorData (loaded from SensorData.avsc) json # Schemaless JSON xml:PersonSchema # XML with schema PersonSchema (loaded from PersonSchema.xsd)","title":"Notations"},{"location":"types/#primitives","text":"The following native types are supported. Type Description ?, or any Any type null, or none Null type, available for variables without a value (eg. Kafka tombstone messages, or optional AVRO fields) boolean Boolean values, ie. true or false double Double precision floating point float Single precision floating point byte 8-bit integer short 16-bit integer int 32-bit integer long 64-bit long bytes Byte array string String of characters struct Key-value map, where with string keys and values of any type","title":"Primitives"},{"location":"types/#any","text":"The special type ? or any can be used in places where input is uncertain. Code that deals with input of this type should always perform proper type checking before assuming any specific underlying type.","title":"Any"},{"location":"types/#duration","text":"Some fields in the KSML spec are of type duration . These fields have a fixed format 123x , where 123 is an integer and x is any of the following: : milliseconds s : seconds m : minutes h : hours d : days w : weeks","title":"Duration"},{"location":"types/#enum","text":"Enumerations can be defined as individual types, through: enum(literal1, literal2, ...)","title":"Enum"},{"location":"types/#list","text":"Lists contain elements of the same type. They are defined using: [elementType] Examples: [string] [long] [avro:SensorData] [(long,string)]","title":"List"},{"location":"types/#struct","text":"Structs are key-value maps, as usually found in AVRO or JSON messages. They are defined using: struct","title":"Struct"},{"location":"types/#tuple","text":"Tuples combine multiple subtypes into one. For example (1, \"text\") is a tuple containing an integer and a string element. Tuple types always have a fixed number of elements. Examples: (long, string) (avro:SensorData, string, long, string) ([string], long)","title":"Tuple"},{"location":"types/#union","text":"Unions are 'either-or' types. They have their own internal structure and can be described by respective data schema. Unions are defined using: union(type1, type2, ...) Examples: union(null, string) union(avro:SensorData, long, string)","title":"Union"},{"location":"types/#windowed","text":"Some Kafka Streams operations modify the key type from K to Windowed\\ . Kafka Streams uses the Windowed\\ type to group Kafka messages with similar keys together. The result is always a time-bound window, with a defined start and end time. KSML can convert the internal Windowed\\ type into a struct type with five fields: start : The window start timestamp (type long ) end : The window end timestamp (type long ) startTime : The window start time (type string ) endTime : The window end time (type string ) key : The key used to group items together in this window (type is the same as the original key type) However, in pipelines or topic definitions users may need to refer to this type explicitly. This is done in the following manner: notation:windowed(keytype) For example: avro:windowed(avro:SensorData) xml:windowed(long)","title":"Windowed"},{"location":"core-concepts/","text":"Core Concepts Welcome to the KSML Core Concepts section! This section provides in-depth explanations of the fundamental components and concepts that make up KSML. Understanding these core concepts will give you a solid foundation for building effective stream processing applications with KSML, regardless of the specific use case or complexity level. Key KSML Components Data Types Check out which data types KSML applications can handle. What you'll learn is: Primitive data types Complex data types Schema Key and value types Function parameter types Function result types Stream Types Learn about the fundamental building blocks of KSML applications: Understanding stream types (KStream, KTable, GlobalKTable) Choosing the right stream type Best practices Examples Notations Find out how to use different notations for Kafka topics: Key and value types Working with different data formats (Avro, JSON, CSV, etc.) Schema management Serialization and deserialization Functions Discover how to use Python functions in your KSML applications: Types of functions in KSML Writing Python functions Function parameters and return types Reusing functions across pipelines Function execution context and limitations Pipelines Explore how data flows through KSML applications: Pipeline structure and components Input and output configurations Connecting pipelines Best practices for pipeline design Error handling in pipelines Operations Learn about the various operations you can perform on your data: Stateless operations (map, filter, etc.) Stateful operations (aggregate, count, etc.) Windowing operations Joining streams and tables Sink operations Conceptual Understanding Beyond just the syntax and components, this section helps you understand: The \"why\" behind KSML patterns : Not just how to use features, but when and why to use them Mental models for stream processing : How to think about your data as streams and transformations Design principles : Guidelines for creating maintainable and efficient KSML applications Performance considerations : How different operations and patterns affect performance How to Use This Section You can read through these core concepts in order for a comprehensive understanding, or jump to specific topics as needed: Start with Streams and Data Types to understand the basic data model Move on to Pipelines to learn how data flows through your application Explore Functions to see how to implement custom logic Finish with Operations to learn about all the ways you can process your data Each page includes conceptual explanations, code examples, and best practices to help you master KSML. Additional Resources Getting Started: KSML Basics Tutorial - A hands-on introduction to KSML Reference Documentation - Complete technical reference Examples Library - Ready-to-use examples demonstrating these concepts","title":"Core Concepts"},{"location":"core-concepts/#core-concepts","text":"Welcome to the KSML Core Concepts section! This section provides in-depth explanations of the fundamental components and concepts that make up KSML. Understanding these core concepts will give you a solid foundation for building effective stream processing applications with KSML, regardless of the specific use case or complexity level.","title":"Core Concepts"},{"location":"core-concepts/#key-ksml-components","text":"","title":"Key KSML Components"},{"location":"core-concepts/#data-types","text":"Check out which data types KSML applications can handle. What you'll learn is: Primitive data types Complex data types Schema Key and value types Function parameter types Function result types","title":"Data Types"},{"location":"core-concepts/#stream-types","text":"Learn about the fundamental building blocks of KSML applications: Understanding stream types (KStream, KTable, GlobalKTable) Choosing the right stream type Best practices Examples","title":"Stream Types"},{"location":"core-concepts/#notations","text":"Find out how to use different notations for Kafka topics: Key and value types Working with different data formats (Avro, JSON, CSV, etc.) Schema management Serialization and deserialization","title":"Notations"},{"location":"core-concepts/#functions","text":"Discover how to use Python functions in your KSML applications: Types of functions in KSML Writing Python functions Function parameters and return types Reusing functions across pipelines Function execution context and limitations","title":"Functions"},{"location":"core-concepts/#pipelines","text":"Explore how data flows through KSML applications: Pipeline structure and components Input and output configurations Connecting pipelines Best practices for pipeline design Error handling in pipelines","title":"Pipelines"},{"location":"core-concepts/#operations","text":"Learn about the various operations you can perform on your data: Stateless operations (map, filter, etc.) Stateful operations (aggregate, count, etc.) Windowing operations Joining streams and tables Sink operations","title":"Operations"},{"location":"core-concepts/#conceptual-understanding","text":"Beyond just the syntax and components, this section helps you understand: The \"why\" behind KSML patterns : Not just how to use features, but when and why to use them Mental models for stream processing : How to think about your data as streams and transformations Design principles : Guidelines for creating maintainable and efficient KSML applications Performance considerations : How different operations and patterns affect performance","title":"Conceptual Understanding"},{"location":"core-concepts/#how-to-use-this-section","text":"You can read through these core concepts in order for a comprehensive understanding, or jump to specific topics as needed: Start with Streams and Data Types to understand the basic data model Move on to Pipelines to learn how data flows through your application Explore Functions to see how to implement custom logic Finish with Operations to learn about all the ways you can process your data Each page includes conceptual explanations, code examples, and best practices to help you master KSML.","title":"How to Use This Section"},{"location":"core-concepts/#additional-resources","text":"Getting Started: KSML Basics Tutorial - A hands-on introduction to KSML Reference Documentation - Complete technical reference Examples Library - Ready-to-use examples demonstrating these concepts","title":"Additional Resources"},{"location":"core-concepts/functions/","text":"Functions Discover how to use Python functions in your KSML applications and understand their role in stream processing. What are Functions in KSML? Functions in KSML allow you to implement custom logic for processing your streaming data. They provide the flexibility to go beyond the built-in operations and implement specific business logic, transformations, and data processing requirements. KSML functions are written in Python, making them accessible to data scientists, analysts, and developers who may not be familiar with Java or Kafka Streams API. This approach combines the power of Kafka Streams with the simplicity and expressiveness of Python. Types of Functions in KSML KSML supports various function types, each designed for specific purposes in stream processing: Functions used by stateless operations These functions process each message independently without maintaining state between invocations: forEach : Process each message for side effects keyTransformer : Convert a key to another type or value keyValueToKeyValueListTransformer : Convert key and value to a list of key/values keyValueToValueListTransformer : Convert key and value to a list of values keyValueTransformer : Convert key and value to another key and value predicate : Return true/false based on message content valueTransformer : Convert value to another type or value Functions used by stateful operations These functions maintain state across multiple messages: aggregator : Incrementally build aggregated results initializer : Provide initial values for aggregations merger : Merge two aggregation results into one reducer : Combine two values into one Special Purpose Functions foreignKeyExtractor : Extract a key from a join table's record generator : Function used in producers to generate a message keyValueMapper : Convert key and value into a single output value keyValuePrinter : Output key and value metadataTransformer : Convert Kafka headers and timestamps valueJoiner : Combine data from multiple streams Stream Related Functions timestampExtractor : Extract timestamps from messages topicNameExtractor : Derive a target topic name from key and value streamPartitioner : Determine to which partition(s) a record is produced Other Functions generic : Generic custom function Writing Python Functions KSML functions are defined in the functions section of your KSML definition file. A typical function definition includes: Type : Specifies the function's purpose and behavior Parameters : Input parameters the function accepts GlobalCode : Python code that is executed only once upon application start Code : Python code implementing the function's logic Expression : Shorthand for simple return expressions Functions can range from simple one-liners to complex implementations with multiple operations. Function Execution Context When your Python functions execute, they have access to: Logger : For outputting information to the application logs Metrics : For monitoring function performance and behavior State Stores : For maintaining state between function invocations (when configured) This execution context provides the tools needed for debugging, monitoring, and implementing stateful processing. Best Practices for Functions Keep functions focused : Each function should do one thing well Handle errors gracefully : Use try/except blocks to prevent pipeline failures Consider performance : Python functions introduce some overhead, so keep them efficient Use appropriate function types : Choose the right function type for your use case Leverage state stores : For complex stateful operations, use state stores rather than global variables Examples Simple Predicate Function functions: temperature_filter: type: predicate expression: value.get('temperature') > 30 Value Transformation Function functions: celsius_to_fahrenheit: type: valueTransformer code: | if value is not None and 'temperature' in value: celsius = value['temperature'] value['temperature_f'] = (celsius * 9/5) + 32 return value resultType: struct Stateful Aggregation Function functions: average_calculator: type: aggregator code: | if aggregatedValue is None: return {'count': 1, 'sum': value['amount'], 'average': value['amount']} else: count = aggregatedValue['count'] + 1 sum = aggregatedValue['sum'] + value['amount'] return {'count': count, 'sum': sum, 'average': sum / count} resultType: struct Related Topics Pipelines : Learn how functions fit into the overall pipeline structure Operations : Discover the operations that use functions Streams and Data Types : Understand the data types that functions work with By mastering functions in KSML, you gain the ability to implement custom logic that goes beyond the built-in operations, allowing you to solve complex stream processing challenges with elegant Python code.","title":"Functions"},{"location":"core-concepts/functions/#functions","text":"Discover how to use Python functions in your KSML applications and understand their role in stream processing.","title":"Functions"},{"location":"core-concepts/functions/#what-are-functions-in-ksml","text":"Functions in KSML allow you to implement custom logic for processing your streaming data. They provide the flexibility to go beyond the built-in operations and implement specific business logic, transformations, and data processing requirements. KSML functions are written in Python, making them accessible to data scientists, analysts, and developers who may not be familiar with Java or Kafka Streams API. This approach combines the power of Kafka Streams with the simplicity and expressiveness of Python.","title":"What are Functions in KSML?"},{"location":"core-concepts/functions/#types-of-functions-in-ksml","text":"KSML supports various function types, each designed for specific purposes in stream processing:","title":"Types of Functions in KSML"},{"location":"core-concepts/functions/#functions-used-by-stateless-operations","text":"These functions process each message independently without maintaining state between invocations: forEach : Process each message for side effects keyTransformer : Convert a key to another type or value keyValueToKeyValueListTransformer : Convert key and value to a list of key/values keyValueToValueListTransformer : Convert key and value to a list of values keyValueTransformer : Convert key and value to another key and value predicate : Return true/false based on message content valueTransformer : Convert value to another type or value","title":"Functions used by stateless operations"},{"location":"core-concepts/functions/#functions-used-by-stateful-operations","text":"These functions maintain state across multiple messages: aggregator : Incrementally build aggregated results initializer : Provide initial values for aggregations merger : Merge two aggregation results into one reducer : Combine two values into one","title":"Functions used by stateful operations"},{"location":"core-concepts/functions/#special-purpose-functions","text":"foreignKeyExtractor : Extract a key from a join table's record generator : Function used in producers to generate a message keyValueMapper : Convert key and value into a single output value keyValuePrinter : Output key and value metadataTransformer : Convert Kafka headers and timestamps valueJoiner : Combine data from multiple streams","title":"Special Purpose Functions"},{"location":"core-concepts/functions/#stream-related-functions","text":"timestampExtractor : Extract timestamps from messages topicNameExtractor : Derive a target topic name from key and value streamPartitioner : Determine to which partition(s) a record is produced","title":"Stream Related Functions"},{"location":"core-concepts/functions/#other-functions","text":"generic : Generic custom function","title":"Other Functions"},{"location":"core-concepts/functions/#writing-python-functions","text":"KSML functions are defined in the functions section of your KSML definition file. A typical function definition includes: Type : Specifies the function's purpose and behavior Parameters : Input parameters the function accepts GlobalCode : Python code that is executed only once upon application start Code : Python code implementing the function's logic Expression : Shorthand for simple return expressions Functions can range from simple one-liners to complex implementations with multiple operations.","title":"Writing Python Functions"},{"location":"core-concepts/functions/#function-execution-context","text":"When your Python functions execute, they have access to: Logger : For outputting information to the application logs Metrics : For monitoring function performance and behavior State Stores : For maintaining state between function invocations (when configured) This execution context provides the tools needed for debugging, monitoring, and implementing stateful processing.","title":"Function Execution Context"},{"location":"core-concepts/functions/#best-practices-for-functions","text":"Keep functions focused : Each function should do one thing well Handle errors gracefully : Use try/except blocks to prevent pipeline failures Consider performance : Python functions introduce some overhead, so keep them efficient Use appropriate function types : Choose the right function type for your use case Leverage state stores : For complex stateful operations, use state stores rather than global variables","title":"Best Practices for Functions"},{"location":"core-concepts/functions/#examples","text":"","title":"Examples"},{"location":"core-concepts/functions/#simple-predicate-function","text":"functions: temperature_filter: type: predicate expression: value.get('temperature') > 30","title":"Simple Predicate Function"},{"location":"core-concepts/functions/#value-transformation-function","text":"functions: celsius_to_fahrenheit: type: valueTransformer code: | if value is not None and 'temperature' in value: celsius = value['temperature'] value['temperature_f'] = (celsius * 9/5) + 32 return value resultType: struct","title":"Value Transformation Function"},{"location":"core-concepts/functions/#stateful-aggregation-function","text":"functions: average_calculator: type: aggregator code: | if aggregatedValue is None: return {'count': 1, 'sum': value['amount'], 'average': value['amount']} else: count = aggregatedValue['count'] + 1 sum = aggregatedValue['sum'] + value['amount'] return {'count': count, 'sum': sum, 'average': sum / count} resultType: struct","title":"Stateful Aggregation Function"},{"location":"core-concepts/functions/#related-topics","text":"Pipelines : Learn how functions fit into the overall pipeline structure Operations : Discover the operations that use functions Streams and Data Types : Understand the data types that functions work with By mastering functions in KSML, you gain the ability to implement custom logic that goes beyond the built-in operations, allowing you to solve complex stream processing challenges with elegant Python code.","title":"Related Topics"},{"location":"core-concepts/operations/","text":"Operations Learn about the various operations you can perform on your data streams in KSML applications. What are Operations in KSML? Operations are the building blocks of stream processing in KSML. They define how data is transformed, filtered, aggregated, and otherwise processed as it flows through your application. Operations form the middle part of pipelines, taking input from the previous operation and producing output for the next operation. Understanding the different types of operations and when to use them is crucial for building effective stream processing applications. Types of Operations KSML operations can be broadly categorized into several types: Stateless Operations These operations process each message independently, without maintaining state between messages: Map Operations : Transform individual messages (e.g., map , mapValues , mapKey ) Filter Operations : Include or exclude messages based on conditions (e.g., filter , filterNot ) Conversion Operations : Change the format or structure of messages (e.g., convertKey , convertValue ) Stateless operations are typically simpler and more efficient, as they don't require state storage. Stateful Operations These operations maintain state across multiple messages, allowing for more complex processing: Aggregation Operations : Combine multiple messages into a single result (e.g., aggregate , count , reduce ) Join Operations : Combine data from multiple streams (e.g., join , leftJoin , outerJoin ) Windowing Operations : Group and process data within time windows (e.g., windowByTime , windowBySession ) Stateful operations require state stores to maintain their state, which has implications for performance and resource usage. Grouping Operations These operations reorganize messages based on keys: Group By Operations : Group messages by a specific key (e.g., groupBy , groupByKey ) Repartition Operations : Change how messages are distributed across partitions (e.g., repartition ) Grouping operations often precede aggregation operations, as they organize data in a way that makes aggregation possible. Sink Operations These operations represent the end of a pipeline, where data is sent to an external system or another part of your application: Output Operations : Send data to Kafka topics (e.g., to , toTopicNameExtractor ) Terminal Operations : Process data without producing further output (e.g., forEach , print ) Branching Operations : Split a stream into multiple streams based on conditions (e.g., branch ) Common Operations and Their Uses Transforming Data map : Transform both key and value of messages mapValues : Transform only the value of messages (preserves key) flatMap : Transform a message into multiple output messages Filtering Data filter : Include messages that match a condition filterNot : Exclude messages that match a condition Aggregating Data aggregate : Build custom aggregations using an initializer and aggregator function count : Count the number of messages with the same key reduce : Combine messages with the same key using a reducer function Joining Streams join : Inner join of two streams leftJoin : Left join of two streams outerJoin : Full outer join of two streams Working with Windows windowByTime : Group messages into time-based windows windowBySession : Group messages into session-based windows Choosing the Right Operation When designing your KSML application, consider these factors when choosing operations: State Requirements : Stateful operations require more resources Performance Impact : Some operations are more computationally expensive than others Ordering Guarantees : Some operations may affect message ordering Parallelism : Some operations affect how data is partitioned and processed in parallel Examples Transforming Messages pipelines: transform_temperatures: from: temperature_readings mapValues: celsius_to_fahrenheit to: fahrenheit_temperatures Filtering and Branching pipelines: filter_by_region: from: sensor_readings branch: - predicate: is_north_america to: north_america_readings - predicate: is_europe to: europe_readings - to: other_regions_readings Aggregating Data pipelines: calculate_averages: from: sales_data groupByKey: aggregate: initializer: initialize_sales_aggregation aggregator: aggregate_sales to: sales_summaries Related Topics Streams and Data Types : Understand the data types that operations work with Functions : Learn about the functions that power many operations Pipelines : See how operations fit into the overall pipeline structure By understanding the different operations available in KSML and when to use them, you can build powerful stream processing applications that efficiently transform, filter, and aggregate your data.","title":"Operations"},{"location":"core-concepts/operations/#operations","text":"Learn about the various operations you can perform on your data streams in KSML applications.","title":"Operations"},{"location":"core-concepts/operations/#what-are-operations-in-ksml","text":"Operations are the building blocks of stream processing in KSML. They define how data is transformed, filtered, aggregated, and otherwise processed as it flows through your application. Operations form the middle part of pipelines, taking input from the previous operation and producing output for the next operation. Understanding the different types of operations and when to use them is crucial for building effective stream processing applications.","title":"What are Operations in KSML?"},{"location":"core-concepts/operations/#types-of-operations","text":"KSML operations can be broadly categorized into several types:","title":"Types of Operations"},{"location":"core-concepts/operations/#stateless-operations","text":"These operations process each message independently, without maintaining state between messages: Map Operations : Transform individual messages (e.g., map , mapValues , mapKey ) Filter Operations : Include or exclude messages based on conditions (e.g., filter , filterNot ) Conversion Operations : Change the format or structure of messages (e.g., convertKey , convertValue ) Stateless operations are typically simpler and more efficient, as they don't require state storage.","title":"Stateless Operations"},{"location":"core-concepts/operations/#stateful-operations","text":"These operations maintain state across multiple messages, allowing for more complex processing: Aggregation Operations : Combine multiple messages into a single result (e.g., aggregate , count , reduce ) Join Operations : Combine data from multiple streams (e.g., join , leftJoin , outerJoin ) Windowing Operations : Group and process data within time windows (e.g., windowByTime , windowBySession ) Stateful operations require state stores to maintain their state, which has implications for performance and resource usage.","title":"Stateful Operations"},{"location":"core-concepts/operations/#grouping-operations","text":"These operations reorganize messages based on keys: Group By Operations : Group messages by a specific key (e.g., groupBy , groupByKey ) Repartition Operations : Change how messages are distributed across partitions (e.g., repartition ) Grouping operations often precede aggregation operations, as they organize data in a way that makes aggregation possible.","title":"Grouping Operations"},{"location":"core-concepts/operations/#sink-operations","text":"These operations represent the end of a pipeline, where data is sent to an external system or another part of your application: Output Operations : Send data to Kafka topics (e.g., to , toTopicNameExtractor ) Terminal Operations : Process data without producing further output (e.g., forEach , print ) Branching Operations : Split a stream into multiple streams based on conditions (e.g., branch )","title":"Sink Operations"},{"location":"core-concepts/operations/#common-operations-and-their-uses","text":"","title":"Common Operations and Their Uses"},{"location":"core-concepts/operations/#transforming-data","text":"map : Transform both key and value of messages mapValues : Transform only the value of messages (preserves key) flatMap : Transform a message into multiple output messages","title":"Transforming Data"},{"location":"core-concepts/operations/#filtering-data","text":"filter : Include messages that match a condition filterNot : Exclude messages that match a condition","title":"Filtering Data"},{"location":"core-concepts/operations/#aggregating-data","text":"aggregate : Build custom aggregations using an initializer and aggregator function count : Count the number of messages with the same key reduce : Combine messages with the same key using a reducer function","title":"Aggregating Data"},{"location":"core-concepts/operations/#joining-streams","text":"join : Inner join of two streams leftJoin : Left join of two streams outerJoin : Full outer join of two streams","title":"Joining Streams"},{"location":"core-concepts/operations/#working-with-windows","text":"windowByTime : Group messages into time-based windows windowBySession : Group messages into session-based windows","title":"Working with Windows"},{"location":"core-concepts/operations/#choosing-the-right-operation","text":"When designing your KSML application, consider these factors when choosing operations: State Requirements : Stateful operations require more resources Performance Impact : Some operations are more computationally expensive than others Ordering Guarantees : Some operations may affect message ordering Parallelism : Some operations affect how data is partitioned and processed in parallel","title":"Choosing the Right Operation"},{"location":"core-concepts/operations/#examples","text":"","title":"Examples"},{"location":"core-concepts/operations/#transforming-messages","text":"pipelines: transform_temperatures: from: temperature_readings mapValues: celsius_to_fahrenheit to: fahrenheit_temperatures","title":"Transforming Messages"},{"location":"core-concepts/operations/#filtering-and-branching","text":"pipelines: filter_by_region: from: sensor_readings branch: - predicate: is_north_america to: north_america_readings - predicate: is_europe to: europe_readings - to: other_regions_readings","title":"Filtering and Branching"},{"location":"core-concepts/operations/#aggregating-data_1","text":"pipelines: calculate_averages: from: sales_data groupByKey: aggregate: initializer: initialize_sales_aggregation aggregator: aggregate_sales to: sales_summaries","title":"Aggregating Data"},{"location":"core-concepts/operations/#related-topics","text":"Streams and Data Types : Understand the data types that operations work with Functions : Learn about the functions that power many operations Pipelines : See how operations fit into the overall pipeline structure By understanding the different operations available in KSML and when to use them, you can build powerful stream processing applications that efficiently transform, filter, and aggregate your data.","title":"Related Topics"},{"location":"core-concepts/pipelines/","text":"Pipelines This page explains the concept of pipelines in KSML, how they work, and best practices for designing effective data processing flows. What is a Pipeline? In KSML, pipelines form the heart of stream processing logic. A pipeline defines how data flows from one or more input streams through a series of operations and finally to one or more output destinations. Pipelines connect the dots between: - Sources : Where data comes from (Kafka topics via streams, tables, or global tables) - Operations : What happens to the data (transformations, filters, aggregations, etc.) - Sinks : Where the processed data goes (output topics, other pipelines, functions, etc.) Think of a pipeline as a recipe that describes exactly how data should be processed from start to finish. Pipeline Structure Basic Structure Every KSML pipeline has a consistent structure with three main components: pipelines: my_pipeline_name: from: input_stream_name # Source via: # Operations (optional) - type: operation1 # operation parameters - type: operation2 # operation parameters to: output_stream_name # Sink This structure makes pipelines easy to read and understand, with a clear flow from source to operations to sink. Sources: Where Data Comes From The source of a pipeline is specified with the from keyword. It defines where the pipeline gets its data from: from: user_clicks_stream A source can be: - A stream (KStream): For event-based processing - A table (KTable): For state-based processing - A globalTable (GlobalKTable): For reference data - Another pipeline : For chaining pipelines together Sources must be defined elsewhere in your KSML file (in the streams , tables , or globalTables sections) or be the result of a previous pipeline. Operations: Transforming the Data Operations are defined in the via section as a list of transformations to apply to the data: via: - type: filter if: expression: value.get('amount') > 100 - type: mapValues mapper: expression: {\"user\": key, \"amount\": value.get('amount'), \"currency\": \"USD\"} Each operation: - Has a type that defines what it does - Takes parameters specific to that operation type - Receives data from the previous operation (or the source) - Passes its output to the next operation (or the sink) Operations are applied in sequence, creating a processing pipeline where data flows from one operation to the next. Sinks: Where Data Goes The sink defines where the processed data should be sent. KSML supports several sink types: to: processed_orders_stream # Send to a predefined stream Or more complex sinks: branch: # Split the pipeline based on conditions - if: expression: value.get('type') == 'purchase' to: purchases_stream - if: expression: value.get('type') == 'refund' to: refunds_stream - to: other_events_stream # Default branch Common sink types include: - to : Send to a predefined stream, table, or topic - as : Save the result under a name for use in later pipelines - branch : Split the pipeline based on conditions - forEach : Process each message with a function (terminal operation) - print : Output messages for debugging - toTopicNameExtractor : Dynamically determine the output topic Pipeline Patterns and Techniques Connecting Pipelines KSML allows you to connect pipelines together, creating more complex processing flows: pipelines: filter_high_value_orders: from: orders_stream via: - type: filter if: expression: value.get('total') > 1000 as: high_value_orders # Save the result for use in another pipeline process_high_value_orders: from: high_value_orders # Use the result from the previous pipeline via: - type: mapValues mapper: expression: {\"orderId\": value.get('id'), \"amount\": value.get('total'), \"priority\": \"high\"} to: priority_orders_stream This approach allows you to: - Break complex logic into smaller, more manageable pieces - Reuse intermediate results in multiple downstream pipelines - Create cleaner, more maintainable code Branching Pipelines The branch sink allows you to split a pipeline based on conditions: pipelines: route_messages_by_type: from: events_stream branch: - if: expression: value.get('type') == 'click' to: clicks_stream - if: expression: value.get('type') == 'purchase' to: purchases_stream - if: expression: value.get('type') == 'login' to: logins_stream - to: other_events_stream # Default branch for anything else This is useful for: - Routing different types of events to different destinations - Implementing content-based routing patterns - Creating specialized processing flows for different data types Dynamic Output Topics The toTopicNameExtractor sink allows you to dynamically determine the output topic: pipelines: route_to_dynamic_topics: from: events_stream toTopicNameExtractor: expression: \"events-\" + value.get('category').lower() This is useful for: - Partitioning data across multiple topics - Creating topic hierarchies - Implementing multi-tenant architectures Input and Output Configurations Configuring Input Streams When defining input streams, you can configure various properties: streams: orders_stream: topic: orders keyType: string valueType: json offsetResetPolicy: earliest # Start from the beginning of the topic timestampExtractor: order_timestamp_extractor # Custom timestamp extraction Important configuration options include: - offsetResetPolicy : Determines where to start consuming from when no offset is stored - timestampExtractor : Defines how to extract event timestamps from messages - keyType and valueType : Define the data types and formats Configuring Output Streams Similarly, output streams can be configured: streams: processed_orders_stream: topic: processed-orders keyType: string valueType: avro:ProcessedOrder For inline topic definitions in sinks: to: topic: dynamic-output-topic keyType: string valueType: json Best Practices for Pipeline Design 1. Keep Pipelines Focused Each pipeline should have a clear, single responsibility: # Good: Focused pipeline pipelines: enrich_user_events: from: raw_user_events via: - type: join with: user_profiles valueJoiner: expression: {\"event\": value1, \"user\": value2} to: enriched_user_events # Avoid: Pipeline doing too many things pipelines: do_everything: from: raw_events via: - type: filter # Filter logic - type: join # Join logic - type: aggregate # Aggregation logic - type: mapValues # Transformation logic to: final_output 2. Use Meaningful Names Choose descriptive names for pipelines and intermediate results: # Good: Clear naming pipelines: filter_active_users: from: user_events via: - type: filter if: expression: value.get('status') == 'active' as: active_user_events # Avoid: Vague naming pipelines: pipeline1: from: stream1 via: - type: filter if: expression: value.get('x') == 'y' as: filtered_data 3. Chain Pipelines for Complex Logic Break complex processing into multiple pipelines: pipelines: # Step 1: Filter and enrich prepare_data: from: raw_data via: - type: filter # Filter logic - type: join # Enrichment logic as: prepared_data # Step 2: Transform transform_data: from: prepared_data via: - type: mapValues # Transformation logic as: transformed_data # Step 3: Aggregate aggregate_data: from: transformed_data via: - type: groupByKey - type: aggregate # Aggregation logic to: aggregated_results 4. Use Comments for Complex Logic Add comments to explain complex operations: pipelines: process_transactions: from: transactions via: # Filter out test transactions - type: filter if: expression: not value.get('isTest', False) # Convert amounts to standard currency - type: mapValues mapper: functionRef: convert_to_usd # Group by user for aggregation - type: groupBy keySelector: expression: value.get('userId') to: processed_transactions 5. Consider Performance Implications Be mindful of operations that can impact performance: # Potentially expensive: Joining large streams without proper keying pipelines: expensive_join: from: large_stream_1 via: - type: join with: large_stream_2 # Missing proper key selection to: joined_output # Better: Ensure proper keying for efficient joins pipelines: efficient_join: from: large_stream_1 via: - type: selectKey keySelector: expression: value.get('joinKey') - type: join with: large_stream_2 # Now the streams are properly keyed to: joined_output Duration Specification Some pipeline operations require specifying time durations. In KSML, durations are expressed using a simple format: ###x Where: - ### is a positive number - x is an optional time unit: - (none): milliseconds - s : seconds - m : minutes - h : hours - d : days - w : weeks Examples: 100 # 100 milliseconds 30s # 30 seconds 5m # 5 minutes 2h # 2 hours 1d # 1 day 4w # 4 weeks Durations are commonly used in windowing operations: - type: windowedBy timeWindows: size: 5m # 5-minute windows advanceBy: 1m # Advancing every minute (sliding window) Examples Example 1: Simple Filtering and Transformation pipelines: process_sensor_readings: from: raw_sensor_readings via: # Filter out readings with errors - type: filter if: expression: value.get('error') is None # Convert temperature from Celsius to Fahrenheit - type: mapValues mapper: expression: { \"sensorId\": value.get('sensorId'), \"timestamp\": value.get('timestamp'), \"tempF\": value.get('tempC') * 9/5 + 32, \"humidity\": value.get('humidity') } # Add a timestamp for when we processed the reading - type: mapValues mapper: expression: dict(list(value.items()) + [(\"processedAt\", time.time())]) to: processed_sensor_readings Example 2: Joining Streams pipelines: enrich_orders: from: orders via: # Join with customer information - type: join with: customers valueJoiner: expression: { \"orderId\": value1.get('orderId'), \"products\": value1.get('products'), \"total\": value1.get('total'), \"customer\": { \"id\": key, \"name\": value2.get('name'), \"email\": value2.get('email') } } # Join with shipping information - type: join with: shipping_info valueJoiner: expression: dict(list(value1.items()) + [(\"shipping\", value2)]) to: enriched_orders Example 3: Aggregation pipelines: calculate_sales_by_category: from: sales via: # Group by product category - type: groupBy keySelector: expression: value.get('category') # Sum the sales amounts - type: aggregate initializer: expression: {\"count\": 0, \"total\": 0.0} aggregator: expression: { \"count\": aggregate.get('count') + 1, \"total\": aggregate.get('total') + value.get('amount') } # Format the output - type: mapValues mapper: expression: { \"category\": key, \"count\": value.get('count'), \"total\": value.get('total'), \"average\": value.get('total') / value.get('count') if value.get('count') > 0 else 0 } to: sales_by_category Conclusion Pipelines are the core building blocks of KSML applications. By understanding how to design and connect pipelines effectively, you can create powerful stream processing applications that are both maintainable and efficient. For more detailed information about specific operations that can be used in pipelines, see the Operations page. To learn about functions that can be used within pipeline operations, see the Functions page.","title":"Pipelines"},{"location":"core-concepts/pipelines/#pipelines","text":"This page explains the concept of pipelines in KSML, how they work, and best practices for designing effective data processing flows.","title":"Pipelines"},{"location":"core-concepts/pipelines/#what-is-a-pipeline","text":"In KSML, pipelines form the heart of stream processing logic. A pipeline defines how data flows from one or more input streams through a series of operations and finally to one or more output destinations. Pipelines connect the dots between: - Sources : Where data comes from (Kafka topics via streams, tables, or global tables) - Operations : What happens to the data (transformations, filters, aggregations, etc.) - Sinks : Where the processed data goes (output topics, other pipelines, functions, etc.) Think of a pipeline as a recipe that describes exactly how data should be processed from start to finish.","title":"What is a Pipeline?"},{"location":"core-concepts/pipelines/#pipeline-structure","text":"","title":"Pipeline Structure"},{"location":"core-concepts/pipelines/#basic-structure","text":"Every KSML pipeline has a consistent structure with three main components: pipelines: my_pipeline_name: from: input_stream_name # Source via: # Operations (optional) - type: operation1 # operation parameters - type: operation2 # operation parameters to: output_stream_name # Sink This structure makes pipelines easy to read and understand, with a clear flow from source to operations to sink.","title":"Basic Structure"},{"location":"core-concepts/pipelines/#sources-where-data-comes-from","text":"The source of a pipeline is specified with the from keyword. It defines where the pipeline gets its data from: from: user_clicks_stream A source can be: - A stream (KStream): For event-based processing - A table (KTable): For state-based processing - A globalTable (GlobalKTable): For reference data - Another pipeline : For chaining pipelines together Sources must be defined elsewhere in your KSML file (in the streams , tables , or globalTables sections) or be the result of a previous pipeline.","title":"Sources: Where Data Comes From"},{"location":"core-concepts/pipelines/#operations-transforming-the-data","text":"Operations are defined in the via section as a list of transformations to apply to the data: via: - type: filter if: expression: value.get('amount') > 100 - type: mapValues mapper: expression: {\"user\": key, \"amount\": value.get('amount'), \"currency\": \"USD\"} Each operation: - Has a type that defines what it does - Takes parameters specific to that operation type - Receives data from the previous operation (or the source) - Passes its output to the next operation (or the sink) Operations are applied in sequence, creating a processing pipeline where data flows from one operation to the next.","title":"Operations: Transforming the Data"},{"location":"core-concepts/pipelines/#sinks-where-data-goes","text":"The sink defines where the processed data should be sent. KSML supports several sink types: to: processed_orders_stream # Send to a predefined stream Or more complex sinks: branch: # Split the pipeline based on conditions - if: expression: value.get('type') == 'purchase' to: purchases_stream - if: expression: value.get('type') == 'refund' to: refunds_stream - to: other_events_stream # Default branch Common sink types include: - to : Send to a predefined stream, table, or topic - as : Save the result under a name for use in later pipelines - branch : Split the pipeline based on conditions - forEach : Process each message with a function (terminal operation) - print : Output messages for debugging - toTopicNameExtractor : Dynamically determine the output topic","title":"Sinks: Where Data Goes"},{"location":"core-concepts/pipelines/#pipeline-patterns-and-techniques","text":"","title":"Pipeline Patterns and Techniques"},{"location":"core-concepts/pipelines/#connecting-pipelines","text":"KSML allows you to connect pipelines together, creating more complex processing flows: pipelines: filter_high_value_orders: from: orders_stream via: - type: filter if: expression: value.get('total') > 1000 as: high_value_orders # Save the result for use in another pipeline process_high_value_orders: from: high_value_orders # Use the result from the previous pipeline via: - type: mapValues mapper: expression: {\"orderId\": value.get('id'), \"amount\": value.get('total'), \"priority\": \"high\"} to: priority_orders_stream This approach allows you to: - Break complex logic into smaller, more manageable pieces - Reuse intermediate results in multiple downstream pipelines - Create cleaner, more maintainable code","title":"Connecting Pipelines"},{"location":"core-concepts/pipelines/#branching-pipelines","text":"The branch sink allows you to split a pipeline based on conditions: pipelines: route_messages_by_type: from: events_stream branch: - if: expression: value.get('type') == 'click' to: clicks_stream - if: expression: value.get('type') == 'purchase' to: purchases_stream - if: expression: value.get('type') == 'login' to: logins_stream - to: other_events_stream # Default branch for anything else This is useful for: - Routing different types of events to different destinations - Implementing content-based routing patterns - Creating specialized processing flows for different data types","title":"Branching Pipelines"},{"location":"core-concepts/pipelines/#dynamic-output-topics","text":"The toTopicNameExtractor sink allows you to dynamically determine the output topic: pipelines: route_to_dynamic_topics: from: events_stream toTopicNameExtractor: expression: \"events-\" + value.get('category').lower() This is useful for: - Partitioning data across multiple topics - Creating topic hierarchies - Implementing multi-tenant architectures","title":"Dynamic Output Topics"},{"location":"core-concepts/pipelines/#input-and-output-configurations","text":"","title":"Input and Output Configurations"},{"location":"core-concepts/pipelines/#configuring-input-streams","text":"When defining input streams, you can configure various properties: streams: orders_stream: topic: orders keyType: string valueType: json offsetResetPolicy: earliest # Start from the beginning of the topic timestampExtractor: order_timestamp_extractor # Custom timestamp extraction Important configuration options include: - offsetResetPolicy : Determines where to start consuming from when no offset is stored - timestampExtractor : Defines how to extract event timestamps from messages - keyType and valueType : Define the data types and formats","title":"Configuring Input Streams"},{"location":"core-concepts/pipelines/#configuring-output-streams","text":"Similarly, output streams can be configured: streams: processed_orders_stream: topic: processed-orders keyType: string valueType: avro:ProcessedOrder For inline topic definitions in sinks: to: topic: dynamic-output-topic keyType: string valueType: json","title":"Configuring Output Streams"},{"location":"core-concepts/pipelines/#best-practices-for-pipeline-design","text":"","title":"Best Practices for Pipeline Design"},{"location":"core-concepts/pipelines/#1-keep-pipelines-focused","text":"Each pipeline should have a clear, single responsibility: # Good: Focused pipeline pipelines: enrich_user_events: from: raw_user_events via: - type: join with: user_profiles valueJoiner: expression: {\"event\": value1, \"user\": value2} to: enriched_user_events # Avoid: Pipeline doing too many things pipelines: do_everything: from: raw_events via: - type: filter # Filter logic - type: join # Join logic - type: aggregate # Aggregation logic - type: mapValues # Transformation logic to: final_output","title":"1. Keep Pipelines Focused"},{"location":"core-concepts/pipelines/#2-use-meaningful-names","text":"Choose descriptive names for pipelines and intermediate results: # Good: Clear naming pipelines: filter_active_users: from: user_events via: - type: filter if: expression: value.get('status') == 'active' as: active_user_events # Avoid: Vague naming pipelines: pipeline1: from: stream1 via: - type: filter if: expression: value.get('x') == 'y' as: filtered_data","title":"2. Use Meaningful Names"},{"location":"core-concepts/pipelines/#3-chain-pipelines-for-complex-logic","text":"Break complex processing into multiple pipelines: pipelines: # Step 1: Filter and enrich prepare_data: from: raw_data via: - type: filter # Filter logic - type: join # Enrichment logic as: prepared_data # Step 2: Transform transform_data: from: prepared_data via: - type: mapValues # Transformation logic as: transformed_data # Step 3: Aggregate aggregate_data: from: transformed_data via: - type: groupByKey - type: aggregate # Aggregation logic to: aggregated_results","title":"3. Chain Pipelines for Complex Logic"},{"location":"core-concepts/pipelines/#4-use-comments-for-complex-logic","text":"Add comments to explain complex operations: pipelines: process_transactions: from: transactions via: # Filter out test transactions - type: filter if: expression: not value.get('isTest', False) # Convert amounts to standard currency - type: mapValues mapper: functionRef: convert_to_usd # Group by user for aggregation - type: groupBy keySelector: expression: value.get('userId') to: processed_transactions","title":"4. Use Comments for Complex Logic"},{"location":"core-concepts/pipelines/#5-consider-performance-implications","text":"Be mindful of operations that can impact performance: # Potentially expensive: Joining large streams without proper keying pipelines: expensive_join: from: large_stream_1 via: - type: join with: large_stream_2 # Missing proper key selection to: joined_output # Better: Ensure proper keying for efficient joins pipelines: efficient_join: from: large_stream_1 via: - type: selectKey keySelector: expression: value.get('joinKey') - type: join with: large_stream_2 # Now the streams are properly keyed to: joined_output","title":"5. Consider Performance Implications"},{"location":"core-concepts/pipelines/#duration-specification","text":"Some pipeline operations require specifying time durations. In KSML, durations are expressed using a simple format: ###x Where: - ### is a positive number - x is an optional time unit: - (none): milliseconds - s : seconds - m : minutes - h : hours - d : days - w : weeks Examples: 100 # 100 milliseconds 30s # 30 seconds 5m # 5 minutes 2h # 2 hours 1d # 1 day 4w # 4 weeks Durations are commonly used in windowing operations: - type: windowedBy timeWindows: size: 5m # 5-minute windows advanceBy: 1m # Advancing every minute (sliding window)","title":"Duration Specification"},{"location":"core-concepts/pipelines/#examples","text":"","title":"Examples"},{"location":"core-concepts/pipelines/#example-1-simple-filtering-and-transformation","text":"pipelines: process_sensor_readings: from: raw_sensor_readings via: # Filter out readings with errors - type: filter if: expression: value.get('error') is None # Convert temperature from Celsius to Fahrenheit - type: mapValues mapper: expression: { \"sensorId\": value.get('sensorId'), \"timestamp\": value.get('timestamp'), \"tempF\": value.get('tempC') * 9/5 + 32, \"humidity\": value.get('humidity') } # Add a timestamp for when we processed the reading - type: mapValues mapper: expression: dict(list(value.items()) + [(\"processedAt\", time.time())]) to: processed_sensor_readings","title":"Example 1: Simple Filtering and Transformation"},{"location":"core-concepts/pipelines/#example-2-joining-streams","text":"pipelines: enrich_orders: from: orders via: # Join with customer information - type: join with: customers valueJoiner: expression: { \"orderId\": value1.get('orderId'), \"products\": value1.get('products'), \"total\": value1.get('total'), \"customer\": { \"id\": key, \"name\": value2.get('name'), \"email\": value2.get('email') } } # Join with shipping information - type: join with: shipping_info valueJoiner: expression: dict(list(value1.items()) + [(\"shipping\", value2)]) to: enriched_orders","title":"Example 2: Joining Streams"},{"location":"core-concepts/pipelines/#example-3-aggregation","text":"pipelines: calculate_sales_by_category: from: sales via: # Group by product category - type: groupBy keySelector: expression: value.get('category') # Sum the sales amounts - type: aggregate initializer: expression: {\"count\": 0, \"total\": 0.0} aggregator: expression: { \"count\": aggregate.get('count') + 1, \"total\": aggregate.get('total') + value.get('amount') } # Format the output - type: mapValues mapper: expression: { \"category\": key, \"count\": value.get('count'), \"total\": value.get('total'), \"average\": value.get('total') / value.get('count') if value.get('count') > 0 else 0 } to: sales_by_category","title":"Example 3: Aggregation"},{"location":"core-concepts/pipelines/#conclusion","text":"Pipelines are the core building blocks of KSML applications. By understanding how to design and connect pipelines effectively, you can create powerful stream processing applications that are both maintainable and efficient. For more detailed information about specific operations that can be used in pipelines, see the Operations page. To learn about functions that can be used within pipeline operations, see the Functions page.","title":"Conclusion"},{"location":"reference/","text":"Reference Documentation Welcome to the KSML Reference Documentation! This section provides comprehensive technical details about all aspects of KSML. It serves as a complete reference for KSML syntax, operations, functions, data types, and configuration options. While the tutorials and core concepts sections focus on learning and understanding, this reference section is designed for looking up specific details when you need them. Reference Sections KSML Language Reference Complete documentation of the KSML language syntax and structure: YAML structure and formatting Definition file organization Syntax rules and conventions Schema validation Common syntax patterns Data Types Reference Detailed information about all supported data types in KSML: Primitive types (string, integer, etc.) Complex types (JSON, Avro, etc.) Type conversion Serialization and deserialization Schema management Custom data types Notations Reference Explanation of how you can configure the serialization/deserialization formats associated with KSML's notations: Introduction to notations How to configure notations List of available supported variations Functions Reference Complete documentation of KSML function types and usage: Function types (forEach, mapper, predicate, etc.) Function parameters and return values Python code integration Built-in functions Function execution context Best practices for function implementation Operations Reference Detailed documentation for all KSML operations: Stateless operations (map, filter, etc.) Stateful operations (aggregate, count, etc.) Windowing operations Joining operations Sink operations Each operation includes: Syntax and parameters Return values Examples Common use cases Performance considerations Configuration Reference Complete documentation of KSML configuration options: Runner configuration Kafka client configuration Schema Registry configuration Performance tuning options Security settings Logging configuration Environment variables How to Use This Reference This reference documentation is designed to be: Comprehensive : Covering all aspects of KSML Precise : Providing exact syntax and behavior details Practical : Including examples for all features Searchable : Organized to help you find what you need quickly You can use the navigation menu to browse the reference sections, or use the search function to find specific topics. Additional Resources Getting Started - Introduction to KSML for beginners Core Concepts - Conceptual explanations of KSML components Tutorials - Step-by-step guides for learning KSML Examples Library - Ready-to-use examples for common patterns","title":"Reference"},{"location":"reference/#reference-documentation","text":"Welcome to the KSML Reference Documentation! This section provides comprehensive technical details about all aspects of KSML. It serves as a complete reference for KSML syntax, operations, functions, data types, and configuration options. While the tutorials and core concepts sections focus on learning and understanding, this reference section is designed for looking up specific details when you need them.","title":"Reference Documentation"},{"location":"reference/#reference-sections","text":"","title":"Reference Sections"},{"location":"reference/#ksml-language-reference","text":"Complete documentation of the KSML language syntax and structure: YAML structure and formatting Definition file organization Syntax rules and conventions Schema validation Common syntax patterns","title":"KSML Language Reference"},{"location":"reference/#data-types-reference","text":"Detailed information about all supported data types in KSML: Primitive types (string, integer, etc.) Complex types (JSON, Avro, etc.) Type conversion Serialization and deserialization Schema management Custom data types","title":"Data Types Reference"},{"location":"reference/#notations-reference","text":"Explanation of how you can configure the serialization/deserialization formats associated with KSML's notations: Introduction to notations How to configure notations List of available supported variations","title":"Notations Reference"},{"location":"reference/#functions-reference","text":"Complete documentation of KSML function types and usage: Function types (forEach, mapper, predicate, etc.) Function parameters and return values Python code integration Built-in functions Function execution context Best practices for function implementation","title":"Functions Reference"},{"location":"reference/#operations-reference","text":"Detailed documentation for all KSML operations: Stateless operations (map, filter, etc.) Stateful operations (aggregate, count, etc.) Windowing operations Joining operations Sink operations Each operation includes: Syntax and parameters Return values Examples Common use cases Performance considerations","title":"Operations Reference"},{"location":"reference/#configuration-reference","text":"Complete documentation of KSML configuration options: Runner configuration Kafka client configuration Schema Registry configuration Performance tuning options Security settings Logging configuration Environment variables","title":"Configuration Reference"},{"location":"reference/#how-to-use-this-reference","text":"This reference documentation is designed to be: Comprehensive : Covering all aspects of KSML Precise : Providing exact syntax and behavior details Practical : Including examples for all features Searchable : Organized to help you find what you need quickly You can use the navigation menu to browse the reference sections, or use the search function to find specific topics.","title":"How to Use This Reference"},{"location":"reference/#additional-resources","text":"Getting Started - Introduction to KSML for beginners Core Concepts - Conceptual explanations of KSML components Tutorials - Step-by-step guides for learning KSML Examples Library - Ready-to-use examples for common patterns","title":"Additional Resources"},{"location":"reference/configuration-reference/","text":"KSML Configuration Reference This document provides a comprehensive reference for all configuration options available in KSML. Each configuration section is described with its purpose, available options, and examples. KSML Configuration File Structure A KSML configuration file typically consists of the following main sections: # Basic metadata name: \"my-ksml-application\" version: \"1.0.0\" description: \"My KSML Application\" # Data definitions streams: # Stream definitions tables: # Table definitions globalTables: # Global table definitions # Function definitions functions: # Function definitions # Pipeline definitions pipelines: # Pipeline definitions # Application configuration config: # Application configuration Application Metadata Basic Metadata Property Type Required Description name String Yes The name of the KSML definition version String No The version of the KSML definition description String No A description of the KSML definition Example: name: \"order-processing-app\" version: \"1.2.3\" description: \"Processes orders from the order topic and enriches them with customer data\" Data Definitions Streams Streams represent unbounded sequences of records. Property Type Required Description topic String Yes The Kafka topic to read from or write to keyType String Yes The type of the record key valueType String Yes The type of the record value offsetResetPolicy String No The offset reset policy ( earliest , latest , none ) timestampExtractor String No The function to extract timestamps from records Example: streams: orders: topic: \"orders\" keyType: \"string\" valueType: \"avro:Order\" offsetResetPolicy: \"earliest\" Tables Tables represent changelog streams from a primary-keyed table. Property Type Required Description topic String Yes The Kafka topic to read from or write to keyType String Yes The type of the record key valueType String Yes The type of the record value offsetResetPolicy String No The offset reset policy ( earliest , latest , none ) timestampExtractor String No The function to extract timestamps from records store String No The name of the key/value state store to use Example: tables: customers: topic: \"customers\" keyType: \"string\" valueType: \"avro:Customer\" store: \"customer-store\" Global Tables Global tables are similar to tables but are fully replicated on each instance of the application. Property Type Required Description topic String Yes The Kafka topic to read from keyType String Yes The type of the record key valueType String Yes The type of the record value offsetResetPolicy String No The offset reset policy ( earliest , latest , none ) timestampExtractor String No The function to extract timestamps from records store String No The name of the key/value state store to use Example: globalTables: products: topic: \"products\" keyType: \"string\" valueType: \"avro:Product\" Function Definitions Functions define reusable pieces of logic that can be referenced in pipelines. Property Type Required Description type String Yes The type of function (predicate, mapper, aggregator, etc.) expression String No A simple expression for the function code String No Python code implementing the function parameters Array No Parameters for the function Example: functions: is_valid_order: type: \"predicate\" code: | if value is None: return False if \"orderId\" not in value: return False if \"items\" not in value or not value[\"items\"]: return False return True enrich_order: type: \"mapper\" code: | return { \"order_id\": value.get(\"orderId\"), \"customer_id\": value.get(\"customerId\"), \"items\": value.get(\"items\", []), \"total\": sum(item.get(\"price\", 0) * item.get(\"quantity\", 0) for item in value.get(\"items\", [])), \"timestamp\": value.get(\"timestamp\", int(time.time() * 1000)) } Pipeline Definitions Pipelines define the flow of data through the application. Property Type Required Description from String/Array Yes The source stream(s) or table(s) via Array No The operations to apply to the data to String/Array Yes The destination stream(s) Example: pipelines: process_orders: from: \"orders\" via: - type: \"filter\" if: code: \"is_valid_order(key, value)\" - type: \"mapValues\" mapper: code: \"enrich_order(key, value)\" - type: \"peek\" forEach: code: | log.info(\"Processing order: {}\", value.get(\"order_id\")) to: \"processed_orders\" Application Configuration The config section contains application-level configuration options. Kafka Configuration Property Type Required Description bootstrap.servers String Yes Comma-separated list of Kafka broker addresses application.id String Yes The unique identifier for the Kafka Streams application client.id String No The client identifier auto.offset.reset String No Default offset reset policy ( earliest , latest , none ) schema.registry.url String No The URL of the schema registry Example: config: kafka: bootstrap.servers: \"kafka1:9092,kafka2:9092,kafka3:9092\" application.id: \"order-processing-app\" client.id: \"order-processing-client\" auto.offset.reset: \"earliest\" schema.registry.url: \"http://schema-registry:8081\" State Store Configuration Property Type Required Description state.dir String No The directory for state stores cache.max.bytes.buffering Integer No The maximum size of the cache in bytes commit.interval.ms Integer No The commit interval in milliseconds Example: config: state: state.dir: \"/tmp/kafka-streams\" cache.max.bytes.buffering: 10485760 # 10 MB commit.interval.ms: 30000 # 30 seconds Logging Configuration Property Type Required Description level String No The log level ( DEBUG , INFO , WARN , ERROR ) file String No The log file path pattern String No The log pattern Example: config: logging: level: \"INFO\" file: \"/var/log/ksml/application.log\" pattern: \"%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n\" Metrics Configuration Property Type Required Description reporters Array No The metrics reporters to use interval.ms Integer No The metrics reporting interval in milliseconds Example: config: metrics: reporters: - type: \"jmx\" - type: \"prometheus\" port: 8080 interval.ms: 60000 # 60 seconds Security Configuration Property Type Required Description protocol String No The security protocol ( PLAINTEXT , SSL , SASL_PLAINTEXT , SASL_SSL ) ssl Object No SSL configuration sasl Object No SASL configuration Example: config: security: protocol: \"SASL_SSL\" ssl: truststore.location: \"/etc/kafka/ssl/kafka.truststore.jks\" truststore.password: \"${TRUSTSTORE_PASSWORD}\" sasl: mechanism: \"PLAIN\" jaas.config: \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"${KAFKA_USERNAME}\\\" password=\\\"${KAFKA_PASSWORD}\\\";\" Environment Variables KSML supports environment variable substitution in configuration values. Example: config: kafka: bootstrap.servers: \"${KAFKA_BOOTSTRAP_SERVERS}\" application.id: \"${APPLICATION_ID:-order-processing-app}\" # Default value if not set Advanced Configuration Custom Serializers and Deserializers Property Type Required Description key.serializer String No The key serializer class key.deserializer String No The key deserializer class value.serializer String No The value serializer class value.deserializer String No The value deserializer class Example: config: serialization: key.serializer: \"org.apache.kafka.common.serialization.StringSerializer\" key.deserializer: \"org.apache.kafka.common.serialization.StringDeserializer\" value.serializer: \"io.confluent.kafka.serializers.KafkaAvroSerializer\" value.deserializer: \"io.confluent.kafka.serializers.KafkaAvroDeserializer\" Custom State Stores Property Type Required Description name String Yes The name of the state store type String Yes The type of state store ( persistent , in-memory ) config Object No Additional configuration for the state store Example: config: stateStores: - name: \"order-store\" type: \"persistent\" config: retention.ms: 604800000 # 7 days cleanup.policy: \"compact\" Processing Guarantees Property Type Required Description processing.guarantee String No The processing guarantee ( at_least_once , exactly_once , exactly_once_v2 ) Example: config: processing: processing.guarantee: \"exactly_once_v2\" Best Practices Use environment variables for sensitive information : Avoid hardcoding sensitive information like passwords Set appropriate retention periods for state stores : Consider your application's requirements and available disk space Configure appropriate commit intervals : Balance between throughput and recovery time Use descriptive names for streams, tables, and functions : Make your KSML definitions self-documenting Set appropriate log levels : Use INFO for production and DEBUG for development Monitor your application : Configure metrics reporters to track your application's performance Use exactly-once processing guarantees for critical applications : Ensure data integrity for important applications Related Topics KSML Language Reference Operations Reference Functions Reference Data Types Reference","title":"KSML Configuration Reference"},{"location":"reference/configuration-reference/#ksml-configuration-reference","text":"This document provides a comprehensive reference for all configuration options available in KSML. Each configuration section is described with its purpose, available options, and examples.","title":"KSML Configuration Reference"},{"location":"reference/configuration-reference/#ksml-configuration-file-structure","text":"A KSML configuration file typically consists of the following main sections: # Basic metadata name: \"my-ksml-application\" version: \"1.0.0\" description: \"My KSML Application\" # Data definitions streams: # Stream definitions tables: # Table definitions globalTables: # Global table definitions # Function definitions functions: # Function definitions # Pipeline definitions pipelines: # Pipeline definitions # Application configuration config: # Application configuration","title":"KSML Configuration File Structure"},{"location":"reference/configuration-reference/#application-metadata","text":"","title":"Application Metadata"},{"location":"reference/configuration-reference/#basic-metadata","text":"Property Type Required Description name String Yes The name of the KSML definition version String No The version of the KSML definition description String No A description of the KSML definition Example: name: \"order-processing-app\" version: \"1.2.3\" description: \"Processes orders from the order topic and enriches them with customer data\"","title":"Basic Metadata"},{"location":"reference/configuration-reference/#data-definitions","text":"","title":"Data Definitions"},{"location":"reference/configuration-reference/#streams","text":"Streams represent unbounded sequences of records. Property Type Required Description topic String Yes The Kafka topic to read from or write to keyType String Yes The type of the record key valueType String Yes The type of the record value offsetResetPolicy String No The offset reset policy ( earliest , latest , none ) timestampExtractor String No The function to extract timestamps from records Example: streams: orders: topic: \"orders\" keyType: \"string\" valueType: \"avro:Order\" offsetResetPolicy: \"earliest\"","title":"Streams"},{"location":"reference/configuration-reference/#tables","text":"Tables represent changelog streams from a primary-keyed table. Property Type Required Description topic String Yes The Kafka topic to read from or write to keyType String Yes The type of the record key valueType String Yes The type of the record value offsetResetPolicy String No The offset reset policy ( earliest , latest , none ) timestampExtractor String No The function to extract timestamps from records store String No The name of the key/value state store to use Example: tables: customers: topic: \"customers\" keyType: \"string\" valueType: \"avro:Customer\" store: \"customer-store\"","title":"Tables"},{"location":"reference/configuration-reference/#global-tables","text":"Global tables are similar to tables but are fully replicated on each instance of the application. Property Type Required Description topic String Yes The Kafka topic to read from keyType String Yes The type of the record key valueType String Yes The type of the record value offsetResetPolicy String No The offset reset policy ( earliest , latest , none ) timestampExtractor String No The function to extract timestamps from records store String No The name of the key/value state store to use Example: globalTables: products: topic: \"products\" keyType: \"string\" valueType: \"avro:Product\"","title":"Global Tables"},{"location":"reference/configuration-reference/#function-definitions","text":"Functions define reusable pieces of logic that can be referenced in pipelines. Property Type Required Description type String Yes The type of function (predicate, mapper, aggregator, etc.) expression String No A simple expression for the function code String No Python code implementing the function parameters Array No Parameters for the function Example: functions: is_valid_order: type: \"predicate\" code: | if value is None: return False if \"orderId\" not in value: return False if \"items\" not in value or not value[\"items\"]: return False return True enrich_order: type: \"mapper\" code: | return { \"order_id\": value.get(\"orderId\"), \"customer_id\": value.get(\"customerId\"), \"items\": value.get(\"items\", []), \"total\": sum(item.get(\"price\", 0) * item.get(\"quantity\", 0) for item in value.get(\"items\", [])), \"timestamp\": value.get(\"timestamp\", int(time.time() * 1000)) }","title":"Function Definitions"},{"location":"reference/configuration-reference/#pipeline-definitions","text":"Pipelines define the flow of data through the application. Property Type Required Description from String/Array Yes The source stream(s) or table(s) via Array No The operations to apply to the data to String/Array Yes The destination stream(s) Example: pipelines: process_orders: from: \"orders\" via: - type: \"filter\" if: code: \"is_valid_order(key, value)\" - type: \"mapValues\" mapper: code: \"enrich_order(key, value)\" - type: \"peek\" forEach: code: | log.info(\"Processing order: {}\", value.get(\"order_id\")) to: \"processed_orders\"","title":"Pipeline Definitions"},{"location":"reference/configuration-reference/#application-configuration","text":"The config section contains application-level configuration options.","title":"Application Configuration"},{"location":"reference/configuration-reference/#kafka-configuration","text":"Property Type Required Description bootstrap.servers String Yes Comma-separated list of Kafka broker addresses application.id String Yes The unique identifier for the Kafka Streams application client.id String No The client identifier auto.offset.reset String No Default offset reset policy ( earliest , latest , none ) schema.registry.url String No The URL of the schema registry Example: config: kafka: bootstrap.servers: \"kafka1:9092,kafka2:9092,kafka3:9092\" application.id: \"order-processing-app\" client.id: \"order-processing-client\" auto.offset.reset: \"earliest\" schema.registry.url: \"http://schema-registry:8081\"","title":"Kafka Configuration"},{"location":"reference/configuration-reference/#state-store-configuration","text":"Property Type Required Description state.dir String No The directory for state stores cache.max.bytes.buffering Integer No The maximum size of the cache in bytes commit.interval.ms Integer No The commit interval in milliseconds Example: config: state: state.dir: \"/tmp/kafka-streams\" cache.max.bytes.buffering: 10485760 # 10 MB commit.interval.ms: 30000 # 30 seconds","title":"State Store Configuration"},{"location":"reference/configuration-reference/#logging-configuration","text":"Property Type Required Description level String No The log level ( DEBUG , INFO , WARN , ERROR ) file String No The log file path pattern String No The log pattern Example: config: logging: level: \"INFO\" file: \"/var/log/ksml/application.log\" pattern: \"%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n\"","title":"Logging Configuration"},{"location":"reference/configuration-reference/#metrics-configuration","text":"Property Type Required Description reporters Array No The metrics reporters to use interval.ms Integer No The metrics reporting interval in milliseconds Example: config: metrics: reporters: - type: \"jmx\" - type: \"prometheus\" port: 8080 interval.ms: 60000 # 60 seconds","title":"Metrics Configuration"},{"location":"reference/configuration-reference/#security-configuration","text":"Property Type Required Description protocol String No The security protocol ( PLAINTEXT , SSL , SASL_PLAINTEXT , SASL_SSL ) ssl Object No SSL configuration sasl Object No SASL configuration Example: config: security: protocol: \"SASL_SSL\" ssl: truststore.location: \"/etc/kafka/ssl/kafka.truststore.jks\" truststore.password: \"${TRUSTSTORE_PASSWORD}\" sasl: mechanism: \"PLAIN\" jaas.config: \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"${KAFKA_USERNAME}\\\" password=\\\"${KAFKA_PASSWORD}\\\";\"","title":"Security Configuration"},{"location":"reference/configuration-reference/#environment-variables","text":"KSML supports environment variable substitution in configuration values. Example: config: kafka: bootstrap.servers: \"${KAFKA_BOOTSTRAP_SERVERS}\" application.id: \"${APPLICATION_ID:-order-processing-app}\" # Default value if not set","title":"Environment Variables"},{"location":"reference/configuration-reference/#advanced-configuration","text":"","title":"Advanced Configuration"},{"location":"reference/configuration-reference/#custom-serializers-and-deserializers","text":"Property Type Required Description key.serializer String No The key serializer class key.deserializer String No The key deserializer class value.serializer String No The value serializer class value.deserializer String No The value deserializer class Example: config: serialization: key.serializer: \"org.apache.kafka.common.serialization.StringSerializer\" key.deserializer: \"org.apache.kafka.common.serialization.StringDeserializer\" value.serializer: \"io.confluent.kafka.serializers.KafkaAvroSerializer\" value.deserializer: \"io.confluent.kafka.serializers.KafkaAvroDeserializer\"","title":"Custom Serializers and Deserializers"},{"location":"reference/configuration-reference/#custom-state-stores","text":"Property Type Required Description name String Yes The name of the state store type String Yes The type of state store ( persistent , in-memory ) config Object No Additional configuration for the state store Example: config: stateStores: - name: \"order-store\" type: \"persistent\" config: retention.ms: 604800000 # 7 days cleanup.policy: \"compact\"","title":"Custom State Stores"},{"location":"reference/configuration-reference/#processing-guarantees","text":"Property Type Required Description processing.guarantee String No The processing guarantee ( at_least_once , exactly_once , exactly_once_v2 ) Example: config: processing: processing.guarantee: \"exactly_once_v2\"","title":"Processing Guarantees"},{"location":"reference/configuration-reference/#best-practices","text":"Use environment variables for sensitive information : Avoid hardcoding sensitive information like passwords Set appropriate retention periods for state stores : Consider your application's requirements and available disk space Configure appropriate commit intervals : Balance between throughput and recovery time Use descriptive names for streams, tables, and functions : Make your KSML definitions self-documenting Set appropriate log levels : Use INFO for production and DEBUG for development Monitor your application : Configure metrics reporters to track your application's performance Use exactly-once processing guarantees for critical applications : Ensure data integrity for important applications","title":"Best Practices"},{"location":"reference/configuration-reference/#related-topics","text":"KSML Language Reference Operations Reference Functions Reference Data Types Reference","title":"Related Topics"},{"location":"reference/data-types-reference/","text":"KSML Data Types Reference KSML supports a wide range of data types for both keys and values in your streams. Understanding these types is essential for properly defining your streams and functions that process your data. Data Types in KSML Primitive Types KSML supports the following primitive types: Type Description Example Java Equivalent boolean True or false values true , false Boolean byte 8-bit integer 42 Byte short 16-bit integer 1000 Short int 32-bit integer 1000000 Integer long 64-bit integer 9223372036854775807 Long float Single-precision floating point 3.14 Float double Double-precision floating point 3.141592653589793 Double string Text string \"Hello, World!\" String bytes Array of bytes Binary data byte[] null Null value null null Complex Types KSML also supports several complex types that can contain multiple values: Enum An enumeration defines a set of allowed values. Syntax valueType: \"enum(<value1>, <value2>, ...)\" Where <value1> , <value2> , etc. are the allowed values. Example # Enum type for status values streams: order_status_stream: topic: order-statuses keyType: string valueType: \"enum(PENDING, PROCESSING, SHIPPED, DELIVERED, CANCELLED)\" In Python code, an enum value is always represented as a string: functions: update_status: type: valueTransformer code: | if value.get(\"shipped\"): return \"SHIPPED\" elif value.get(\"processing\"): return \"PROCESSING\" expression: \"PENDING\" resultType: string Note that the expression here is mandatory, since the valueTransformer function type determines that the function is required to return a value. List A list contains multiple elements of the same type. Syntax valueType: \"[<element_type>]\" Where <element_type> can be any valid KSML type. Examples # Example 1: List of strings streams: tags_stream: topic: tags keyType: string valueType: \"[string]\" orders_stream: topic: orders keyType: string valueType: \"[struct]\" In Python code, a list is represented as a Python list: functions: extract_tags: type: keyValueToValueListTransformer expression: value.get(\"tags\", []) resultType: \"[string]\" Struct A struct is a key-value map where all keys are strings. This is the most common complex type and is used for JSON objects, AVRO records, etc. Syntax valueType: struct Example streams: user_profiles: topic: user-profiles keyType: string valueType: struct In Python code, a struct is represented as a dictionary: functions: create_user: type: valueTransformer expression: | return { \"id\": value.get(\"user_id\"), \"name\": value.get(\"first_name\") + \" \" + value.get(\"last_name\"), \"email\": value.get(\"email\"), \"age\": value.get(\"age\") } Tuple A tuple combines multiple elements of different types into a single value. Note that the elements in a tuple have no name, only a data type. Syntax valueType: \"(<type1>, <type2>, ...)\" Where <type1> , <type2> , etc. can be any valid KSML type. Note that tuples can be used as keyType or valueType on Kafka topics, since KSML comes with its own schema format for tuples. These schemas are generated dynamically based on the tuple's element types. For example, a tuple defined as (string, int, struct) will lead to the following schema: namespace: io.axual.ksml.data name: TupleOfStringAndIntAndStruct doc: \"Tuple of 3 fields\" fields: - name: elem1 type: string - name: elem2 type: int - name: elem3 type: struct Since tuple schemas are dynamic, when you want to use a tuple as keyType or valueType on a topic, be aware that the tuple is serialized as a struct. So the schema will be registered by the serializer in the corresponding schema registry, or expected to be registered on the topic by the deserializer. If the latter is not the case, but you still want to maintain strong typing, then save the generated schema in your desired format (AVRO, JSON Schema, ...) to a local file from which KSML can read it. Then you will be able to (re)use it in other pipelines as avro:TupleOfStringAndIntAndStruct for AVRO, json:TupleOfStringAndIntAndStruct for JSON, etc. Example # Example: Tuple of string and SensorData streams: sensor_stream: topic: tags keyType: string valueType: \"(string, avro:SensorData)\" In Python code, a tuple is represented as a Python tuple: functions: create_user_age_pair: type: keyValueTransformer expression: | (value.get(\"name\"), value.get(\"age\")) resultType: \"(string, int)\" Union A union type can be one of several possible types. Syntax valueType: \"union(<type1>, <type2>, ...)\" Where <type1> , <type2> , etc. can be any valid KSML type. Example Unions are serializable to Kafka topics. KSML comes with an internal UnionSerde. Upon serialization, it will use the serde that belongs to the value stored in the union. When deserializing it will try all serdes associated with the member types until one is able to decode the message without throwing exceptions. # Union type that can be either null or a string streams: optional_message_stream: topic: optional-messages keyType: string valueType: \"union(null, string)\" In Python code, a union is represented as a value of one of the specified types: functions: get_message: type: valueMapper code: | message = value.get(\"message\") expression: message if message else None Windowed Some operations group messages on an input stream together in user-defined windows. If the input stream's keyType is string , then the resulting stream will have a keyType of windowed(string) . Sometimes you need to specify this type explicitly in your KSML definition, which is why it will be recognized as its own type. Whenever a windowed type is exposed to a Kafka topic, or to Python code, KSML converts the Windowed object from Kafka Streams into a struct with five fields. This is dynamically done, and the output depends on the type of key. For example, a windowed(string) type is converted into the following struct schema: namespace: io.axual.ksml.data name: WindowedString doc: Windowed String fields: - name: start type: long doc: \"Start timestamp in milliseconds\" - name: end type: long doc: \"End timestamp in milliseconds\" - name: startTime type: string doc: \"Start time in UTC\" - name: endTime type: string doc: \"End time in UTC\" - name: key type: string # This is determined by the windowed type within brackets doc: \"Window key\" Syntax keyType: \"windowed(<base_type>)\" Where <base_type> is the type of the key within the window. Example # Windowed key type streams: windowed_counts: topic: windowed-counts keyType: \"windowed(string)\" valueType: long In Python code, a windowed key provides access to both the key and the window: functions: format_windowed_result: type: keyValueTransformer expression: | (None, { \"key\": key.key(), \"start\": key.start(), \"end\": key.end(), \"count\": value }) resultType: \"(null, struct)\" The Any Type The special type ? or any can be used when the exact type is unknown or variable. Code that deals with this type should always perform proper type checking. Syntax # Either of these can be used: # resultType: \"?\" # resultType: \"any\" Example It is not possible to use the any type on a Kafka topic, as KSML would not be able to tie the type to the right serde. Therefore, on Kafka topics you always need to specify a precise type. In Python code, you should check the type before using values of the any type: functions: process_any: type: valueTransformer code: | if isinstance(value, dict): return {\"type\": \"object\", \"value\": value} elif isinstance(value, list): return {\"type\": \"array\", \"value\": value} elif isinstance(value, str): return {\"type\": \"string\", \"value\": value} elif isinstance(value, (int, float)): return {\"type\": \"number\", \"value\": value} expression: | {\"type\": \"unknown\", \"value\": str(value)} resultType: struct Type Conversion KSML automatically performs type conversion wherever required and possible. This holds for numbers (integer to long, etc.) but also for enums, lists, structs, tuples, and unions. Given a value and the desired data type, KSML will try the best possible conversion to allow developers to focus on their logic instead of data type compatibility. Below is an example of a function that returns a string functions: generate_message: type: generator globalCode: | import random sensorCounter = 0 code: | global sensorCounter # Generate key key = \"sensor\"+str(sensorCounter) # Set the key to return (\"sensor0\" to \"sensor9\") sensorCounter = (sensorCounter+1) % 10 # Increase the counter for next iteration # Generate temperature data as CSV, Temperature.csv file contains: \"type,unit,value\" value = \"TEMPERATURE,C,\"+str(random.randrange(-100, 100) expression: (key, value) # Return a message tuple with the key and value resultType: (string, csv:Temperature) # Value is converted to {\"type\":\"TEMPERATURE\", \"unit\":\"C\", \"value\":\"83\"} Best Practices Be specific about your types : Avoid using any when possible Use complex types to represent structured data : Use structs, lists, etc. to represent structured data Consider schema evolution : Use AVRO with a schema registry for production systems Choose the right notation for your use case : Consider compatibility with upstream and downstream systems Validate data : Check that data conforms to expected types and formats Handle missing or null values : Always handle the case where a value might be null Document your types : Add comments to explain complex type structures Related Topics KSML Language Reference Operations Reference Functions Reference Configuration Reference","title":"KSML Data Types Reference"},{"location":"reference/data-types-reference/#ksml-data-types-reference","text":"KSML supports a wide range of data types for both keys and values in your streams. Understanding these types is essential for properly defining your streams and functions that process your data.","title":"KSML Data Types Reference"},{"location":"reference/data-types-reference/#data-types-in-ksml","text":"","title":"Data Types in KSML"},{"location":"reference/data-types-reference/#primitive-types","text":"KSML supports the following primitive types: Type Description Example Java Equivalent boolean True or false values true , false Boolean byte 8-bit integer 42 Byte short 16-bit integer 1000 Short int 32-bit integer 1000000 Integer long 64-bit integer 9223372036854775807 Long float Single-precision floating point 3.14 Float double Double-precision floating point 3.141592653589793 Double string Text string \"Hello, World!\" String bytes Array of bytes Binary data byte[] null Null value null null","title":"Primitive Types"},{"location":"reference/data-types-reference/#complex-types","text":"KSML also supports several complex types that can contain multiple values:","title":"Complex Types"},{"location":"reference/data-types-reference/#enum","text":"An enumeration defines a set of allowed values.","title":"Enum"},{"location":"reference/data-types-reference/#syntax","text":"valueType: \"enum(<value1>, <value2>, ...)\" Where <value1> , <value2> , etc. are the allowed values.","title":"Syntax"},{"location":"reference/data-types-reference/#example","text":"# Enum type for status values streams: order_status_stream: topic: order-statuses keyType: string valueType: \"enum(PENDING, PROCESSING, SHIPPED, DELIVERED, CANCELLED)\" In Python code, an enum value is always represented as a string: functions: update_status: type: valueTransformer code: | if value.get(\"shipped\"): return \"SHIPPED\" elif value.get(\"processing\"): return \"PROCESSING\" expression: \"PENDING\" resultType: string Note that the expression here is mandatory, since the valueTransformer function type determines that the function is required to return a value.","title":"Example"},{"location":"reference/data-types-reference/#list","text":"A list contains multiple elements of the same type.","title":"List"},{"location":"reference/data-types-reference/#syntax_1","text":"valueType: \"[<element_type>]\" Where <element_type> can be any valid KSML type.","title":"Syntax"},{"location":"reference/data-types-reference/#examples","text":"# Example 1: List of strings streams: tags_stream: topic: tags keyType: string valueType: \"[string]\" orders_stream: topic: orders keyType: string valueType: \"[struct]\" In Python code, a list is represented as a Python list: functions: extract_tags: type: keyValueToValueListTransformer expression: value.get(\"tags\", []) resultType: \"[string]\"","title":"Examples"},{"location":"reference/data-types-reference/#struct","text":"A struct is a key-value map where all keys are strings. This is the most common complex type and is used for JSON objects, AVRO records, etc.","title":"Struct"},{"location":"reference/data-types-reference/#syntax_2","text":"valueType: struct","title":"Syntax"},{"location":"reference/data-types-reference/#example_1","text":"streams: user_profiles: topic: user-profiles keyType: string valueType: struct In Python code, a struct is represented as a dictionary: functions: create_user: type: valueTransformer expression: | return { \"id\": value.get(\"user_id\"), \"name\": value.get(\"first_name\") + \" \" + value.get(\"last_name\"), \"email\": value.get(\"email\"), \"age\": value.get(\"age\") }","title":"Example"},{"location":"reference/data-types-reference/#tuple","text":"A tuple combines multiple elements of different types into a single value. Note that the elements in a tuple have no name, only a data type.","title":"Tuple"},{"location":"reference/data-types-reference/#syntax_3","text":"valueType: \"(<type1>, <type2>, ...)\" Where <type1> , <type2> , etc. can be any valid KSML type. Note that tuples can be used as keyType or valueType on Kafka topics, since KSML comes with its own schema format for tuples. These schemas are generated dynamically based on the tuple's element types. For example, a tuple defined as (string, int, struct) will lead to the following schema: namespace: io.axual.ksml.data name: TupleOfStringAndIntAndStruct doc: \"Tuple of 3 fields\" fields: - name: elem1 type: string - name: elem2 type: int - name: elem3 type: struct Since tuple schemas are dynamic, when you want to use a tuple as keyType or valueType on a topic, be aware that the tuple is serialized as a struct. So the schema will be registered by the serializer in the corresponding schema registry, or expected to be registered on the topic by the deserializer. If the latter is not the case, but you still want to maintain strong typing, then save the generated schema in your desired format (AVRO, JSON Schema, ...) to a local file from which KSML can read it. Then you will be able to (re)use it in other pipelines as avro:TupleOfStringAndIntAndStruct for AVRO, json:TupleOfStringAndIntAndStruct for JSON, etc.","title":"Syntax"},{"location":"reference/data-types-reference/#example_2","text":"# Example: Tuple of string and SensorData streams: sensor_stream: topic: tags keyType: string valueType: \"(string, avro:SensorData)\" In Python code, a tuple is represented as a Python tuple: functions: create_user_age_pair: type: keyValueTransformer expression: | (value.get(\"name\"), value.get(\"age\")) resultType: \"(string, int)\"","title":"Example"},{"location":"reference/data-types-reference/#union","text":"A union type can be one of several possible types.","title":"Union"},{"location":"reference/data-types-reference/#syntax_4","text":"valueType: \"union(<type1>, <type2>, ...)\" Where <type1> , <type2> , etc. can be any valid KSML type.","title":"Syntax"},{"location":"reference/data-types-reference/#example_3","text":"Unions are serializable to Kafka topics. KSML comes with an internal UnionSerde. Upon serialization, it will use the serde that belongs to the value stored in the union. When deserializing it will try all serdes associated with the member types until one is able to decode the message without throwing exceptions. # Union type that can be either null or a string streams: optional_message_stream: topic: optional-messages keyType: string valueType: \"union(null, string)\" In Python code, a union is represented as a value of one of the specified types: functions: get_message: type: valueMapper code: | message = value.get(\"message\") expression: message if message else None","title":"Example"},{"location":"reference/data-types-reference/#windowed","text":"Some operations group messages on an input stream together in user-defined windows. If the input stream's keyType is string , then the resulting stream will have a keyType of windowed(string) . Sometimes you need to specify this type explicitly in your KSML definition, which is why it will be recognized as its own type. Whenever a windowed type is exposed to a Kafka topic, or to Python code, KSML converts the Windowed object from Kafka Streams into a struct with five fields. This is dynamically done, and the output depends on the type of key. For example, a windowed(string) type is converted into the following struct schema: namespace: io.axual.ksml.data name: WindowedString doc: Windowed String fields: - name: start type: long doc: \"Start timestamp in milliseconds\" - name: end type: long doc: \"End timestamp in milliseconds\" - name: startTime type: string doc: \"Start time in UTC\" - name: endTime type: string doc: \"End time in UTC\" - name: key type: string # This is determined by the windowed type within brackets doc: \"Window key\"","title":"Windowed"},{"location":"reference/data-types-reference/#syntax_5","text":"keyType: \"windowed(<base_type>)\" Where <base_type> is the type of the key within the window.","title":"Syntax"},{"location":"reference/data-types-reference/#example_4","text":"# Windowed key type streams: windowed_counts: topic: windowed-counts keyType: \"windowed(string)\" valueType: long In Python code, a windowed key provides access to both the key and the window: functions: format_windowed_result: type: keyValueTransformer expression: | (None, { \"key\": key.key(), \"start\": key.start(), \"end\": key.end(), \"count\": value }) resultType: \"(null, struct)\"","title":"Example"},{"location":"reference/data-types-reference/#the-any-type","text":"The special type ? or any can be used when the exact type is unknown or variable. Code that deals with this type should always perform proper type checking.","title":"The Any Type"},{"location":"reference/data-types-reference/#syntax_6","text":"# Either of these can be used: # resultType: \"?\" # resultType: \"any\"","title":"Syntax"},{"location":"reference/data-types-reference/#example_5","text":"It is not possible to use the any type on a Kafka topic, as KSML would not be able to tie the type to the right serde. Therefore, on Kafka topics you always need to specify a precise type. In Python code, you should check the type before using values of the any type: functions: process_any: type: valueTransformer code: | if isinstance(value, dict): return {\"type\": \"object\", \"value\": value} elif isinstance(value, list): return {\"type\": \"array\", \"value\": value} elif isinstance(value, str): return {\"type\": \"string\", \"value\": value} elif isinstance(value, (int, float)): return {\"type\": \"number\", \"value\": value} expression: | {\"type\": \"unknown\", \"value\": str(value)} resultType: struct","title":"Example"},{"location":"reference/data-types-reference/#type-conversion","text":"KSML automatically performs type conversion wherever required and possible. This holds for numbers (integer to long, etc.) but also for enums, lists, structs, tuples, and unions. Given a value and the desired data type, KSML will try the best possible conversion to allow developers to focus on their logic instead of data type compatibility. Below is an example of a function that returns a string functions: generate_message: type: generator globalCode: | import random sensorCounter = 0 code: | global sensorCounter # Generate key key = \"sensor\"+str(sensorCounter) # Set the key to return (\"sensor0\" to \"sensor9\") sensorCounter = (sensorCounter+1) % 10 # Increase the counter for next iteration # Generate temperature data as CSV, Temperature.csv file contains: \"type,unit,value\" value = \"TEMPERATURE,C,\"+str(random.randrange(-100, 100) expression: (key, value) # Return a message tuple with the key and value resultType: (string, csv:Temperature) # Value is converted to {\"type\":\"TEMPERATURE\", \"unit\":\"C\", \"value\":\"83\"}","title":"Type Conversion"},{"location":"reference/data-types-reference/#best-practices","text":"Be specific about your types : Avoid using any when possible Use complex types to represent structured data : Use structs, lists, etc. to represent structured data Consider schema evolution : Use AVRO with a schema registry for production systems Choose the right notation for your use case : Consider compatibility with upstream and downstream systems Validate data : Check that data conforms to expected types and formats Handle missing or null values : Always handle the case where a value might be null Document your types : Add comments to explain complex type structures","title":"Best Practices"},{"location":"reference/data-types-reference/#related-topics","text":"KSML Language Reference Operations Reference Functions Reference Configuration Reference","title":"Related Topics"},{"location":"reference/definition-reference/","text":"KSML Definition Reference This document provides a comprehensive reference for the syntax with which developers can set up their own KSML definitions. Each section is described with its purpose, available options, and examples. KSML Definition File Structure A KSML definition typically consists of the following main sections: # Basic metadata name: \"my-ksml-application\" version: \"1.0.0\" description: \"My KSML Application\" # Data definitions streams: # Stream definitions tables: # Table definitions globalTables: # Global table definitions # State store configuration stores: # State store configurations # Function definitions functions: # Function definitions # Pipeline definitions pipelines: # Pipeline definitions Application Metadata Basic Metadata Property Type Required Description name String No The name of the KSML definition version String No The version of the KSML definition description String No A description of the KSML definition Example: name: \"order-processing-app\" version: \"1.2.3\" description: \"Processes orders from the order topic and enriches them with customer data\" Data source and target definitions Streams Streams represent unbounded sequences of records. Property Type Required Description topic String Yes The Kafka topic to read from or write to keyType String Yes The type of the record key valueType String Yes The type of the record value offsetResetPolicy String No The offset reset policy ( earliest , latest , none ) timestampExtractor String No The function to extract timestamps from records partitioner String No The function that determines to which topic partitions a message is produced Example: streams: orders: topic: \"orders\" keyType: \"string\" valueType: \"avro:Order\" offsetResetPolicy: \"earliest\" Tables Tables represent changelog streams from a primary-keyed table. Property Type Required Description topic String Yes The Kafka topic to read from or write to keyType String Yes The type of the record key valueType String Yes The type of the record value offsetResetPolicy String No The offset reset policy ( earliest , latest , none ) timestampExtractor String No The function to extract timestamps from records partitioner String No The function that determines to which topic partitions a message is produced store String No The name of the key/value state store to use Example: tables: customers: topic: \"customers\" keyType: \"string\" valueType: \"avro:Customer\" store: \"customer-store\" Global Tables Global tables are similar to tables but are fully replicated on each instance of the application. Property Type Required Description topic String Yes The Kafka topic to read from keyType String Yes The type of the record key valueType String Yes The type of the record value offsetResetPolicy String No The offset reset policy ( earliest , latest , none ) timestampExtractor String No The function to extract timestamps from records partitioner String No The function that determines to which topic partitions a message is produced store String No The name of the key/value state store to use Example: globalTables: products: topic: \"products\" keyType: \"string\" valueType: \"avro:Product\" Function Definitions Functions define reusable pieces of logic that can be referenced in pipelines. Property Type Required Description type String Yes The type of function (predicate, mapper, aggregator, etc.) parameters Array No Parameters for the function globalCode String No Python code executed once upon startup code String No Python code implementing the function expression String No An expression that the function will return as value resultType Data type Sometimes The data type returned by the function. This is sometimes derived from the function type , but where it is not, you need to explicitly declare the result type. Example: functions: is_valid_order: type: \"predicate\" code: | if value is None: return False if \"orderId\" not in value: return False if \"items\" not in value or not value[\"items\"]: return False expression: True enrich_order: type: \"mapper\" expression: | { \"order_id\": value.get(\"orderId\"), \"customer_id\": value.get(\"customerId\"), \"items\": value.get(\"items\", []), \"total\": sum(item.get(\"price\", 0) * item.get(\"quantity\", 0) for item in value.get(\"items\", [])), \"timestamp\": value.get(\"timestamp\", int(time.time() * 1000)) } Pipeline Definitions Pipelines define the flow of data through the application. Property Type Required Description from String/Array Yes The source stream(s) or table(s) via Array No The operations to apply to the data to String/Array Yes The destination stream(s) Example: pipelines: process_orders: from: \"orders\" via: - type: \"filter\" if: code: \"is_valid_order(key, value)\" - type: \"mapValues\" mapper: code: \"enrich_order(key, value)\" - type: \"peek\" forEach: code: | log.info(\"Processing order: {}\", value.get(\"order_id\")) to: \"processed_orders\" Application Configuration The config section contains application-level configuration options. Kafka Configuration Property Type Required Description bootstrap.servers String Yes Comma-separated list of Kafka broker addresses application.id String Yes The unique identifier for the Kafka Streams application client.id String No The client identifier auto.offset.reset String No Default offset reset policy ( earliest , latest , none ) schema.registry.url String No The URL of the schema registry Example: config: kafka: bootstrap.servers: \"kafka1:9092,kafka2:9092,kafka3:9092\" application.id: \"order-processing-app\" client.id: \"order-processing-client\" auto.offset.reset: \"earliest\" schema.registry.url: \"http://schema-registry:8081\" State Store Configuration Property Type Required Description state.dir String No The directory for state stores cache.max.bytes.buffering Integer No The maximum size of the cache in bytes commit.interval.ms Integer No The commit interval in milliseconds Example: config: state: state.dir: \"/tmp/kafka-streams\" cache.max.bytes.buffering: 10485760 # 10 MB commit.interval.ms: 30000 # 30 seconds Logging Configuration Property Type Required Description level String No The log level ( DEBUG , INFO , WARN , ERROR ) file String No The log file path pattern String No The log pattern Example: config: logging: level: \"INFO\" file: \"/var/log/ksml/application.log\" pattern: \"%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n\" Metrics Configuration Property Type Required Description reporters Array No The metrics reporters to use interval.ms Integer No The metrics reporting interval in milliseconds Example: config: metrics: reporters: - type: \"jmx\" - type: \"prometheus\" port: 8080 interval.ms: 60000 # 60 seconds Security Configuration Property Type Required Description protocol String No The security protocol ( PLAINTEXT , SSL , SASL_PLAINTEXT , SASL_SSL ) ssl Object No SSL configuration sasl Object No SASL configuration Example: config: security: protocol: \"SASL_SSL\" ssl: truststore.location: \"/etc/kafka/ssl/kafka.truststore.jks\" truststore.password: \"${TRUSTSTORE_PASSWORD}\" sasl: mechanism: \"PLAIN\" jaas.config: \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"${KAFKA_USERNAME}\\\" password=\\\"${KAFKA_PASSWORD}\\\";\" Environment Variables KSML supports environment variable substitution in configuration values. Example: config: kafka: bootstrap.servers: \"${KAFKA_BOOTSTRAP_SERVERS}\" application.id: \"${APPLICATION_ID:-order-processing-app}\" # Default value if not set Advanced Configuration Custom Serializers and Deserializers Property Type Required Description key.serializer String No The key serializer class key.deserializer String No The key deserializer class value.serializer String No The value serializer class value.deserializer String No The value deserializer class Example: config: serialization: key.serializer: \"org.apache.kafka.common.serialization.StringSerializer\" key.deserializer: \"org.apache.kafka.common.serialization.StringDeserializer\" value.serializer: \"io.confluent.kafka.serializers.KafkaAvroSerializer\" value.deserializer: \"io.confluent.kafka.serializers.KafkaAvroDeserializer\" Custom State Stores Property Type Required Description name String Yes The name of the state store type String Yes The type of state store ( persistent , in-memory ) config Object No Additional configuration for the state store Example: config: stateStores: - name: \"order-store\" type: \"persistent\" config: retention.ms: 604800000 # 7 days cleanup.policy: \"compact\" Processing Guarantees Property Type Required Description processing.guarantee String No The processing guarantee ( at_least_once , exactly_once , exactly_once_v2 ) Example: config: processing: processing.guarantee: \"exactly_once_v2\" Best Practices Use environment variables for sensitive information : Avoid hardcoding sensitive information like passwords Set appropriate retention periods for state stores : Consider your application's requirements and available disk space Configure appropriate commit intervals : Balance between throughput and recovery time Use descriptive names for streams, tables, and functions : Make your KSML definitions self-documenting Set appropriate log levels : Use INFO for production and DEBUG for development Monitor your application : Configure metrics reporters to track your application's performance Use exactly-once processing guarantees for critical applications : Ensure data integrity for important applications Related Topics KSML Language Reference Operations Reference Functions Reference Data Types Reference","title":"KSML Definition Reference"},{"location":"reference/definition-reference/#ksml-definition-reference","text":"This document provides a comprehensive reference for the syntax with which developers can set up their own KSML definitions. Each section is described with its purpose, available options, and examples.","title":"KSML Definition Reference"},{"location":"reference/definition-reference/#ksml-definition-file-structure","text":"A KSML definition typically consists of the following main sections: # Basic metadata name: \"my-ksml-application\" version: \"1.0.0\" description: \"My KSML Application\" # Data definitions streams: # Stream definitions tables: # Table definitions globalTables: # Global table definitions # State store configuration stores: # State store configurations # Function definitions functions: # Function definitions # Pipeline definitions pipelines: # Pipeline definitions","title":"KSML Definition File Structure"},{"location":"reference/definition-reference/#application-metadata","text":"","title":"Application Metadata"},{"location":"reference/definition-reference/#basic-metadata","text":"Property Type Required Description name String No The name of the KSML definition version String No The version of the KSML definition description String No A description of the KSML definition Example: name: \"order-processing-app\" version: \"1.2.3\" description: \"Processes orders from the order topic and enriches them with customer data\"","title":"Basic Metadata"},{"location":"reference/definition-reference/#data-source-and-target-definitions","text":"","title":"Data source and target definitions"},{"location":"reference/definition-reference/#streams","text":"Streams represent unbounded sequences of records. Property Type Required Description topic String Yes The Kafka topic to read from or write to keyType String Yes The type of the record key valueType String Yes The type of the record value offsetResetPolicy String No The offset reset policy ( earliest , latest , none ) timestampExtractor String No The function to extract timestamps from records partitioner String No The function that determines to which topic partitions a message is produced Example: streams: orders: topic: \"orders\" keyType: \"string\" valueType: \"avro:Order\" offsetResetPolicy: \"earliest\"","title":"Streams"},{"location":"reference/definition-reference/#tables","text":"Tables represent changelog streams from a primary-keyed table. Property Type Required Description topic String Yes The Kafka topic to read from or write to keyType String Yes The type of the record key valueType String Yes The type of the record value offsetResetPolicy String No The offset reset policy ( earliest , latest , none ) timestampExtractor String No The function to extract timestamps from records partitioner String No The function that determines to which topic partitions a message is produced store String No The name of the key/value state store to use Example: tables: customers: topic: \"customers\" keyType: \"string\" valueType: \"avro:Customer\" store: \"customer-store\"","title":"Tables"},{"location":"reference/definition-reference/#global-tables","text":"Global tables are similar to tables but are fully replicated on each instance of the application. Property Type Required Description topic String Yes The Kafka topic to read from keyType String Yes The type of the record key valueType String Yes The type of the record value offsetResetPolicy String No The offset reset policy ( earliest , latest , none ) timestampExtractor String No The function to extract timestamps from records partitioner String No The function that determines to which topic partitions a message is produced store String No The name of the key/value state store to use Example: globalTables: products: topic: \"products\" keyType: \"string\" valueType: \"avro:Product\"","title":"Global Tables"},{"location":"reference/definition-reference/#function-definitions","text":"Functions define reusable pieces of logic that can be referenced in pipelines. Property Type Required Description type String Yes The type of function (predicate, mapper, aggregator, etc.) parameters Array No Parameters for the function globalCode String No Python code executed once upon startup code String No Python code implementing the function expression String No An expression that the function will return as value resultType Data type Sometimes The data type returned by the function. This is sometimes derived from the function type , but where it is not, you need to explicitly declare the result type. Example: functions: is_valid_order: type: \"predicate\" code: | if value is None: return False if \"orderId\" not in value: return False if \"items\" not in value or not value[\"items\"]: return False expression: True enrich_order: type: \"mapper\" expression: | { \"order_id\": value.get(\"orderId\"), \"customer_id\": value.get(\"customerId\"), \"items\": value.get(\"items\", []), \"total\": sum(item.get(\"price\", 0) * item.get(\"quantity\", 0) for item in value.get(\"items\", [])), \"timestamp\": value.get(\"timestamp\", int(time.time() * 1000)) }","title":"Function Definitions"},{"location":"reference/definition-reference/#pipeline-definitions","text":"Pipelines define the flow of data through the application. Property Type Required Description from String/Array Yes The source stream(s) or table(s) via Array No The operations to apply to the data to String/Array Yes The destination stream(s) Example: pipelines: process_orders: from: \"orders\" via: - type: \"filter\" if: code: \"is_valid_order(key, value)\" - type: \"mapValues\" mapper: code: \"enrich_order(key, value)\" - type: \"peek\" forEach: code: | log.info(\"Processing order: {}\", value.get(\"order_id\")) to: \"processed_orders\"","title":"Pipeline Definitions"},{"location":"reference/definition-reference/#application-configuration","text":"The config section contains application-level configuration options.","title":"Application Configuration"},{"location":"reference/definition-reference/#kafka-configuration","text":"Property Type Required Description bootstrap.servers String Yes Comma-separated list of Kafka broker addresses application.id String Yes The unique identifier for the Kafka Streams application client.id String No The client identifier auto.offset.reset String No Default offset reset policy ( earliest , latest , none ) schema.registry.url String No The URL of the schema registry Example: config: kafka: bootstrap.servers: \"kafka1:9092,kafka2:9092,kafka3:9092\" application.id: \"order-processing-app\" client.id: \"order-processing-client\" auto.offset.reset: \"earliest\" schema.registry.url: \"http://schema-registry:8081\"","title":"Kafka Configuration"},{"location":"reference/definition-reference/#state-store-configuration","text":"Property Type Required Description state.dir String No The directory for state stores cache.max.bytes.buffering Integer No The maximum size of the cache in bytes commit.interval.ms Integer No The commit interval in milliseconds Example: config: state: state.dir: \"/tmp/kafka-streams\" cache.max.bytes.buffering: 10485760 # 10 MB commit.interval.ms: 30000 # 30 seconds","title":"State Store Configuration"},{"location":"reference/definition-reference/#logging-configuration","text":"Property Type Required Description level String No The log level ( DEBUG , INFO , WARN , ERROR ) file String No The log file path pattern String No The log pattern Example: config: logging: level: \"INFO\" file: \"/var/log/ksml/application.log\" pattern: \"%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n\"","title":"Logging Configuration"},{"location":"reference/definition-reference/#metrics-configuration","text":"Property Type Required Description reporters Array No The metrics reporters to use interval.ms Integer No The metrics reporting interval in milliseconds Example: config: metrics: reporters: - type: \"jmx\" - type: \"prometheus\" port: 8080 interval.ms: 60000 # 60 seconds","title":"Metrics Configuration"},{"location":"reference/definition-reference/#security-configuration","text":"Property Type Required Description protocol String No The security protocol ( PLAINTEXT , SSL , SASL_PLAINTEXT , SASL_SSL ) ssl Object No SSL configuration sasl Object No SASL configuration Example: config: security: protocol: \"SASL_SSL\" ssl: truststore.location: \"/etc/kafka/ssl/kafka.truststore.jks\" truststore.password: \"${TRUSTSTORE_PASSWORD}\" sasl: mechanism: \"PLAIN\" jaas.config: \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"${KAFKA_USERNAME}\\\" password=\\\"${KAFKA_PASSWORD}\\\";\"","title":"Security Configuration"},{"location":"reference/definition-reference/#environment-variables","text":"KSML supports environment variable substitution in configuration values. Example: config: kafka: bootstrap.servers: \"${KAFKA_BOOTSTRAP_SERVERS}\" application.id: \"${APPLICATION_ID:-order-processing-app}\" # Default value if not set","title":"Environment Variables"},{"location":"reference/definition-reference/#advanced-configuration","text":"","title":"Advanced Configuration"},{"location":"reference/definition-reference/#custom-serializers-and-deserializers","text":"Property Type Required Description key.serializer String No The key serializer class key.deserializer String No The key deserializer class value.serializer String No The value serializer class value.deserializer String No The value deserializer class Example: config: serialization: key.serializer: \"org.apache.kafka.common.serialization.StringSerializer\" key.deserializer: \"org.apache.kafka.common.serialization.StringDeserializer\" value.serializer: \"io.confluent.kafka.serializers.KafkaAvroSerializer\" value.deserializer: \"io.confluent.kafka.serializers.KafkaAvroDeserializer\"","title":"Custom Serializers and Deserializers"},{"location":"reference/definition-reference/#custom-state-stores","text":"Property Type Required Description name String Yes The name of the state store type String Yes The type of state store ( persistent , in-memory ) config Object No Additional configuration for the state store Example: config: stateStores: - name: \"order-store\" type: \"persistent\" config: retention.ms: 604800000 # 7 days cleanup.policy: \"compact\"","title":"Custom State Stores"},{"location":"reference/definition-reference/#processing-guarantees","text":"Property Type Required Description processing.guarantee String No The processing guarantee ( at_least_once , exactly_once , exactly_once_v2 ) Example: config: processing: processing.guarantee: \"exactly_once_v2\"","title":"Processing Guarantees"},{"location":"reference/definition-reference/#best-practices","text":"Use environment variables for sensitive information : Avoid hardcoding sensitive information like passwords Set appropriate retention periods for state stores : Consider your application's requirements and available disk space Configure appropriate commit intervals : Balance between throughput and recovery time Use descriptive names for streams, tables, and functions : Make your KSML definitions self-documenting Set appropriate log levels : Use INFO for production and DEBUG for development Monitor your application : Configure metrics reporters to track your application's performance Use exactly-once processing guarantees for critical applications : Ensure data integrity for important applications","title":"Best Practices"},{"location":"reference/definition-reference/#related-topics","text":"KSML Language Reference Operations Reference Functions Reference Data Types Reference","title":"Related Topics"},{"location":"reference/functions-reference/","text":"KSML Functions Reference This document provides a comprehensive reference for all function types available in KSML. Each function type is described with its parameters, behavior, and examples. Function Types Overview KSML supports various function types, each designed for specific purposes in stream processing: Function Type Purpose Used In Functions for stateless operations forEach Process each message for side effects peek keyTransformer Convert a key to another type or value mapKey, selectKey, toStream, transformKey keyValueToKeyValueListTransformer Convert key and value to a list of key/values flatMap, transformKeyValueToKeyValueList keyValueToValueListTransformer Convert key and value to a list of values flatMapValues, transformKeyValueToValueList keyValueTransformer Convert key and value to another key and value flatMapValues, transformKeyValueToValueList predicate Return true/false based on message content filter, branch valueTransformer Convert value to another type or value mapValue, mapValues, transformValue Functions for stateful operations aggregator Incrementally build aggregated results aggregate initializer Provide initial values for aggregations aggregate merger Merge two aggregation results into one aggregate reducer Combine two values into one reduce Special Purpose Functions foreignKeyExtractor Extract a key from a join table's record join, leftJoin generator Function used in producers to generate a message producer keyValueMapper Convert key and value into a single output value groupBy, join, leftJoin keyValuePrinter Output key and value print metadataTransformer Convert Kafka headers and timestamps transformMetadata valueJoiner Combine data from multiple streams join, leftJoin, outerJoin Stream Related Functions timestampExtractor Extract timestamps from messages stream, table, globalTable topicNameExtractor Derive a target topic name from key and value toTopicNameExtractor streamPartitioner Determine to which partition(s) a record is produced stream, table, globalTable Other Functions generic Generic custom function Functions for stateless operations forEach Processes each message for side effects like logging, without changing the message. Parameters Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed Return Value None (the function is called for its side effects) Example functions: log_message: type: forEach code: | log.info(\"Processing record with key={}, value={}\", key, value) # You can also increment metrics metrics.counter(\"records_processed\").increment() keyTransformer Transforms a key/value into a new key, which then gets combined with the original value as a new message on the output stream. Parameters Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed Return Value New key for the output message Example functions: extract_region: type: keyTransformer code: | # Extract region from the sale event and use it as the new key return value.get(\"region\", \"unknown\") resultType: string keyValueToKeyValueListTransformer Takes one message and converts it into a list of output messages, which then get sent to the output stream. Parameters Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed Return Value A list of key-value pairs [(key1, value1), (key2, value2), ...] Example functions: alert_split: type: keyValueToKeyValueListTransformer code: | newRecords = [] if value is not None and len(value[\"alerts\"]) > 0: sensordata = value[\"sensordata\"] new_key = { \"name\": sensordata[\"name\"], \"type\": sensordata[\"type\"], \"city\": sensordata[\"city\"] } for alert in value[\"alerts\"]: new_value = { \"alert\": alert, \"sensordata\": sensordata } newRecords.append((new_key, new_value)) return newRecords resultType: \"[(struct,struct)]\" keyValueToValueListTransformer Takes one message and converts it into a list of output values, which then get combined with the original key and sent to the output stream. Parameters Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed Return Value A list of values [value1, value2, ...] that will be combined with the original key Example functions: explode_items: type: keyValueToValueListTransformer code: | # Input: key = \"order123\", value = {\"items\": [{\"id\": \"item1\"}, {\"id\": \"item2\"}]} # Output: (\"order123\", {\"id\": \"item1\"}), (\"order123\", {\"id\": \"item2\"}) if value is None or \"items\" not in value: return [] return value[\"items\"] result: \"[struct]\" keyValueTransformer Takes one message and converts it into another message, which may have different key/value types. Parameters Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed Return Value A tuple of (new_key, new_value) Example functions: transform_order: type: keyValueTransformer code: | if value is None: return (None, None) # Create a new key based on customer ID new_key = value.get(\"customer_id\", \"unknown\") # Create a new value with selected fields new_value = { \"order_id\": value.get(\"order_id\"), \"total_amount\": value.get(\"total_amount\", 0), \"item_count\": len(value.get(\"items\", [])), \"processed_at\": int(time.time() * 1000) } return (new_key, new_value) resultType: \"(string,struct)\" predicate Returns true or false based on message content. Used for filtering and branching operations. Parameters Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed Return Value Boolean (true or false) Example functions: is_adult: type: predicate expression: value.get(\"age\") >= 18 is_valid_transaction: type: predicate code: | if value is None: return False amount = value.get(\"amount\") if amount is None or amount <= 0: return False return True deduplicate_events: type: predicate code: | # Access a state store to check for duplicates event_id = value.get(\"event_id\") if event_id is None: return True # Check if we've seen this event before seen_before = event_store.get(event_id) if seen_before: # Skip duplicate event return False # Mark this event as seen stateStore.put(event_id, True) # Process the event return True stores: - event_store valueTransformer Transforms a key/value into a new value, which is combined with the original key and sent to the output stream. Parameters Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed Return Value New value for the output message Example functions: enrich_user: type: valueTransformer code: | return { \"id\": value.get(\"user_id\"), \"full_name\": value.get(\"first_name\") + \" \" + value.get(\"last_name\"), \"email\": value.get(\"email\"), \"age\": value.get(\"age\"), \"is_adult\": value.get(\"age\", 0) >= 18, \"processed_at\": int(time.time() * 1000) } resultType: struct Functions for stateful operations aggregator Incrementally builds aggregated results from multiple messages. Parameters Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed aggregatedValue Any The current aggregated value (can be None) Return Value New aggregated value Example functions: average_calculator: type: aggregator code: | if aggregatedValue is None: return {\"count\": 1, \"sum\": value.get(\"amount\", 0), \"average\": value.get(\"amount\", 0)} else: count = aggregatedValue.get(\"count\", 0) + 1 sum = aggregatedValue.get(\"sum\", 0) + value.get(\"amount\", 0) return { \"count\": count, \"sum\": sum, \"average\": sum / count } resultType: struct initializer Provides initial values for aggregations. Parameters None Return Value Initial value for aggregation Example functions: counter_initializer: type: initializer expression: { \"count\": 0, \"sum\": 0, \"average\": 0 } resultType: struct merger Merges two aggregation results into one. Used in aggregation operations to combine partial results. Parameters Parameter Type Description key Any The key of the record being processed value1 Any The value of the first aggregation value2 Any The value of the second aggregation Return Value The merged aggregation result Example functions: merge_stats: type: merger code: | # Merge two statistics objects if value1 is None: return value2 if value2 is None: return value1 # Combine counts and sums count = value1.get(\"count\", 0) + value2.get(\"count\", 0) sum = value1.get(\"sum\", 0) + value2.get(\"sum\", 0) result = { \"count\": count, \"sum\": sum, \"average\": sum/count if count>0 else 0 } return result resultType: struct reducer Combines two values into one. Parameters Parameter Type Description value1 Any The first value to combine value2 Any The second value to combine Return Value Combined value Example functions: sum_reducer: type: reducer code: | count = value1.get(\"count\", 0) + value2.get(\"count\", 0) sum = value1.get(\"sum\", 0) + value2.get(\"sum\", 0) return { \"count\": count, \"sum\": sum, \"average\": sum/count if count>0 else 0 } resultType: struct Special Purpose Functions foreignKeyExtractor Extracts a key from a join table's record. Used during join operations to determine which records to join. Parameters Parameter Type Description value Any The value of the record to get a key from Return Value The key to look up in the table being joined with Example functions: extract_customer_id: type: foreignKeyExtractor code: | # Extract customer ID from an order to join with customer table if value is None: return None return value.get(\"customer_id\") resultType: string generator Function used in producers to generate messages. It takes no input parameters and produces key-value pairs. Parameters None Return Value A tuple of (key, value) representing the generated message Example functions: generate_sensordata_message: type: generator globalCode: | import time import random sensorCounter = 0 code: | global sensorCounter key = \"sensor\"+str(sensorCounter) # Set the key to return (\"sensor0\" to \"sensor9\") sensorCounter = (sensorCounter+1) % 10 # Increase the counter for next iteration # Generate some random sensor measurement data types = { 0: { \"type\": \"AREA\", \"unit\": random.choice([ \"m2\", \"ft2\" ]), \"value\": str(random.randrange(1000)) }, 1: { \"type\": \"HUMIDITY\", \"unit\": random.choice([ \"g/m3\", \"%\" ]), \"value\": str(random.randrange(100)) }, 2: { \"type\": \"LENGTH\", \"unit\": random.choice([ \"m\", \"ft\" ]), \"value\": str(random.randrange(1000)) }, 3: { \"type\": \"STATE\", \"unit\": \"state\", \"value\": random.choice([ \"off\", \"on\" ]) }, 4: { \"type\": \"TEMPERATURE\", \"unit\": random.choice([ \"C\", \"F\" ]), \"value\": str(random.randrange(-100, 100)) } } # Build the result value using any of the above measurement types value = { \"name\": key, \"timestamp\": str(round(time.time()*1000)), **random.choice(types) } value[\"color\"] = random.choice([ \"black\", \"blue\", \"red\", \"yellow\", \"white\" ]) value[\"owner\"] = random.choice([ \"Alice\", \"Bob\", \"Charlie\", \"Dave\", \"Evan\" ]) value[\"city\"] = random.choice([ \"Amsterdam\", \"Xanten\", \"Utrecht\", \"Alkmaar\", \"Leiden\" ]) if random.randrange(10) == 0: key = None if random.randrange(10) == 0: value = None expression: (key, value) # Return a message tuple with the key and value resultType: (string, struct) # Indicate the type of key and value keyValueMapper Transforms both the key and value of a record. Parameters Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed Return Value Tuple of (new_key, new_value) Example functions: repartition_by_user_id: type: keyValueMapper code: | new_key = value.get(\"user_id\") new_value = value return (new_key, new_value) resultType: \"(string, struct)\" keyValuePrinter Converts a message to a string for output to a file or stdout. Parameters Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed Return Value String to be written to file or stdout Example functions: format_message: type: keyValuePrinter code: | # Format the message as a JSON string with timestamp import json import time timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) if value is None: return f\"[{timestamp}] Key: {key}, Value: null\" try: # Try to format value as JSON value_str = json.dumps(value, indent=2) return f\"[{timestamp}] Key: {key}\\nValue:\\n{value_str}\" except: # Fall back to string representation return f\"[{timestamp}] Key: {key}, Value: {str(value)}\" metadataTransformer Transforms a message's metadata (headers and timestamp). Parameters Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed metadata Object Contains the headers and timestamp of the message Return Value Modified metadata for the output message Example functions: addTime: type: metadataTransformer code: | # Add a custom header to the message metadata[\"headers\"] = metadata[\"headers\"] + [ { \"key\": \"my_own_header_key\", \"value\": \"some_value\" } ] expression: metadata valueJoiner Combines data from multiple streams during join operations. Parameters Parameter Type Description value1 Any The value from the first stream value2 Any The value from the second stream Return Value Combined value Example functions: join_order_with_customer: type: valueJoiner code: | order = value1 customer = value2 if customer is None: customer_name = \"Unknown\" customer_email = \"Unknown\" else: customer_name = customer.get(\"name\", \"Unknown\") customer_email = customer.get(\"email\", \"Unknown\") return { \"order_id\": order.get(\"order_id\"), \"customer_id\": order.get(\"customer_id\"), \"customer_name\": customer_name, \"customer_email\": customer_email, \"items\": order.get(\"items\", []), \"total\": order.get(\"total\", 0), \"status\": order.get(\"status\", \"PENDING\") } resultType: struct Stream Related Functions timestampExtractor Extracts timestamps from messages for time-based operations. Parameters Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed previousTimestamp Long The previous timestamp (can be used as fallback) Return Value Timestamp in milliseconds (long) Example functions: event_timestamp_extractor: type: timestampExtractor code: | # Try to get timestamp from the event if value is not None and \"timestamp\" in value: return value.get(\"timestamp\") # Fall back to record timestamp return previousTimestamp topicNameExtractor Derives a target topic name from key and value. Used to dynamically route messages to different topics. Parameters Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed Return Value String representing the topic name to send the message to Example functions: route_by_sensor: type: topicNameExtractor code: | if key == 'sensor1': return 'sensordata_sensor1' if key == 'sensor2': return 'sensordata_sensor2' return 'sensordata_other_sensors' streamPartitioner Determines to which partition a record is produced. Used to control the partitioning of output topics. Parameters Parameter Type Description topic String The topic of the message key Any The key of the record being processed value Any The value of the record being processed numPartitions Integer The number of partitions in the output topic Return Value Integer representing the partition number to which the message will be sent Example functions: custom_partitioner: type: streamPartitioner code: | # Partition by the first character of the key (if it's a string) if key is None: # Use default partitioning for null keys return None if isinstance(key, str) and len(key) > 0: # Use the first character's ASCII value modulo number of partitions return ord(key[0]) % numPartitions # For non-string keys, use a hash of the string representation return hash(str(key)) % numPartitions Other Functions generic Generic custom function that can be used for any purpose. It can accept custom parameters and return any type of value. Parameters User-defined parameters Return Value Any value, depending on the function's purpose Example functions: calculate_discount: type: generic parameters: - name: basePrice type: double - name: discountPercentage type: double code: | # Calculate the discounted price discountAmount = basePrice * (discountPercentage / 100) finalPrice = basePrice - discountAmount # Return both the final price and the discount amount return { \"finalPrice\": finalPrice, \"discountAmount\": discountAmount, \"discountPercentage\": discountPercentage } resultType: struct Function Definition Formats KSML supports two formats for defining functions: Expression Format For simple, one-line functions: functions: is_valid: type: predicate code: | # Code is optional here expression: value.get(\"status\") == \"ACTIVE\" Code Block Format For more complex functions: functions: process_transaction: type: keyValueMapper code: | result = {} # Copy basic fields result[\"transaction_id\"] = value.get(\"id\") result[\"amount\"] = value.get(\"amount\", 0) # Calculate fee amount = value.get(\"amount\", 0) if amount > 1000: result[\"fee\"] = amount * 0.02 else: result[\"fee\"] = amount * 0.03 # Add timestamp result[\"processed_at\"] = int(time.time() * 1000) return result resultType: struct Function Execution Context When your Python functions execute, they have access to: Logger For outputting information to the application logs: # In your function code, you can use the log object: # Example: # log.debug(\"Debug message\") # log.info(\"Info message\") # log.warn(\"Warning message\") # log.error(\"Error message\") Metrics For monitoring function performance and behavior: # In your function code, you can use the metrics object: # Examples: # Increment a counter # metrics.counter(\"records_processed\").increment() # # Record a value # metrics.gauge(\"record_size\").record(len(str(value))) State Stores In your function code, you can use the state stores declared by the function as variables: deduplicate_events: type: predicate code: | previous_value = my_state_store.get(key) if previous_value is not None: # Access a state store to check for duplicates event_id = value.get(\"event_id\") if event_id is None: return True # Check if we've seen this event before seen_before = event_store.get(event_id) if seen_before: # Skip duplicate event return False # Mark this event as seen stateStore.put(event_id, True) # Process the event return True stores: - event_store # Examples: # Get a value from the state store # previous_value = state_store.get(key) # # Put a value in the state store # state_store.put(key, new_value) # # Delete a value from the state store # state_store.delete(key) Best Practices Keep functions focused : Each function should do one thing well Handle errors gracefully : Use try/except blocks to prevent pipeline failures Consider performance : Python functions introduce some overhead, so keep them efficient Use appropriate function types : Choose the right function type for your use case Leverage state stores : For complex stateful operations, use state stores rather than global variables Document your functions : Add comments to explain complex logic and business rules Test thoroughly : Write unit tests for your functions to ensure they behave as expected Related Topics KSML Language Reference Operations Reference Data Types Reference Configuration Reference","title":"KSML Functions Reference"},{"location":"reference/functions-reference/#ksml-functions-reference","text":"This document provides a comprehensive reference for all function types available in KSML. Each function type is described with its parameters, behavior, and examples.","title":"KSML Functions Reference"},{"location":"reference/functions-reference/#function-types-overview","text":"KSML supports various function types, each designed for specific purposes in stream processing: Function Type Purpose Used In Functions for stateless operations forEach Process each message for side effects peek keyTransformer Convert a key to another type or value mapKey, selectKey, toStream, transformKey keyValueToKeyValueListTransformer Convert key and value to a list of key/values flatMap, transformKeyValueToKeyValueList keyValueToValueListTransformer Convert key and value to a list of values flatMapValues, transformKeyValueToValueList keyValueTransformer Convert key and value to another key and value flatMapValues, transformKeyValueToValueList predicate Return true/false based on message content filter, branch valueTransformer Convert value to another type or value mapValue, mapValues, transformValue Functions for stateful operations aggregator Incrementally build aggregated results aggregate initializer Provide initial values for aggregations aggregate merger Merge two aggregation results into one aggregate reducer Combine two values into one reduce Special Purpose Functions foreignKeyExtractor Extract a key from a join table's record join, leftJoin generator Function used in producers to generate a message producer keyValueMapper Convert key and value into a single output value groupBy, join, leftJoin keyValuePrinter Output key and value print metadataTransformer Convert Kafka headers and timestamps transformMetadata valueJoiner Combine data from multiple streams join, leftJoin, outerJoin Stream Related Functions timestampExtractor Extract timestamps from messages stream, table, globalTable topicNameExtractor Derive a target topic name from key and value toTopicNameExtractor streamPartitioner Determine to which partition(s) a record is produced stream, table, globalTable Other Functions generic Generic custom function","title":"Function Types Overview"},{"location":"reference/functions-reference/#functions-for-stateless-operations","text":"","title":"Functions for stateless operations"},{"location":"reference/functions-reference/#foreach","text":"Processes each message for side effects like logging, without changing the message.","title":"forEach"},{"location":"reference/functions-reference/#parameters","text":"Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed","title":"Parameters"},{"location":"reference/functions-reference/#return-value","text":"None (the function is called for its side effects)","title":"Return Value"},{"location":"reference/functions-reference/#example","text":"functions: log_message: type: forEach code: | log.info(\"Processing record with key={}, value={}\", key, value) # You can also increment metrics metrics.counter(\"records_processed\").increment()","title":"Example"},{"location":"reference/functions-reference/#keytransformer","text":"Transforms a key/value into a new key, which then gets combined with the original value as a new message on the output stream.","title":"keyTransformer"},{"location":"reference/functions-reference/#parameters_1","text":"Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed","title":"Parameters"},{"location":"reference/functions-reference/#return-value_1","text":"New key for the output message","title":"Return Value"},{"location":"reference/functions-reference/#example_1","text":"functions: extract_region: type: keyTransformer code: | # Extract region from the sale event and use it as the new key return value.get(\"region\", \"unknown\") resultType: string","title":"Example"},{"location":"reference/functions-reference/#keyvaluetokeyvaluelisttransformer","text":"Takes one message and converts it into a list of output messages, which then get sent to the output stream.","title":"keyValueToKeyValueListTransformer"},{"location":"reference/functions-reference/#parameters_2","text":"Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed","title":"Parameters"},{"location":"reference/functions-reference/#return-value_2","text":"A list of key-value pairs [(key1, value1), (key2, value2), ...]","title":"Return Value"},{"location":"reference/functions-reference/#example_2","text":"functions: alert_split: type: keyValueToKeyValueListTransformer code: | newRecords = [] if value is not None and len(value[\"alerts\"]) > 0: sensordata = value[\"sensordata\"] new_key = { \"name\": sensordata[\"name\"], \"type\": sensordata[\"type\"], \"city\": sensordata[\"city\"] } for alert in value[\"alerts\"]: new_value = { \"alert\": alert, \"sensordata\": sensordata } newRecords.append((new_key, new_value)) return newRecords resultType: \"[(struct,struct)]\"","title":"Example"},{"location":"reference/functions-reference/#keyvaluetovaluelisttransformer","text":"Takes one message and converts it into a list of output values, which then get combined with the original key and sent to the output stream.","title":"keyValueToValueListTransformer"},{"location":"reference/functions-reference/#parameters_3","text":"Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed","title":"Parameters"},{"location":"reference/functions-reference/#return-value_3","text":"A list of values [value1, value2, ...] that will be combined with the original key","title":"Return Value"},{"location":"reference/functions-reference/#example_3","text":"functions: explode_items: type: keyValueToValueListTransformer code: | # Input: key = \"order123\", value = {\"items\": [{\"id\": \"item1\"}, {\"id\": \"item2\"}]} # Output: (\"order123\", {\"id\": \"item1\"}), (\"order123\", {\"id\": \"item2\"}) if value is None or \"items\" not in value: return [] return value[\"items\"] result: \"[struct]\"","title":"Example"},{"location":"reference/functions-reference/#keyvaluetransformer","text":"Takes one message and converts it into another message, which may have different key/value types.","title":"keyValueTransformer"},{"location":"reference/functions-reference/#parameters_4","text":"Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed","title":"Parameters"},{"location":"reference/functions-reference/#return-value_4","text":"A tuple of (new_key, new_value)","title":"Return Value"},{"location":"reference/functions-reference/#example_4","text":"functions: transform_order: type: keyValueTransformer code: | if value is None: return (None, None) # Create a new key based on customer ID new_key = value.get(\"customer_id\", \"unknown\") # Create a new value with selected fields new_value = { \"order_id\": value.get(\"order_id\"), \"total_amount\": value.get(\"total_amount\", 0), \"item_count\": len(value.get(\"items\", [])), \"processed_at\": int(time.time() * 1000) } return (new_key, new_value) resultType: \"(string,struct)\"","title":"Example"},{"location":"reference/functions-reference/#predicate","text":"Returns true or false based on message content. Used for filtering and branching operations.","title":"predicate"},{"location":"reference/functions-reference/#parameters_5","text":"Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed","title":"Parameters"},{"location":"reference/functions-reference/#return-value_5","text":"Boolean (true or false)","title":"Return Value"},{"location":"reference/functions-reference/#example_5","text":"functions: is_adult: type: predicate expression: value.get(\"age\") >= 18 is_valid_transaction: type: predicate code: | if value is None: return False amount = value.get(\"amount\") if amount is None or amount <= 0: return False return True deduplicate_events: type: predicate code: | # Access a state store to check for duplicates event_id = value.get(\"event_id\") if event_id is None: return True # Check if we've seen this event before seen_before = event_store.get(event_id) if seen_before: # Skip duplicate event return False # Mark this event as seen stateStore.put(event_id, True) # Process the event return True stores: - event_store","title":"Example"},{"location":"reference/functions-reference/#valuetransformer","text":"Transforms a key/value into a new value, which is combined with the original key and sent to the output stream.","title":"valueTransformer"},{"location":"reference/functions-reference/#parameters_6","text":"Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed","title":"Parameters"},{"location":"reference/functions-reference/#return-value_6","text":"New value for the output message","title":"Return Value"},{"location":"reference/functions-reference/#example_6","text":"functions: enrich_user: type: valueTransformer code: | return { \"id\": value.get(\"user_id\"), \"full_name\": value.get(\"first_name\") + \" \" + value.get(\"last_name\"), \"email\": value.get(\"email\"), \"age\": value.get(\"age\"), \"is_adult\": value.get(\"age\", 0) >= 18, \"processed_at\": int(time.time() * 1000) } resultType: struct","title":"Example"},{"location":"reference/functions-reference/#functions-for-stateful-operations","text":"","title":"Functions for stateful operations"},{"location":"reference/functions-reference/#aggregator","text":"Incrementally builds aggregated results from multiple messages.","title":"aggregator"},{"location":"reference/functions-reference/#parameters_7","text":"Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed aggregatedValue Any The current aggregated value (can be None)","title":"Parameters"},{"location":"reference/functions-reference/#return-value_7","text":"New aggregated value","title":"Return Value"},{"location":"reference/functions-reference/#example_7","text":"functions: average_calculator: type: aggregator code: | if aggregatedValue is None: return {\"count\": 1, \"sum\": value.get(\"amount\", 0), \"average\": value.get(\"amount\", 0)} else: count = aggregatedValue.get(\"count\", 0) + 1 sum = aggregatedValue.get(\"sum\", 0) + value.get(\"amount\", 0) return { \"count\": count, \"sum\": sum, \"average\": sum / count } resultType: struct","title":"Example"},{"location":"reference/functions-reference/#initializer","text":"Provides initial values for aggregations.","title":"initializer"},{"location":"reference/functions-reference/#parameters_8","text":"None","title":"Parameters"},{"location":"reference/functions-reference/#return-value_8","text":"Initial value for aggregation","title":"Return Value"},{"location":"reference/functions-reference/#example_8","text":"functions: counter_initializer: type: initializer expression: { \"count\": 0, \"sum\": 0, \"average\": 0 } resultType: struct","title":"Example"},{"location":"reference/functions-reference/#merger","text":"Merges two aggregation results into one. Used in aggregation operations to combine partial results.","title":"merger"},{"location":"reference/functions-reference/#parameters_9","text":"Parameter Type Description key Any The key of the record being processed value1 Any The value of the first aggregation value2 Any The value of the second aggregation","title":"Parameters"},{"location":"reference/functions-reference/#return-value_9","text":"The merged aggregation result","title":"Return Value"},{"location":"reference/functions-reference/#example_9","text":"functions: merge_stats: type: merger code: | # Merge two statistics objects if value1 is None: return value2 if value2 is None: return value1 # Combine counts and sums count = value1.get(\"count\", 0) + value2.get(\"count\", 0) sum = value1.get(\"sum\", 0) + value2.get(\"sum\", 0) result = { \"count\": count, \"sum\": sum, \"average\": sum/count if count>0 else 0 } return result resultType: struct","title":"Example"},{"location":"reference/functions-reference/#reducer","text":"Combines two values into one.","title":"reducer"},{"location":"reference/functions-reference/#parameters_10","text":"Parameter Type Description value1 Any The first value to combine value2 Any The second value to combine","title":"Parameters"},{"location":"reference/functions-reference/#return-value_10","text":"Combined value","title":"Return Value"},{"location":"reference/functions-reference/#example_10","text":"functions: sum_reducer: type: reducer code: | count = value1.get(\"count\", 0) + value2.get(\"count\", 0) sum = value1.get(\"sum\", 0) + value2.get(\"sum\", 0) return { \"count\": count, \"sum\": sum, \"average\": sum/count if count>0 else 0 } resultType: struct","title":"Example"},{"location":"reference/functions-reference/#special-purpose-functions","text":"","title":"Special Purpose Functions"},{"location":"reference/functions-reference/#foreignkeyextractor","text":"Extracts a key from a join table's record. Used during join operations to determine which records to join.","title":"foreignKeyExtractor"},{"location":"reference/functions-reference/#parameters_11","text":"Parameter Type Description value Any The value of the record to get a key from","title":"Parameters"},{"location":"reference/functions-reference/#return-value_11","text":"The key to look up in the table being joined with","title":"Return Value"},{"location":"reference/functions-reference/#example_11","text":"functions: extract_customer_id: type: foreignKeyExtractor code: | # Extract customer ID from an order to join with customer table if value is None: return None return value.get(\"customer_id\") resultType: string","title":"Example"},{"location":"reference/functions-reference/#generator","text":"Function used in producers to generate messages. It takes no input parameters and produces key-value pairs.","title":"generator"},{"location":"reference/functions-reference/#parameters_12","text":"None","title":"Parameters"},{"location":"reference/functions-reference/#return-value_12","text":"A tuple of (key, value) representing the generated message","title":"Return Value"},{"location":"reference/functions-reference/#example_12","text":"functions: generate_sensordata_message: type: generator globalCode: | import time import random sensorCounter = 0 code: | global sensorCounter key = \"sensor\"+str(sensorCounter) # Set the key to return (\"sensor0\" to \"sensor9\") sensorCounter = (sensorCounter+1) % 10 # Increase the counter for next iteration # Generate some random sensor measurement data types = { 0: { \"type\": \"AREA\", \"unit\": random.choice([ \"m2\", \"ft2\" ]), \"value\": str(random.randrange(1000)) }, 1: { \"type\": \"HUMIDITY\", \"unit\": random.choice([ \"g/m3\", \"%\" ]), \"value\": str(random.randrange(100)) }, 2: { \"type\": \"LENGTH\", \"unit\": random.choice([ \"m\", \"ft\" ]), \"value\": str(random.randrange(1000)) }, 3: { \"type\": \"STATE\", \"unit\": \"state\", \"value\": random.choice([ \"off\", \"on\" ]) }, 4: { \"type\": \"TEMPERATURE\", \"unit\": random.choice([ \"C\", \"F\" ]), \"value\": str(random.randrange(-100, 100)) } } # Build the result value using any of the above measurement types value = { \"name\": key, \"timestamp\": str(round(time.time()*1000)), **random.choice(types) } value[\"color\"] = random.choice([ \"black\", \"blue\", \"red\", \"yellow\", \"white\" ]) value[\"owner\"] = random.choice([ \"Alice\", \"Bob\", \"Charlie\", \"Dave\", \"Evan\" ]) value[\"city\"] = random.choice([ \"Amsterdam\", \"Xanten\", \"Utrecht\", \"Alkmaar\", \"Leiden\" ]) if random.randrange(10) == 0: key = None if random.randrange(10) == 0: value = None expression: (key, value) # Return a message tuple with the key and value resultType: (string, struct) # Indicate the type of key and value","title":"Example"},{"location":"reference/functions-reference/#keyvaluemapper","text":"Transforms both the key and value of a record.","title":"keyValueMapper"},{"location":"reference/functions-reference/#parameters_13","text":"Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed","title":"Parameters"},{"location":"reference/functions-reference/#return-value_13","text":"Tuple of (new_key, new_value)","title":"Return Value"},{"location":"reference/functions-reference/#example_13","text":"functions: repartition_by_user_id: type: keyValueMapper code: | new_key = value.get(\"user_id\") new_value = value return (new_key, new_value) resultType: \"(string, struct)\"","title":"Example"},{"location":"reference/functions-reference/#keyvalueprinter","text":"Converts a message to a string for output to a file or stdout.","title":"keyValuePrinter"},{"location":"reference/functions-reference/#parameters_14","text":"Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed","title":"Parameters"},{"location":"reference/functions-reference/#return-value_14","text":"String to be written to file or stdout","title":"Return Value"},{"location":"reference/functions-reference/#example_14","text":"functions: format_message: type: keyValuePrinter code: | # Format the message as a JSON string with timestamp import json import time timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) if value is None: return f\"[{timestamp}] Key: {key}, Value: null\" try: # Try to format value as JSON value_str = json.dumps(value, indent=2) return f\"[{timestamp}] Key: {key}\\nValue:\\n{value_str}\" except: # Fall back to string representation return f\"[{timestamp}] Key: {key}, Value: {str(value)}\"","title":"Example"},{"location":"reference/functions-reference/#metadatatransformer","text":"Transforms a message's metadata (headers and timestamp).","title":"metadataTransformer"},{"location":"reference/functions-reference/#parameters_15","text":"Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed metadata Object Contains the headers and timestamp of the message","title":"Parameters"},{"location":"reference/functions-reference/#return-value_15","text":"Modified metadata for the output message","title":"Return Value"},{"location":"reference/functions-reference/#example_15","text":"functions: addTime: type: metadataTransformer code: | # Add a custom header to the message metadata[\"headers\"] = metadata[\"headers\"] + [ { \"key\": \"my_own_header_key\", \"value\": \"some_value\" } ] expression: metadata","title":"Example"},{"location":"reference/functions-reference/#valuejoiner","text":"Combines data from multiple streams during join operations.","title":"valueJoiner"},{"location":"reference/functions-reference/#parameters_16","text":"Parameter Type Description value1 Any The value from the first stream value2 Any The value from the second stream","title":"Parameters"},{"location":"reference/functions-reference/#return-value_16","text":"Combined value","title":"Return Value"},{"location":"reference/functions-reference/#example_16","text":"functions: join_order_with_customer: type: valueJoiner code: | order = value1 customer = value2 if customer is None: customer_name = \"Unknown\" customer_email = \"Unknown\" else: customer_name = customer.get(\"name\", \"Unknown\") customer_email = customer.get(\"email\", \"Unknown\") return { \"order_id\": order.get(\"order_id\"), \"customer_id\": order.get(\"customer_id\"), \"customer_name\": customer_name, \"customer_email\": customer_email, \"items\": order.get(\"items\", []), \"total\": order.get(\"total\", 0), \"status\": order.get(\"status\", \"PENDING\") } resultType: struct","title":"Example"},{"location":"reference/functions-reference/#stream-related-functions","text":"","title":"Stream Related Functions"},{"location":"reference/functions-reference/#timestampextractor","text":"Extracts timestamps from messages for time-based operations.","title":"timestampExtractor"},{"location":"reference/functions-reference/#parameters_17","text":"Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed previousTimestamp Long The previous timestamp (can be used as fallback)","title":"Parameters"},{"location":"reference/functions-reference/#return-value_17","text":"Timestamp in milliseconds (long)","title":"Return Value"},{"location":"reference/functions-reference/#example_17","text":"functions: event_timestamp_extractor: type: timestampExtractor code: | # Try to get timestamp from the event if value is not None and \"timestamp\" in value: return value.get(\"timestamp\") # Fall back to record timestamp return previousTimestamp","title":"Example"},{"location":"reference/functions-reference/#topicnameextractor","text":"Derives a target topic name from key and value. Used to dynamically route messages to different topics.","title":"topicNameExtractor"},{"location":"reference/functions-reference/#parameters_18","text":"Parameter Type Description key Any The key of the record being processed value Any The value of the record being processed","title":"Parameters"},{"location":"reference/functions-reference/#return-value_18","text":"String representing the topic name to send the message to","title":"Return Value"},{"location":"reference/functions-reference/#example_18","text":"functions: route_by_sensor: type: topicNameExtractor code: | if key == 'sensor1': return 'sensordata_sensor1' if key == 'sensor2': return 'sensordata_sensor2' return 'sensordata_other_sensors'","title":"Example"},{"location":"reference/functions-reference/#streampartitioner","text":"Determines to which partition a record is produced. Used to control the partitioning of output topics.","title":"streamPartitioner"},{"location":"reference/functions-reference/#parameters_19","text":"Parameter Type Description topic String The topic of the message key Any The key of the record being processed value Any The value of the record being processed numPartitions Integer The number of partitions in the output topic","title":"Parameters"},{"location":"reference/functions-reference/#return-value_19","text":"Integer representing the partition number to which the message will be sent","title":"Return Value"},{"location":"reference/functions-reference/#example_19","text":"functions: custom_partitioner: type: streamPartitioner code: | # Partition by the first character of the key (if it's a string) if key is None: # Use default partitioning for null keys return None if isinstance(key, str) and len(key) > 0: # Use the first character's ASCII value modulo number of partitions return ord(key[0]) % numPartitions # For non-string keys, use a hash of the string representation return hash(str(key)) % numPartitions","title":"Example"},{"location":"reference/functions-reference/#other-functions","text":"","title":"Other Functions"},{"location":"reference/functions-reference/#generic","text":"Generic custom function that can be used for any purpose. It can accept custom parameters and return any type of value.","title":"generic"},{"location":"reference/functions-reference/#parameters_20","text":"User-defined parameters","title":"Parameters"},{"location":"reference/functions-reference/#return-value_20","text":"Any value, depending on the function's purpose","title":"Return Value"},{"location":"reference/functions-reference/#example_20","text":"functions: calculate_discount: type: generic parameters: - name: basePrice type: double - name: discountPercentage type: double code: | # Calculate the discounted price discountAmount = basePrice * (discountPercentage / 100) finalPrice = basePrice - discountAmount # Return both the final price and the discount amount return { \"finalPrice\": finalPrice, \"discountAmount\": discountAmount, \"discountPercentage\": discountPercentage } resultType: struct","title":"Example"},{"location":"reference/functions-reference/#function-definition-formats","text":"KSML supports two formats for defining functions:","title":"Function Definition Formats"},{"location":"reference/functions-reference/#expression-format","text":"For simple, one-line functions: functions: is_valid: type: predicate code: | # Code is optional here expression: value.get(\"status\") == \"ACTIVE\"","title":"Expression Format"},{"location":"reference/functions-reference/#code-block-format","text":"For more complex functions: functions: process_transaction: type: keyValueMapper code: | result = {} # Copy basic fields result[\"transaction_id\"] = value.get(\"id\") result[\"amount\"] = value.get(\"amount\", 0) # Calculate fee amount = value.get(\"amount\", 0) if amount > 1000: result[\"fee\"] = amount * 0.02 else: result[\"fee\"] = amount * 0.03 # Add timestamp result[\"processed_at\"] = int(time.time() * 1000) return result resultType: struct","title":"Code Block Format"},{"location":"reference/functions-reference/#function-execution-context","text":"When your Python functions execute, they have access to:","title":"Function Execution Context"},{"location":"reference/functions-reference/#logger","text":"For outputting information to the application logs: # In your function code, you can use the log object: # Example: # log.debug(\"Debug message\") # log.info(\"Info message\") # log.warn(\"Warning message\") # log.error(\"Error message\")","title":"Logger"},{"location":"reference/functions-reference/#metrics","text":"For monitoring function performance and behavior: # In your function code, you can use the metrics object: # Examples: # Increment a counter # metrics.counter(\"records_processed\").increment() # # Record a value # metrics.gauge(\"record_size\").record(len(str(value)))","title":"Metrics"},{"location":"reference/functions-reference/#state-stores","text":"In your function code, you can use the state stores declared by the function as variables: deduplicate_events: type: predicate code: | previous_value = my_state_store.get(key) if previous_value is not None: # Access a state store to check for duplicates event_id = value.get(\"event_id\") if event_id is None: return True # Check if we've seen this event before seen_before = event_store.get(event_id) if seen_before: # Skip duplicate event return False # Mark this event as seen stateStore.put(event_id, True) # Process the event return True stores: - event_store # Examples: # Get a value from the state store # previous_value = state_store.get(key) # # Put a value in the state store # state_store.put(key, new_value) # # Delete a value from the state store # state_store.delete(key)","title":"State Stores"},{"location":"reference/functions-reference/#best-practices","text":"Keep functions focused : Each function should do one thing well Handle errors gracefully : Use try/except blocks to prevent pipeline failures Consider performance : Python functions introduce some overhead, so keep them efficient Use appropriate function types : Choose the right function type for your use case Leverage state stores : For complex stateful operations, use state stores rather than global variables Document your functions : Add comments to explain complex logic and business rules Test thoroughly : Write unit tests for your functions to ensure they behave as expected","title":"Best Practices"},{"location":"reference/functions-reference/#related-topics","text":"KSML Language Reference Operations Reference Data Types Reference Configuration Reference","title":"Related Topics"},{"location":"reference/language-reference/","text":"KSML Language Reference This document provides a comprehensive reference for the KSML (Kafka Streams Markup Language) syntax and structure. It covers all aspects of the KSML language specification, including file structure, data types, and configuration options. KSML File Structure A KSML definition file is written in YAML and consists of several top-level sections: name: my-ksml-app version: 0.1.2 description: This is what my app does streams: # Stream definitions tables: # Table definitions globalTables: # GlobalTable definitions stores: # State store definitions functions: # Function definitions pipelines: # Pipeline definitions producers: # Producer definitions Required and Optional Sections name : Optional. Defines the name of your application. version : Optional. Defines the version of your application. description : Optional. Defines the purpose of your application. streams : Optional. Defines the input and output Kafka streams. tables : Optional. Defines the input and output Kafka tables. globalTables : Optional. Defines the input and output Kafka globalTables. stores : Optional. Defines the state stores used in your functions and pipelines. functions : Optional. Defines reusable functions that can be called from pipelines. pipelines : Optional. Defines the processing logic of your application. producers : Optional. Defines the producers that generate data for your output topics. Metadata Section The name , version , and description fields hold informational data for the developer of the application. Streams, Tables, and GlobalTables Section The streams , tables , and globalTables sections defines the input and output Kafka topics and their data types. Definition streams: stream_name: topic: kafka_topic_name keyType: key_data_type valueType: value_data_type # Additional configuration options tables: table_name: topic: kafka_topic_name keyType: key_data_type valueType: value_data_type # Additional configuration options globalTables: global_table_name: topic: kafka_topic_name keyType: key_data_type valueType: value_data_type # Additional configuration options Required Properties topic : (Required) The name of the Kafka topic. keyType : (Required) The data type of the message key. valueType : (Required) The data type of the message value. offsetResetPolicy : (Optional) The policy to use when there is no (valid) consumer group offset in Kafka. Choice of earliest , latest , none , or by_duration:<duration> . In the latter case, you can pass in a custom duration. timestampExtractor : (Optional) A function that is able to extract a timestamp from a consumed message, to be used by Kafka Streams as the message timestamp in all pipeline processing. partitioner : (Optional) A stream partitioner that determines to which topic partitions an output record needs to be written. Example streams: orders: topic: incoming_orders keyType: string valueType: json Functions Section The functions section defines reusable functions that can be called from pipelines. Function Definition functions: function_name: type: function_type parameters: - name: parameter_name type: parameter_type code: | # Python code here expression: | # Python expression here resultType: data_type Required Properties type : The type of function (e.g., predicate , aggregator , keyValueMapper ). parameters : List of custom parameters, which may be passed in next to the default parameters determined by the function type. code : Python code that implements the function. expression : Python expression for the return value of the function. resultType : The return value data type of the function. Parameter Definition name : The name of the parameter. type : The data type of the parameter. Example functions: calculate_total: type: mapValues parameters: - name: order type: struct code: | total = 0 for item in order.get(\"items\", []): total += item.get(\"price\", 0) * item.get(\"quantity\", 0) expression: | {\"order_id\": order.get(\"id\"), \"total\": total} resultType: struct Pipelines Section The pipelines section defines the processing logic that connects streams. Pipeline Definition pipelines: pipeline_name: from: input_stream via: - type: operation_type # Operation-specific parameters to: output_stream Required Properties from : The input stream. to : The output stream. Optional Properties via : List of operations to apply to the stream. Example pipelines: order_processing: from: orders via: - type: filter if: expression: value.get(\"total\") > 100 - type: mapValues mapper: code: calculate_total(value) to: processed_orders Operations Operations are the building blocks of pipelines. They transform, filter, or aggregate data. Common Operation Types Stateless Operations mapValues : Transforms the value of each record. map : Transforms both the key and value of each record. filter : Keeps only records that satisfy a condition. flatMap : Transforms each record into zero or more records. peek : Performs a side effect on each record without changing it. Stateful Operations aggregate : Aggregates records by key. count : Counts records by key. reduce : Combines records with the same key. join : Joins two streams based on key. windowedBy : Groups records into time windows. Operation Parameters Each operation type has its own set of parameters. Common parameters include: mapper : For map operations, specifies the transformation. if : For filter operations, specifies the condition. initializer : For aggregate operations, specifies the initial value. aggregator : For aggregate operations, specifies how to combine values. Data Types KSML supports various data types for stream keys and values. Type Conversion KSML automatically handles type conversion between compatible types. For example, a JSON string can be converted to an AVRO message if the structure (schema) matches. Expressions KSML supports two types of expressions: YAML Expressions Simple expressions can be defined directly in YAML: expression: value.get(\"total\") > 100 Python Code Blocks More complex logic can be implemented using Python code blocks: code: | if value.get(\"status\") == \"COMPLETED\" and value.get(\"total\") > 100: return True return False Best Practices Naming Conventions Use descriptive names for streams, functions, and pipelines. Use snake_case for state store, function and pipeline names. Code Organization Keep functions small and focused on a single task. Reuse functions across pipelines when possible. Group related pipelines together. Performance Considerations Use stateless operations when possible. Be mindful of window sizes in windowed operations. Consider partitioning when designing your pipelines. Appendix: Full Example streams: orders: topic: incoming_orders keyType: string valueType: json customers: topic: customer_data keyType: string valueType: json enriched_orders: topic: processed_orders keyType: string valueType: json functions: calculate_total: type: valueTransformer parameters: - name: order type: struct code: | total = 0 for item in order.get(\"items\", []): total += item.get(\"price\", 0) * item.get(\"quantity\", 0) return {**order, \"total\": total} resultType: struct join_with_customers: type: valueJoiner code: | return { **value1, \"customer\": value2 } resultType: struct enrich_order: type: valueTransformer parameters: - name: order type: struct - name: customer type: struct code: | customer = value.get(\"customer\") if customer is None: return order return { **order, \"customer_name\": customer.get(\"name\"), \"customer_tier\": customer.get(\"tier\", \"standard\"), \"loyalty_points\": order.get(\"total\", 0) * 0.1 } resultType: struct pipelines: order_processing: from: orders via: - type: transformValue mapper: calculate_total - type: join with: customers valueJoiner: join_with_customers - type: transformValue mapper: enrich_order - type: filter if: expression: value.get(\"total\") > 0 to: enriched_orders This example demonstrates a complete KSML application that processes orders, calculates totals, enriches them with customer data, and filters out zero-total orders.","title":"KSML Language Reference"},{"location":"reference/language-reference/#ksml-language-reference","text":"This document provides a comprehensive reference for the KSML (Kafka Streams Markup Language) syntax and structure. It covers all aspects of the KSML language specification, including file structure, data types, and configuration options.","title":"KSML Language Reference"},{"location":"reference/language-reference/#ksml-file-structure","text":"A KSML definition file is written in YAML and consists of several top-level sections: name: my-ksml-app version: 0.1.2 description: This is what my app does streams: # Stream definitions tables: # Table definitions globalTables: # GlobalTable definitions stores: # State store definitions functions: # Function definitions pipelines: # Pipeline definitions producers: # Producer definitions","title":"KSML File Structure"},{"location":"reference/language-reference/#required-and-optional-sections","text":"name : Optional. Defines the name of your application. version : Optional. Defines the version of your application. description : Optional. Defines the purpose of your application. streams : Optional. Defines the input and output Kafka streams. tables : Optional. Defines the input and output Kafka tables. globalTables : Optional. Defines the input and output Kafka globalTables. stores : Optional. Defines the state stores used in your functions and pipelines. functions : Optional. Defines reusable functions that can be called from pipelines. pipelines : Optional. Defines the processing logic of your application. producers : Optional. Defines the producers that generate data for your output topics.","title":"Required and Optional Sections"},{"location":"reference/language-reference/#metadata-section","text":"The name , version , and description fields hold informational data for the developer of the application.","title":"Metadata Section"},{"location":"reference/language-reference/#streams-tables-and-globaltables-section","text":"The streams , tables , and globalTables sections defines the input and output Kafka topics and their data types.","title":"Streams, Tables, and GlobalTables Section"},{"location":"reference/language-reference/#definition","text":"streams: stream_name: topic: kafka_topic_name keyType: key_data_type valueType: value_data_type # Additional configuration options tables: table_name: topic: kafka_topic_name keyType: key_data_type valueType: value_data_type # Additional configuration options globalTables: global_table_name: topic: kafka_topic_name keyType: key_data_type valueType: value_data_type # Additional configuration options","title":"Definition"},{"location":"reference/language-reference/#required-properties","text":"topic : (Required) The name of the Kafka topic. keyType : (Required) The data type of the message key. valueType : (Required) The data type of the message value. offsetResetPolicy : (Optional) The policy to use when there is no (valid) consumer group offset in Kafka. Choice of earliest , latest , none , or by_duration:<duration> . In the latter case, you can pass in a custom duration. timestampExtractor : (Optional) A function that is able to extract a timestamp from a consumed message, to be used by Kafka Streams as the message timestamp in all pipeline processing. partitioner : (Optional) A stream partitioner that determines to which topic partitions an output record needs to be written.","title":"Required Properties"},{"location":"reference/language-reference/#example","text":"streams: orders: topic: incoming_orders keyType: string valueType: json","title":"Example"},{"location":"reference/language-reference/#functions-section","text":"The functions section defines reusable functions that can be called from pipelines.","title":"Functions Section"},{"location":"reference/language-reference/#function-definition","text":"functions: function_name: type: function_type parameters: - name: parameter_name type: parameter_type code: | # Python code here expression: | # Python expression here resultType: data_type","title":"Function Definition"},{"location":"reference/language-reference/#required-properties_1","text":"type : The type of function (e.g., predicate , aggregator , keyValueMapper ). parameters : List of custom parameters, which may be passed in next to the default parameters determined by the function type. code : Python code that implements the function. expression : Python expression for the return value of the function. resultType : The return value data type of the function.","title":"Required Properties"},{"location":"reference/language-reference/#parameter-definition","text":"name : The name of the parameter. type : The data type of the parameter.","title":"Parameter Definition"},{"location":"reference/language-reference/#example_1","text":"functions: calculate_total: type: mapValues parameters: - name: order type: struct code: | total = 0 for item in order.get(\"items\", []): total += item.get(\"price\", 0) * item.get(\"quantity\", 0) expression: | {\"order_id\": order.get(\"id\"), \"total\": total} resultType: struct","title":"Example"},{"location":"reference/language-reference/#pipelines-section","text":"The pipelines section defines the processing logic that connects streams.","title":"Pipelines Section"},{"location":"reference/language-reference/#pipeline-definition","text":"pipelines: pipeline_name: from: input_stream via: - type: operation_type # Operation-specific parameters to: output_stream","title":"Pipeline Definition"},{"location":"reference/language-reference/#required-properties_2","text":"from : The input stream. to : The output stream.","title":"Required Properties"},{"location":"reference/language-reference/#optional-properties","text":"via : List of operations to apply to the stream.","title":"Optional Properties"},{"location":"reference/language-reference/#example_2","text":"pipelines: order_processing: from: orders via: - type: filter if: expression: value.get(\"total\") > 100 - type: mapValues mapper: code: calculate_total(value) to: processed_orders","title":"Example"},{"location":"reference/language-reference/#operations","text":"Operations are the building blocks of pipelines. They transform, filter, or aggregate data.","title":"Operations"},{"location":"reference/language-reference/#common-operation-types","text":"","title":"Common Operation Types"},{"location":"reference/language-reference/#stateless-operations","text":"mapValues : Transforms the value of each record. map : Transforms both the key and value of each record. filter : Keeps only records that satisfy a condition. flatMap : Transforms each record into zero or more records. peek : Performs a side effect on each record without changing it.","title":"Stateless Operations"},{"location":"reference/language-reference/#stateful-operations","text":"aggregate : Aggregates records by key. count : Counts records by key. reduce : Combines records with the same key. join : Joins two streams based on key. windowedBy : Groups records into time windows.","title":"Stateful Operations"},{"location":"reference/language-reference/#operation-parameters","text":"Each operation type has its own set of parameters. Common parameters include: mapper : For map operations, specifies the transformation. if : For filter operations, specifies the condition. initializer : For aggregate operations, specifies the initial value. aggregator : For aggregate operations, specifies how to combine values.","title":"Operation Parameters"},{"location":"reference/language-reference/#data-types","text":"KSML supports various data types for stream keys and values.","title":"Data Types"},{"location":"reference/language-reference/#type-conversion","text":"KSML automatically handles type conversion between compatible types. For example, a JSON string can be converted to an AVRO message if the structure (schema) matches.","title":"Type Conversion"},{"location":"reference/language-reference/#expressions","text":"KSML supports two types of expressions:","title":"Expressions"},{"location":"reference/language-reference/#yaml-expressions","text":"Simple expressions can be defined directly in YAML: expression: value.get(\"total\") > 100","title":"YAML Expressions"},{"location":"reference/language-reference/#python-code-blocks","text":"More complex logic can be implemented using Python code blocks: code: | if value.get(\"status\") == \"COMPLETED\" and value.get(\"total\") > 100: return True return False","title":"Python Code Blocks"},{"location":"reference/language-reference/#best-practices","text":"","title":"Best Practices"},{"location":"reference/language-reference/#naming-conventions","text":"Use descriptive names for streams, functions, and pipelines. Use snake_case for state store, function and pipeline names.","title":"Naming Conventions"},{"location":"reference/language-reference/#code-organization","text":"Keep functions small and focused on a single task. Reuse functions across pipelines when possible. Group related pipelines together.","title":"Code Organization"},{"location":"reference/language-reference/#performance-considerations","text":"Use stateless operations when possible. Be mindful of window sizes in windowed operations. Consider partitioning when designing your pipelines.","title":"Performance Considerations"},{"location":"reference/language-reference/#appendix-full-example","text":"streams: orders: topic: incoming_orders keyType: string valueType: json customers: topic: customer_data keyType: string valueType: json enriched_orders: topic: processed_orders keyType: string valueType: json functions: calculate_total: type: valueTransformer parameters: - name: order type: struct code: | total = 0 for item in order.get(\"items\", []): total += item.get(\"price\", 0) * item.get(\"quantity\", 0) return {**order, \"total\": total} resultType: struct join_with_customers: type: valueJoiner code: | return { **value1, \"customer\": value2 } resultType: struct enrich_order: type: valueTransformer parameters: - name: order type: struct - name: customer type: struct code: | customer = value.get(\"customer\") if customer is None: return order return { **order, \"customer_name\": customer.get(\"name\"), \"customer_tier\": customer.get(\"tier\", \"standard\"), \"loyalty_points\": order.get(\"total\", 0) * 0.1 } resultType: struct pipelines: order_processing: from: orders via: - type: transformValue mapper: calculate_total - type: join with: customers valueJoiner: join_with_customers - type: transformValue mapper: enrich_order - type: filter if: expression: value.get(\"total\") > 0 to: enriched_orders This example demonstrates a complete KSML application that processes orders, calculates totals, enriches them with customer data, and filters out zero-total orders.","title":"Appendix: Full Example"},{"location":"reference/notations-reference/","text":"KSML Notations This document provides a comprehensive reference for all notations available in KSML. Each notation may handle schema and/or interaction with a schema registry differently. Introduction to notations KSML uses notations to allow reading/writing different message formats to Kafka topics. However, some notations may require different backends ( serializers and deserializers , or serdes for short). How to configure notations To accommodate for variations in Kafka setups and/or ecosystem variations, the following table lists the available options you can configure. The notation name lists the notation you use in your KSML definitions, for example avro:SensorData . The configuration of all notations is done in the ksml-runner.yaml file, from which KSML reads its technical configuration parameters. List of available supported variations The table below provides a complete list of all notations, their possible implementation alternatives and corresponding characteristics, such as what data types a notation maps to. Notation name KSML defined serde name Serde supplier With(out) schema Requires SR Schema source KSML data type Loaded in KSML Remarks avro apicurio_avro Apicurio Without Y SR struct If configured Any AVRO, schema loaded from SR upon consume avro confluent_avro Confluent Without Y SR struct Always Any AVRO, schema loaded from SR upon consume avro apicurio_avro Apicurio With Y File or dynamic struct If configured Local file or Python-code-generated schema avro confluent_avro Confluent With Y File or dynamic struct Always Local file or Python-code-generated schema csv csv ksml Without N N/A struct Always CSV Schemaless csv csv ksml With N File or dynamic struct Always Local file or Python-code-generated schema json json ksml Without N N/A list or struct Always JSON Schemaless json json ksml With N File or dynamic list or struct Always Local file or Python-code-generated schema jsonschema apicurio_jsonschema Apicurio Without Y SR struct If configured Any JSON Schema, schema loaded from SR upon consume jsonschema confluent_jsonschema Confluent Without Y SR struct If configured Any JSON Schema, schema loaded from SR upon consume jsonschema apicurio_jsonschema Apicurio With Y File or dynamic struct If configured Local file or Python-code-generated schema jsonschema confluent_jsonschema Confluent With Y File or dynamic struct If configured Local file or Python-code-generated schema protobuf apicurio_protobuf Apicurio Without Y SR struct If configured Any Protobuf, schema loaded from SR upon consume protobuf confluent_protobuf Confluent Without Y SR struct If configured Any Protobuf, schema loaded from SR upon consume protobuf apicurio_protobuf Apicurio With Y File or dynamic struct If configured Local file or Python-code-generated schema protobuf confluent_protobuf Confluent With Y File or dynamic struct If configured Local file or Python-code-generated schema soap soap ksml Without N N/A struct Always SOAP Schemaless soap soap ksml With N File or dynamic struct Always SOAP with Schema xml xml ksml Without N N/A struct Always XML Schemaless xml xml ksml With N File or dynamic struct Always XML with Schema Using Notations Notations are specified as a prefix to the schema name: # Example of AVRO notation with a specific schema avroStream: valueType: \"avro:SensorReading\" # Example of JSON notation (schemaless) jsonStream: valueType: \"json\" # Example of XML notation with a schema xmlStream: valueType: \"xml:CustomerData\" Choosing the Right Notation The choice of notation depends on your specific requirements: If you need... Consider using... Schema evolution and backward compatibility AVRO or Protobuf Human-readable data for debugging JSON Integration with legacy systems XML or SOAP Simple tabular data CSV Compact binary format AVRO or Protobuf Using notations in your KSML definitions AVRO AVRO is a binary format that supports schema evolution. Syntax valueType: \"avro:<schema_name>\" Where <schema_name> is the name of an AVRO schema. Example streams: sensor_readings: topic: sensor-data keyType: string valueType: avro:SensorData CSV CSV (Comma-Separated Values) is a simple tabular data format. Syntax # For schemaless CSV: valueType: \"csv\" # For CSV with a schema: resultType: \"csv:<schema_name>\" Example streams: sales_data: topic: sales-data keyType: string valueType: \"csv\" inventory_data: topic: inventory-data keyType: string valueType: \"csv:InventoryRecord\" JSON JSON is a text-based, human-readable format for data transfer. It can be used with and without a schema. Syntax # For schemaless JSON: valueType: \"json\" # For JSON with a schema: resultType: \"json:<schema_name>\" Example streams: user_profiles: topic: user-profiles keyType: string valueType: \"json\" orders: topic: orders keyType: string valueType: \"json:Order\" Python functions can also result in a JSON return value by returning a dictionary: functions: merge_key_value_data: type: valueTransformer expression: | { 'key': key, 'value': value } resultType: json convert_order_info: type: valueTransformer expression: | { 'order_id': key, 'customer_id': value.get('customer_id'), 'order_value': value.get('amount') } resultType: json:OrderSchema JSON Schema JSON Schema adds vendor-specific schema support to the above JSON serialization format. This notation needs a schema for its operations, which it may get from schema registry (upon consuming a message) or through explicit schema naming. Syntax # For any JSON topic, where the concrete schema is fetched from SR valueType: \"jsonschema\" # For JSON with a schema: resultType: \"jsonschema:<schema_name>\" Example streams: user_profiles: topic: user-profiles keyType: string valueType: \"jsonschema\" orders: topic: orders keyType: string valueType: \"jsonschema:Order\" Protobuf Protobuf is a popular encoding format (developed by Google) to allow for easy data exchange between applications of any kind and written in any language. It always needs a schema for its operations, which it may get from a schema registry (upon consuming a message) or through explicit schema naming. Syntax # For any Protobuf topic, where the concrete schema is fetched from SR valueType: \"protobuf\" # For Protobuf with a schema: resultType: \"protobuf:<schema_name>\" Example streams: user_profiles: topic: user-profiles keyType: string valueType: \"protobuf\" orders: topic: orders keyType: string valueType: \"protobuf:Order\" SOAP SOAP (Simple Object Access Protocol) is an XML-based messaging protocol. Syntax valueType: \"soap\" Example streams: service_requests: topic: service-requests keyType: string valueType: \"soap\" XML XML (Extensible Markup Language) is used for complex hierarchical data. Syntax # For schemaless XML: valueType: \"xml\" # For XML with a schema: resultType: \"xml:<schema_name>\" Example streams: customer_data: topic: customer-data keyType: string valueType: \"xml:CustomerData\" Schema Management When working with structured data, it's important to manage your schemas effectively. Local files vs. Schema Registry When a schema is specified, KSML will load the schema from a local file from the schemaDirectory . The notation determines the type of schema and its filename extension. For instance, AVRO schemas will always have the .avsc file extension, while XML schemas have the .xsd extension. KSML always loads schemas from local files: streams: sensor_data: topic: sensor-reading keyType: string valueType: \"avro:SensorReading\" In this example, the SensorReading.avsc file is looked up in the configured schemaDirectory . Whenever a notation is used without specifying the concrete schema, KSML assumes the schema is loadable from Schema Registry. streams: sensor_data: topic: sensor-reading keyType: string valueType: \"avro\" In this example there is no schema specified, so KSML will use the schema registered in Schema Registry for the topic value. Type Conversion KSML automatically performs type conversion wherever required and possible. This holds for numbers (integer to long, etc.) but also for string representation of certain notations. For instance between strings and CSV, JSON and XML. The following example shows how a string value gets converted into a struct with specific fields by declaring it as CSV with a specific schema. functions: generate_message: type: generator globalCode: | import random sensorCounter = 0 code: | global sensorCounter # Generate key key = \"sensor\"+str(sensorCounter) # Set the key to return (\"sensor0\" to \"sensor9\") sensorCounter = (sensorCounter+1) % 10 # Increase the counter for next iteration # Generate temperature data as CSV, Temperature.csv file contains: \"type,unit,value\" value = \"TEMPERATURE,C,\"+str(random.randrange(-100, 100) expression: (key, value) # Return a message tuple with the key and value resultType: (string, csv:Temperature) # Value is converted to {\"type\":\"TEMPERATURE\", \"unit\":\"C\", \"value\":\"83\"}","title":"KSML Notations"},{"location":"reference/notations-reference/#ksml-notations","text":"This document provides a comprehensive reference for all notations available in KSML. Each notation may handle schema and/or interaction with a schema registry differently.","title":"KSML Notations"},{"location":"reference/notations-reference/#introduction-to-notations","text":"KSML uses notations to allow reading/writing different message formats to Kafka topics. However, some notations may require different backends ( serializers and deserializers , or serdes for short).","title":"Introduction to notations"},{"location":"reference/notations-reference/#how-to-configure-notations","text":"To accommodate for variations in Kafka setups and/or ecosystem variations, the following table lists the available options you can configure. The notation name lists the notation you use in your KSML definitions, for example avro:SensorData . The configuration of all notations is done in the ksml-runner.yaml file, from which KSML reads its technical configuration parameters.","title":"How to configure notations"},{"location":"reference/notations-reference/#list-of-available-supported-variations","text":"The table below provides a complete list of all notations, their possible implementation alternatives and corresponding characteristics, such as what data types a notation maps to. Notation name KSML defined serde name Serde supplier With(out) schema Requires SR Schema source KSML data type Loaded in KSML Remarks avro apicurio_avro Apicurio Without Y SR struct If configured Any AVRO, schema loaded from SR upon consume avro confluent_avro Confluent Without Y SR struct Always Any AVRO, schema loaded from SR upon consume avro apicurio_avro Apicurio With Y File or dynamic struct If configured Local file or Python-code-generated schema avro confluent_avro Confluent With Y File or dynamic struct Always Local file or Python-code-generated schema csv csv ksml Without N N/A struct Always CSV Schemaless csv csv ksml With N File or dynamic struct Always Local file or Python-code-generated schema json json ksml Without N N/A list or struct Always JSON Schemaless json json ksml With N File or dynamic list or struct Always Local file or Python-code-generated schema jsonschema apicurio_jsonschema Apicurio Without Y SR struct If configured Any JSON Schema, schema loaded from SR upon consume jsonschema confluent_jsonschema Confluent Without Y SR struct If configured Any JSON Schema, schema loaded from SR upon consume jsonschema apicurio_jsonschema Apicurio With Y File or dynamic struct If configured Local file or Python-code-generated schema jsonschema confluent_jsonschema Confluent With Y File or dynamic struct If configured Local file or Python-code-generated schema protobuf apicurio_protobuf Apicurio Without Y SR struct If configured Any Protobuf, schema loaded from SR upon consume protobuf confluent_protobuf Confluent Without Y SR struct If configured Any Protobuf, schema loaded from SR upon consume protobuf apicurio_protobuf Apicurio With Y File or dynamic struct If configured Local file or Python-code-generated schema protobuf confluent_protobuf Confluent With Y File or dynamic struct If configured Local file or Python-code-generated schema soap soap ksml Without N N/A struct Always SOAP Schemaless soap soap ksml With N File or dynamic struct Always SOAP with Schema xml xml ksml Without N N/A struct Always XML Schemaless xml xml ksml With N File or dynamic struct Always XML with Schema","title":"List of available supported variations"},{"location":"reference/notations-reference/#using-notations","text":"Notations are specified as a prefix to the schema name: # Example of AVRO notation with a specific schema avroStream: valueType: \"avro:SensorReading\" # Example of JSON notation (schemaless) jsonStream: valueType: \"json\" # Example of XML notation with a schema xmlStream: valueType: \"xml:CustomerData\"","title":"Using Notations"},{"location":"reference/notations-reference/#choosing-the-right-notation","text":"The choice of notation depends on your specific requirements: If you need... Consider using... Schema evolution and backward compatibility AVRO or Protobuf Human-readable data for debugging JSON Integration with legacy systems XML or SOAP Simple tabular data CSV Compact binary format AVRO or Protobuf","title":"Choosing the Right Notation"},{"location":"reference/notations-reference/#using-notations-in-your-ksml-definitions","text":"","title":"Using notations in your KSML definitions"},{"location":"reference/notations-reference/#avro","text":"AVRO is a binary format that supports schema evolution.","title":"AVRO"},{"location":"reference/notations-reference/#syntax","text":"valueType: \"avro:<schema_name>\" Where <schema_name> is the name of an AVRO schema.","title":"Syntax"},{"location":"reference/notations-reference/#example","text":"streams: sensor_readings: topic: sensor-data keyType: string valueType: avro:SensorData","title":"Example"},{"location":"reference/notations-reference/#csv","text":"CSV (Comma-Separated Values) is a simple tabular data format.","title":"CSV"},{"location":"reference/notations-reference/#syntax_1","text":"# For schemaless CSV: valueType: \"csv\" # For CSV with a schema: resultType: \"csv:<schema_name>\"","title":"Syntax"},{"location":"reference/notations-reference/#example_1","text":"streams: sales_data: topic: sales-data keyType: string valueType: \"csv\" inventory_data: topic: inventory-data keyType: string valueType: \"csv:InventoryRecord\"","title":"Example"},{"location":"reference/notations-reference/#json","text":"JSON is a text-based, human-readable format for data transfer. It can be used with and without a schema.","title":"JSON"},{"location":"reference/notations-reference/#syntax_2","text":"# For schemaless JSON: valueType: \"json\" # For JSON with a schema: resultType: \"json:<schema_name>\"","title":"Syntax"},{"location":"reference/notations-reference/#example_2","text":"streams: user_profiles: topic: user-profiles keyType: string valueType: \"json\" orders: topic: orders keyType: string valueType: \"json:Order\" Python functions can also result in a JSON return value by returning a dictionary: functions: merge_key_value_data: type: valueTransformer expression: | { 'key': key, 'value': value } resultType: json convert_order_info: type: valueTransformer expression: | { 'order_id': key, 'customer_id': value.get('customer_id'), 'order_value': value.get('amount') } resultType: json:OrderSchema","title":"Example"},{"location":"reference/notations-reference/#json-schema","text":"JSON Schema adds vendor-specific schema support to the above JSON serialization format. This notation needs a schema for its operations, which it may get from schema registry (upon consuming a message) or through explicit schema naming.","title":"JSON Schema"},{"location":"reference/notations-reference/#syntax_3","text":"# For any JSON topic, where the concrete schema is fetched from SR valueType: \"jsonschema\" # For JSON with a schema: resultType: \"jsonschema:<schema_name>\"","title":"Syntax"},{"location":"reference/notations-reference/#example_3","text":"streams: user_profiles: topic: user-profiles keyType: string valueType: \"jsonschema\" orders: topic: orders keyType: string valueType: \"jsonschema:Order\"","title":"Example"},{"location":"reference/notations-reference/#protobuf","text":"Protobuf is a popular encoding format (developed by Google) to allow for easy data exchange between applications of any kind and written in any language. It always needs a schema for its operations, which it may get from a schema registry (upon consuming a message) or through explicit schema naming.","title":"Protobuf"},{"location":"reference/notations-reference/#syntax_4","text":"# For any Protobuf topic, where the concrete schema is fetched from SR valueType: \"protobuf\" # For Protobuf with a schema: resultType: \"protobuf:<schema_name>\"","title":"Syntax"},{"location":"reference/notations-reference/#example_4","text":"streams: user_profiles: topic: user-profiles keyType: string valueType: \"protobuf\" orders: topic: orders keyType: string valueType: \"protobuf:Order\"","title":"Example"},{"location":"reference/notations-reference/#soap","text":"SOAP (Simple Object Access Protocol) is an XML-based messaging protocol.","title":"SOAP"},{"location":"reference/notations-reference/#syntax_5","text":"valueType: \"soap\"","title":"Syntax"},{"location":"reference/notations-reference/#example_5","text":"streams: service_requests: topic: service-requests keyType: string valueType: \"soap\"","title":"Example"},{"location":"reference/notations-reference/#xml","text":"XML (Extensible Markup Language) is used for complex hierarchical data.","title":"XML"},{"location":"reference/notations-reference/#syntax_6","text":"# For schemaless XML: valueType: \"xml\" # For XML with a schema: resultType: \"xml:<schema_name>\"","title":"Syntax"},{"location":"reference/notations-reference/#example_6","text":"streams: customer_data: topic: customer-data keyType: string valueType: \"xml:CustomerData\"","title":"Example"},{"location":"reference/notations-reference/#schema-management","text":"When working with structured data, it's important to manage your schemas effectively.","title":"Schema Management"},{"location":"reference/notations-reference/#local-files-vs-schema-registry","text":"When a schema is specified, KSML will load the schema from a local file from the schemaDirectory . The notation determines the type of schema and its filename extension. For instance, AVRO schemas will always have the .avsc file extension, while XML schemas have the .xsd extension. KSML always loads schemas from local files: streams: sensor_data: topic: sensor-reading keyType: string valueType: \"avro:SensorReading\" In this example, the SensorReading.avsc file is looked up in the configured schemaDirectory . Whenever a notation is used without specifying the concrete schema, KSML assumes the schema is loadable from Schema Registry. streams: sensor_data: topic: sensor-reading keyType: string valueType: \"avro\" In this example there is no schema specified, so KSML will use the schema registered in Schema Registry for the topic value.","title":"Local files vs. Schema Registry"},{"location":"reference/notations-reference/#type-conversion","text":"KSML automatically performs type conversion wherever required and possible. This holds for numbers (integer to long, etc.) but also for string representation of certain notations. For instance between strings and CSV, JSON and XML. The following example shows how a string value gets converted into a struct with specific fields by declaring it as CSV with a specific schema. functions: generate_message: type: generator globalCode: | import random sensorCounter = 0 code: | global sensorCounter # Generate key key = \"sensor\"+str(sensorCounter) # Set the key to return (\"sensor0\" to \"sensor9\") sensorCounter = (sensorCounter+1) % 10 # Increase the counter for next iteration # Generate temperature data as CSV, Temperature.csv file contains: \"type,unit,value\" value = \"TEMPERATURE,C,\"+str(random.randrange(-100, 100) expression: (key, value) # Return a message tuple with the key and value resultType: (string, csv:Temperature) # Value is converted to {\"type\":\"TEMPERATURE\", \"unit\":\"C\", \"value\":\"83\"}","title":"Type Conversion"},{"location":"reference/operations-reference/","text":"KSML Operations Reference This document provides a comprehensive reference for all operations available in KSML. Each operation is described with its parameters, behavior, and examples. Stateless Operations Stateless operations process each record independently, without maintaining any state between records. mapValues Transforms the value of each record without changing the key. Parameters Parameter Type Required Description mapper Object Yes Specifies how to transform the value The mapper can be defined using: - expression : A simple expression - code : A Python code block Example - type: mapValues mapper: expression: {\"name\": value.get(\"firstName\") + \" \" + value.get(\"lastName\"), \"age\": value.get(\"age\")} - type: mapValues mapper: code: | return { \"full_name\": value.get(\"firstName\") + \" \" + value.get(\"lastName\"), \"age_in_months\": value.get(\"age\") * 12 } map Transforms both the key and value of each record. Parameters Parameter Type Required Description mapper Object Yes Specifies how to transform the key and value The mapper can be defined using: - expression : A simple expression returning a tuple (key, value) - code : A Python code block returning a tuple (key, value) Example - type: map mapper: code: | new_key = value.get(\"id\") new_value = { \"name\": value.get(\"firstName\") + \" \" + value.get(\"lastName\"), \"age\": value.get(\"age\") } return (new_key, new_value) filter Keeps only records that satisfy a condition. Parameters Parameter Type Required Description if Object Yes Specifies the condition The if can be defined using: - expression : A simple boolean expression - code : A Python code block returning a boolean Example - type: filter if: expression: value.get(\"age\") >= 18 - type: filter if: code: | if value.get(\"status\") == \"ACTIVE\" and value.get(\"age\") >= 18: return True return False flatMap Transforms each record into zero or more records. Parameters Parameter Type Required Description mapper Object Yes Specifies how to transform each record into multiple records The mapper can be defined using: - expression : A simple expression returning a list of tuples (key, value) - code : A Python code block returning a list of tuples (key, value) Example - type: flatMap mapper: code: | result = [] for item in value.get(\"items\", []): result.append((item.get(\"id\"), item)) return result peek Performs a side effect on each record without changing it. Parameters Parameter Type Required Description forEach Object Yes Specifies the action to perform on each record The forEach can be defined using: - expression : A simple expression (rarely used for peek) - code : A Python code block performing the side effect Example - type: peek forEach: code: | log.info(\"Processing record with key={}, value={}\", key, value) selectKey Changes the key of each record without modifying the value. Parameters Parameter Type Required Description keySelector Object Yes Specifies how to select the new key The keySelector can be defined using: - expression : A simple expression returning the new key - code : A Python code block returning the new key Example - type: selectKey keySelector: expression: value.get(\"userId\") Stateful Operations Stateful operations maintain state between records, typically based on the record key. groupByKey Groups records by key for subsequent aggregation operations. Parameters None. This operation is typically followed by an aggregation operation. Example - type: groupByKey - type: count count Counts the number of records for each key. Parameters None. Example - type: groupByKey - type: count aggregate Aggregates records by key using a custom aggregation function. Parameters Parameter Type Required Description initializer Object Yes Specifies the initial value for the aggregation aggregator Object Yes Specifies how to combine the current record with the aggregate Both initializer and aggregator can be defined using: - expression : A simple expression - code : A Python code block Example - type: aggregate initializer: expression: {\"count\": 0, \"sum\": 0} aggregator: code: | if aggregate is None: return {\"count\": 1, \"sum\": value.get(\"amount\", 0)} else: return { \"count\": aggregate.get(\"count\", 0) + 1, \"sum\": aggregate.get(\"sum\", 0) + value.get(\"amount\", 0) } reduce Combines records with the same key using a reducer function. Parameters Parameter Type Required Description reducer Object Yes Specifies how to combine two values The reducer can be defined using: - expression : A simple expression - code : A Python code block Example - type: reduce reducer: code: | return { \"count\": value1.get(\"count\", 0) + value2.get(\"count\", 0), \"sum\": value1.get(\"sum\", 0) + value2.get(\"sum\", 0) } Join Operations Join operations combine data from multiple streams based on keys. join Performs an inner join between two streams. Parameters Parameter Type Required Description with String Yes The name of the stream to join with windowSize Long No The size of the join window in milliseconds (for stream-stream joins) Example - type: join with: customers leftJoin Performs a left join between two streams. Parameters Parameter Type Required Description with String Yes The name of the stream to join with windowSize Long No The size of the join window in milliseconds (for stream-stream joins) Example - type: leftJoin with: customers outerJoin Performs an outer join between two streams. Parameters Parameter Type Required Description with String Yes The name of the stream to join with windowSize Long No The size of the join window in milliseconds (for stream-stream joins) Example - type: outerJoin with: customers windowSize: 60000 # 1 minute Windowing Operations Windowing operations group records into time-based windows. windowedBy Groups records into time windows. Parameters Parameter Type Required Description windowType String No The type of window (tumbling, hopping, sliding, session) timeDifference Long Yes The size of the window in milliseconds advanceBy Long No For hopping windows, how often to advance the window grace Long No Grace period for late-arriving data Example - type: windowedBy windowType: tumbling timeDifference: 60000 # 1 minute window - type: windowedBy windowType: hopping timeDifference: 300000 # 5 minute window advanceBy: 60000 # Advance every 1 minute Branch Operations Branch operations split a stream into multiple substreams. branch Splits a stream into multiple substreams based on conditions. Parameters Parameter Type Required Description predicates Array Yes List of conditions for each branch Each predicate can be defined using: - expression : A simple boolean expression - code : A Python code block returning a boolean Example - type: branch predicates: - expression: value.get(\"amount\") > 1000 - expression: value.get(\"amount\") > 100 - expression: true # Default branch Error Handling Operations Error handling operations provide mechanisms to handle errors during processing. try Attempts to execute operations and catches any exceptions. Parameters Parameter Type Required Description operations Array Yes Operations to try catch Array Yes Operations to execute if an exception occurs Example - type: try operations: - type: mapValues mapper: code: parse_complex_json(value) catch: - type: mapValues mapper: code: | log.error(\"Failed to parse JSON: {}\", exception) return {\"error\": \"Failed to parse\", \"original\": value} onError Specifies what to do when an error occurs in an operation. Parameters Parameter Type Required Description sendTo String Yes The stream to send error records to withKey Object No How to transform the key for the error record withValue Object No How to transform the value for the error record Example - type: mapValues mapper: code: process_data(value) onError: sendTo: error_stream withKey: \"error-\" + key withValue: {\"original\": value, \"error\": exception.getMessage()} Custom Operations KSML allows you to define custom operations using the custom operation type. custom Executes a custom operation defined in Java code. Parameters Parameter Type Required Description className String Yes The fully qualified class name of the custom operation config Object No Configuration parameters for the custom operation Example - type: custom className: com.example.ksml.operations.MyCustomOperation config: param1: value1 param2: value2 Combining Operations Operations can be combined in various ways to create complex processing pipelines. Sequential Operations Operations are executed in sequence, with each operation processing the output of the previous operation. pipelines: my_pipeline: from: input_stream via: - type: filter if: expression: value.get(\"amount\") > 0 - type: mapValues mapper: code: enrich_transaction(value) - type: peek forEach: code: | log.info(\"Processed transaction: {}\", value) to: output_stream Branching and Merging You can create complex topologies by branching streams and merging them back together. pipelines: branch_pipeline: from: input_stream via: - type: branch predicates: - expression: value.get(\"type\") == \"A\" - expression: value.get(\"type\") == \"B\" to: - type_a_stream - type_b_stream process_a_pipeline: from: type_a_stream via: - type: mapValues mapper: code: process_type_a(value) to: processed_a_stream process_b_pipeline: from: type_b_stream via: - type: mapValues mapper: code: process_type_b(value) to: processed_b_stream merge_pipeline: from: - processed_a_stream - processed_b_stream to: merged_stream Best Practices Chain operations thoughtfully : Consider the performance implications of chaining multiple operations. Use stateless operations when possible : Stateless operations are generally more efficient than stateful ones. Be careful with window sizes : Large windows can consume significant memory. Handle errors gracefully : Use error handling operations to prevent pipeline failures. Monitor performance : Keep an eye on throughput and latency, especially for stateful operations.","title":"KSML Operations Reference"},{"location":"reference/operations-reference/#ksml-operations-reference","text":"This document provides a comprehensive reference for all operations available in KSML. Each operation is described with its parameters, behavior, and examples.","title":"KSML Operations Reference"},{"location":"reference/operations-reference/#stateless-operations","text":"Stateless operations process each record independently, without maintaining any state between records.","title":"Stateless Operations"},{"location":"reference/operations-reference/#mapvalues","text":"Transforms the value of each record without changing the key.","title":"mapValues"},{"location":"reference/operations-reference/#parameters","text":"Parameter Type Required Description mapper Object Yes Specifies how to transform the value The mapper can be defined using: - expression : A simple expression - code : A Python code block","title":"Parameters"},{"location":"reference/operations-reference/#example","text":"- type: mapValues mapper: expression: {\"name\": value.get(\"firstName\") + \" \" + value.get(\"lastName\"), \"age\": value.get(\"age\")} - type: mapValues mapper: code: | return { \"full_name\": value.get(\"firstName\") + \" \" + value.get(\"lastName\"), \"age_in_months\": value.get(\"age\") * 12 }","title":"Example"},{"location":"reference/operations-reference/#map","text":"Transforms both the key and value of each record.","title":"map"},{"location":"reference/operations-reference/#parameters_1","text":"Parameter Type Required Description mapper Object Yes Specifies how to transform the key and value The mapper can be defined using: - expression : A simple expression returning a tuple (key, value) - code : A Python code block returning a tuple (key, value)","title":"Parameters"},{"location":"reference/operations-reference/#example_1","text":"- type: map mapper: code: | new_key = value.get(\"id\") new_value = { \"name\": value.get(\"firstName\") + \" \" + value.get(\"lastName\"), \"age\": value.get(\"age\") } return (new_key, new_value)","title":"Example"},{"location":"reference/operations-reference/#filter","text":"Keeps only records that satisfy a condition.","title":"filter"},{"location":"reference/operations-reference/#parameters_2","text":"Parameter Type Required Description if Object Yes Specifies the condition The if can be defined using: - expression : A simple boolean expression - code : A Python code block returning a boolean","title":"Parameters"},{"location":"reference/operations-reference/#example_2","text":"- type: filter if: expression: value.get(\"age\") >= 18 - type: filter if: code: | if value.get(\"status\") == \"ACTIVE\" and value.get(\"age\") >= 18: return True return False","title":"Example"},{"location":"reference/operations-reference/#flatmap","text":"Transforms each record into zero or more records.","title":"flatMap"},{"location":"reference/operations-reference/#parameters_3","text":"Parameter Type Required Description mapper Object Yes Specifies how to transform each record into multiple records The mapper can be defined using: - expression : A simple expression returning a list of tuples (key, value) - code : A Python code block returning a list of tuples (key, value)","title":"Parameters"},{"location":"reference/operations-reference/#example_3","text":"- type: flatMap mapper: code: | result = [] for item in value.get(\"items\", []): result.append((item.get(\"id\"), item)) return result","title":"Example"},{"location":"reference/operations-reference/#peek","text":"Performs a side effect on each record without changing it.","title":"peek"},{"location":"reference/operations-reference/#parameters_4","text":"Parameter Type Required Description forEach Object Yes Specifies the action to perform on each record The forEach can be defined using: - expression : A simple expression (rarely used for peek) - code : A Python code block performing the side effect","title":"Parameters"},{"location":"reference/operations-reference/#example_4","text":"- type: peek forEach: code: | log.info(\"Processing record with key={}, value={}\", key, value)","title":"Example"},{"location":"reference/operations-reference/#selectkey","text":"Changes the key of each record without modifying the value.","title":"selectKey"},{"location":"reference/operations-reference/#parameters_5","text":"Parameter Type Required Description keySelector Object Yes Specifies how to select the new key The keySelector can be defined using: - expression : A simple expression returning the new key - code : A Python code block returning the new key","title":"Parameters"},{"location":"reference/operations-reference/#example_5","text":"- type: selectKey keySelector: expression: value.get(\"userId\")","title":"Example"},{"location":"reference/operations-reference/#stateful-operations","text":"Stateful operations maintain state between records, typically based on the record key.","title":"Stateful Operations"},{"location":"reference/operations-reference/#groupbykey","text":"Groups records by key for subsequent aggregation operations.","title":"groupByKey"},{"location":"reference/operations-reference/#parameters_6","text":"None. This operation is typically followed by an aggregation operation.","title":"Parameters"},{"location":"reference/operations-reference/#example_6","text":"- type: groupByKey - type: count","title":"Example"},{"location":"reference/operations-reference/#count","text":"Counts the number of records for each key.","title":"count"},{"location":"reference/operations-reference/#parameters_7","text":"None.","title":"Parameters"},{"location":"reference/operations-reference/#example_7","text":"- type: groupByKey - type: count","title":"Example"},{"location":"reference/operations-reference/#aggregate","text":"Aggregates records by key using a custom aggregation function.","title":"aggregate"},{"location":"reference/operations-reference/#parameters_8","text":"Parameter Type Required Description initializer Object Yes Specifies the initial value for the aggregation aggregator Object Yes Specifies how to combine the current record with the aggregate Both initializer and aggregator can be defined using: - expression : A simple expression - code : A Python code block","title":"Parameters"},{"location":"reference/operations-reference/#example_8","text":"- type: aggregate initializer: expression: {\"count\": 0, \"sum\": 0} aggregator: code: | if aggregate is None: return {\"count\": 1, \"sum\": value.get(\"amount\", 0)} else: return { \"count\": aggregate.get(\"count\", 0) + 1, \"sum\": aggregate.get(\"sum\", 0) + value.get(\"amount\", 0) }","title":"Example"},{"location":"reference/operations-reference/#reduce","text":"Combines records with the same key using a reducer function.","title":"reduce"},{"location":"reference/operations-reference/#parameters_9","text":"Parameter Type Required Description reducer Object Yes Specifies how to combine two values The reducer can be defined using: - expression : A simple expression - code : A Python code block","title":"Parameters"},{"location":"reference/operations-reference/#example_9","text":"- type: reduce reducer: code: | return { \"count\": value1.get(\"count\", 0) + value2.get(\"count\", 0), \"sum\": value1.get(\"sum\", 0) + value2.get(\"sum\", 0) }","title":"Example"},{"location":"reference/operations-reference/#join-operations","text":"Join operations combine data from multiple streams based on keys.","title":"Join Operations"},{"location":"reference/operations-reference/#join","text":"Performs an inner join between two streams.","title":"join"},{"location":"reference/operations-reference/#parameters_10","text":"Parameter Type Required Description with String Yes The name of the stream to join with windowSize Long No The size of the join window in milliseconds (for stream-stream joins)","title":"Parameters"},{"location":"reference/operations-reference/#example_10","text":"- type: join with: customers","title":"Example"},{"location":"reference/operations-reference/#leftjoin","text":"Performs a left join between two streams.","title":"leftJoin"},{"location":"reference/operations-reference/#parameters_11","text":"Parameter Type Required Description with String Yes The name of the stream to join with windowSize Long No The size of the join window in milliseconds (for stream-stream joins)","title":"Parameters"},{"location":"reference/operations-reference/#example_11","text":"- type: leftJoin with: customers","title":"Example"},{"location":"reference/operations-reference/#outerjoin","text":"Performs an outer join between two streams.","title":"outerJoin"},{"location":"reference/operations-reference/#parameters_12","text":"Parameter Type Required Description with String Yes The name of the stream to join with windowSize Long No The size of the join window in milliseconds (for stream-stream joins)","title":"Parameters"},{"location":"reference/operations-reference/#example_12","text":"- type: outerJoin with: customers windowSize: 60000 # 1 minute","title":"Example"},{"location":"reference/operations-reference/#windowing-operations","text":"Windowing operations group records into time-based windows.","title":"Windowing Operations"},{"location":"reference/operations-reference/#windowedby","text":"Groups records into time windows.","title":"windowedBy"},{"location":"reference/operations-reference/#parameters_13","text":"Parameter Type Required Description windowType String No The type of window (tumbling, hopping, sliding, session) timeDifference Long Yes The size of the window in milliseconds advanceBy Long No For hopping windows, how often to advance the window grace Long No Grace period for late-arriving data","title":"Parameters"},{"location":"reference/operations-reference/#example_13","text":"- type: windowedBy windowType: tumbling timeDifference: 60000 # 1 minute window - type: windowedBy windowType: hopping timeDifference: 300000 # 5 minute window advanceBy: 60000 # Advance every 1 minute","title":"Example"},{"location":"reference/operations-reference/#branch-operations","text":"Branch operations split a stream into multiple substreams.","title":"Branch Operations"},{"location":"reference/operations-reference/#branch","text":"Splits a stream into multiple substreams based on conditions.","title":"branch"},{"location":"reference/operations-reference/#parameters_14","text":"Parameter Type Required Description predicates Array Yes List of conditions for each branch Each predicate can be defined using: - expression : A simple boolean expression - code : A Python code block returning a boolean","title":"Parameters"},{"location":"reference/operations-reference/#example_14","text":"- type: branch predicates: - expression: value.get(\"amount\") > 1000 - expression: value.get(\"amount\") > 100 - expression: true # Default branch","title":"Example"},{"location":"reference/operations-reference/#error-handling-operations","text":"Error handling operations provide mechanisms to handle errors during processing.","title":"Error Handling Operations"},{"location":"reference/operations-reference/#try","text":"Attempts to execute operations and catches any exceptions.","title":"try"},{"location":"reference/operations-reference/#parameters_15","text":"Parameter Type Required Description operations Array Yes Operations to try catch Array Yes Operations to execute if an exception occurs","title":"Parameters"},{"location":"reference/operations-reference/#example_15","text":"- type: try operations: - type: mapValues mapper: code: parse_complex_json(value) catch: - type: mapValues mapper: code: | log.error(\"Failed to parse JSON: {}\", exception) return {\"error\": \"Failed to parse\", \"original\": value}","title":"Example"},{"location":"reference/operations-reference/#onerror","text":"Specifies what to do when an error occurs in an operation.","title":"onError"},{"location":"reference/operations-reference/#parameters_16","text":"Parameter Type Required Description sendTo String Yes The stream to send error records to withKey Object No How to transform the key for the error record withValue Object No How to transform the value for the error record","title":"Parameters"},{"location":"reference/operations-reference/#example_16","text":"- type: mapValues mapper: code: process_data(value) onError: sendTo: error_stream withKey: \"error-\" + key withValue: {\"original\": value, \"error\": exception.getMessage()}","title":"Example"},{"location":"reference/operations-reference/#custom-operations","text":"KSML allows you to define custom operations using the custom operation type.","title":"Custom Operations"},{"location":"reference/operations-reference/#custom","text":"Executes a custom operation defined in Java code.","title":"custom"},{"location":"reference/operations-reference/#parameters_17","text":"Parameter Type Required Description className String Yes The fully qualified class name of the custom operation config Object No Configuration parameters for the custom operation","title":"Parameters"},{"location":"reference/operations-reference/#example_17","text":"- type: custom className: com.example.ksml.operations.MyCustomOperation config: param1: value1 param2: value2","title":"Example"},{"location":"reference/operations-reference/#combining-operations","text":"Operations can be combined in various ways to create complex processing pipelines.","title":"Combining Operations"},{"location":"reference/operations-reference/#sequential-operations","text":"Operations are executed in sequence, with each operation processing the output of the previous operation. pipelines: my_pipeline: from: input_stream via: - type: filter if: expression: value.get(\"amount\") > 0 - type: mapValues mapper: code: enrich_transaction(value) - type: peek forEach: code: | log.info(\"Processed transaction: {}\", value) to: output_stream","title":"Sequential Operations"},{"location":"reference/operations-reference/#branching-and-merging","text":"You can create complex topologies by branching streams and merging them back together. pipelines: branch_pipeline: from: input_stream via: - type: branch predicates: - expression: value.get(\"type\") == \"A\" - expression: value.get(\"type\") == \"B\" to: - type_a_stream - type_b_stream process_a_pipeline: from: type_a_stream via: - type: mapValues mapper: code: process_type_a(value) to: processed_a_stream process_b_pipeline: from: type_b_stream via: - type: mapValues mapper: code: process_type_b(value) to: processed_b_stream merge_pipeline: from: - processed_a_stream - processed_b_stream to: merged_stream","title":"Branching and Merging"},{"location":"reference/operations-reference/#best-practices","text":"Chain operations thoughtfully : Consider the performance implications of chaining multiple operations. Use stateless operations when possible : Stateless operations are generally more efficient than stateful ones. Be careful with window sizes : Large windows can consume significant memory. Handle errors gracefully : Use error handling operations to prevent pipeline failures. Monitor performance : Keep an eye on throughput and latency, especially for stateful operations.","title":"Best Practices"},{"location":"reference/stream-types-reference/","text":"Streams and Data Types This page explains the fundamental concepts of streams and data types in KSML, providing a foundation for understanding how data flows through your KSML applications. Understanding Streams in KSML In KSML, streams represent continuous flows of data. Every KSML definition file contains a list of declared streams that connect to Kafka topics. These streams are the entry and exit points for your data processing pipelines. Types of Streams KSML supports three types of streams, each with different characteristics and use cases: KStream A KStream represents an unbounded sequence of records. Each record in a KStream is an independent entity or event. Key characteristics: Records are immutable (once created, they cannot be changed) New records can be added to the stream at any time Records are processed one at a time in the order they arrive Ideal for event-based processing Example use cases: Processing individual user actions (clicks, purchases, etc.) Handling sensor readings or log entries Processing transactions KSML definition: streams: user_clicks_stream: topic: user-clicks keyType: string valueType: json KTable A KTable represents a changelog stream from a primary-keyed table. Each record in a KTable is an update to a key in the table. Key characteristics: Records with the same key represent updates to the same entity The latest record for a key represents the current state Records are compacted (only the latest value for each key is retained) Ideal for state-based processing Example use cases: Maintaining user profiles or preferences Tracking current inventory levels Storing configuration settings KSML definition: tables: user_profiles_table: topic: user-profiles keyType: string valueType: avro:UserProfile store: user_profiles_store GlobalKTable A GlobalKTable is similar to a KTable, but with one key difference: it's fully replicated on each instance of your application. Key characteristics: Contains the same data as a KTable Fully replicated on each instance (not partitioned) Allows joins without requiring co-partitioning Ideal for reference data that needs to be available on all instances Example use cases: Reference data (product catalogs, country codes, etc.) Configuration settings needed by all instances Small to medium-sized datasets that change infrequently KSML definition: globalTables: product_catalog: topic: product-catalog keyType: string valueType: avro:Product store: product_catalog_store Optional settings for all stream types Each stream type allows the following additional settings to be specified: offsetResetPolicy : the policy to use when there is no (valid) consumer group offset in Kafka. Choice of earliest , latest , none , or by_duration:<duration> . In the latter case, you can pass in a custom duration. timestampExtractor : a function that is able to extract a timestamp from a consumed message, to be used by Kafka Streams as the message timestamp in all pipeline processing. partitioner : a stream partitioner that determines to which topic partitions an output record needs to be written. Choosing the Right Stream Type The choice between KStream, KTable, and GlobalKTable depends on your specific use case: If you need to... Consider using... Process individual events as they occur KStream Maintain the latest state of entities KTable Join with data that's needed across all partitions GlobalKTable Process time-ordered events KStream Track changes to state over time KTable Access reference data without worrying about partitioning GlobalKTable Best Practices Here are some best practices for working with streams and data types in KSML: Choose the right stream type for your use case : Use KStream for event processing Use KTable for state tracking Use GlobalKTable for reference data Use appropriate data types : Be specific about your types (avoid using any when possible) Use complex types (struct, list, etc.) to represent structured data Consider using windowed types for time-based aggregations Select the right notation : Use AVRO for production systems with schema evolution Use JSON for development and debugging Consider compatibility with upstream and downstream systems Manage schemas effectively : Use a schema registry for AVRO schemas in production Keep schemas under version control Plan for schema evolution Consider performance implications : GlobalKTables replicate data to all instances, which can impact memory usage Complex types and notations may have serialization/deserialization overhead Large schemas can impact performance Examples Example 1: Event Processing with KStream streams: page_views: topic: page-views keyType: string # User ID valueType: json # Page view event offsetResetPolicy: earliest pipelines: process_page_views: from: page_views via: - type: filter if: expression: value.get('duration') > 10 # Only process views longer than 10 seconds # Additional processing steps... to: processed_page_views Example 2: User Profile Management with KTable tables: user_profiles: topic: user-profiles keyType: string # User ID valueType: avro:UserProfile store: user_profiles_store pipelines: update_user_preferences: from: user_preference_updates via: - type: join with: user_profiles valueJoiner: expression: { \"userId\": key, \"name\": value2.get('name'), \"preferences\": value1 } resultType: struct # Additional processing steps... to: updated_user_profiles Example 3: Product Catalog as GlobalKTable globalTables: product_catalog: topic: product-catalog keyType: string # Product ID valueType: avro:Product pipelines: enrich_orders: from: orders via: - type: join with: product_catalog mapper: expression: value.get('productId') # Map from order to product ID resultType: string valueJoiner: expression: { \"orderId\": value1.get('orderId'), \"product\": value2, \"quantity\": value1.get('quantity') } resultType: struct # Additional processing steps... to: enriched_orders Conclusion Understanding streams and data types is fundamental to building effective KSML applications. By choosing the right stream types, data types, and notations for your use case, you can create efficient, maintainable, and scalable stream processing pipelines. For more detailed information, refer to the KSML Language Reference and Data Types Reference .","title":"Streams and Data Types"},{"location":"reference/stream-types-reference/#streams-and-data-types","text":"This page explains the fundamental concepts of streams and data types in KSML, providing a foundation for understanding how data flows through your KSML applications.","title":"Streams and Data Types"},{"location":"reference/stream-types-reference/#understanding-streams-in-ksml","text":"In KSML, streams represent continuous flows of data. Every KSML definition file contains a list of declared streams that connect to Kafka topics. These streams are the entry and exit points for your data processing pipelines.","title":"Understanding Streams in KSML"},{"location":"reference/stream-types-reference/#types-of-streams","text":"KSML supports three types of streams, each with different characteristics and use cases:","title":"Types of Streams"},{"location":"reference/stream-types-reference/#kstream","text":"A KStream represents an unbounded sequence of records. Each record in a KStream is an independent entity or event. Key characteristics: Records are immutable (once created, they cannot be changed) New records can be added to the stream at any time Records are processed one at a time in the order they arrive Ideal for event-based processing Example use cases: Processing individual user actions (clicks, purchases, etc.) Handling sensor readings or log entries Processing transactions KSML definition: streams: user_clicks_stream: topic: user-clicks keyType: string valueType: json","title":"KStream"},{"location":"reference/stream-types-reference/#ktable","text":"A KTable represents a changelog stream from a primary-keyed table. Each record in a KTable is an update to a key in the table. Key characteristics: Records with the same key represent updates to the same entity The latest record for a key represents the current state Records are compacted (only the latest value for each key is retained) Ideal for state-based processing Example use cases: Maintaining user profiles or preferences Tracking current inventory levels Storing configuration settings KSML definition: tables: user_profiles_table: topic: user-profiles keyType: string valueType: avro:UserProfile store: user_profiles_store","title":"KTable"},{"location":"reference/stream-types-reference/#globalktable","text":"A GlobalKTable is similar to a KTable, but with one key difference: it's fully replicated on each instance of your application. Key characteristics: Contains the same data as a KTable Fully replicated on each instance (not partitioned) Allows joins without requiring co-partitioning Ideal for reference data that needs to be available on all instances Example use cases: Reference data (product catalogs, country codes, etc.) Configuration settings needed by all instances Small to medium-sized datasets that change infrequently KSML definition: globalTables: product_catalog: topic: product-catalog keyType: string valueType: avro:Product store: product_catalog_store","title":"GlobalKTable"},{"location":"reference/stream-types-reference/#optional-settings-for-all-stream-types","text":"Each stream type allows the following additional settings to be specified: offsetResetPolicy : the policy to use when there is no (valid) consumer group offset in Kafka. Choice of earliest , latest , none , or by_duration:<duration> . In the latter case, you can pass in a custom duration. timestampExtractor : a function that is able to extract a timestamp from a consumed message, to be used by Kafka Streams as the message timestamp in all pipeline processing. partitioner : a stream partitioner that determines to which topic partitions an output record needs to be written.","title":"Optional settings for all stream types"},{"location":"reference/stream-types-reference/#choosing-the-right-stream-type","text":"The choice between KStream, KTable, and GlobalKTable depends on your specific use case: If you need to... Consider using... Process individual events as they occur KStream Maintain the latest state of entities KTable Join with data that's needed across all partitions GlobalKTable Process time-ordered events KStream Track changes to state over time KTable Access reference data without worrying about partitioning GlobalKTable","title":"Choosing the Right Stream Type"},{"location":"reference/stream-types-reference/#best-practices","text":"Here are some best practices for working with streams and data types in KSML: Choose the right stream type for your use case : Use KStream for event processing Use KTable for state tracking Use GlobalKTable for reference data Use appropriate data types : Be specific about your types (avoid using any when possible) Use complex types (struct, list, etc.) to represent structured data Consider using windowed types for time-based aggregations Select the right notation : Use AVRO for production systems with schema evolution Use JSON for development and debugging Consider compatibility with upstream and downstream systems Manage schemas effectively : Use a schema registry for AVRO schemas in production Keep schemas under version control Plan for schema evolution Consider performance implications : GlobalKTables replicate data to all instances, which can impact memory usage Complex types and notations may have serialization/deserialization overhead Large schemas can impact performance","title":"Best Practices"},{"location":"reference/stream-types-reference/#examples","text":"","title":"Examples"},{"location":"reference/stream-types-reference/#example-1-event-processing-with-kstream","text":"streams: page_views: topic: page-views keyType: string # User ID valueType: json # Page view event offsetResetPolicy: earliest pipelines: process_page_views: from: page_views via: - type: filter if: expression: value.get('duration') > 10 # Only process views longer than 10 seconds # Additional processing steps... to: processed_page_views","title":"Example 1: Event Processing with KStream"},{"location":"reference/stream-types-reference/#example-2-user-profile-management-with-ktable","text":"tables: user_profiles: topic: user-profiles keyType: string # User ID valueType: avro:UserProfile store: user_profiles_store pipelines: update_user_preferences: from: user_preference_updates via: - type: join with: user_profiles valueJoiner: expression: { \"userId\": key, \"name\": value2.get('name'), \"preferences\": value1 } resultType: struct # Additional processing steps... to: updated_user_profiles","title":"Example 2: User Profile Management with KTable"},{"location":"reference/stream-types-reference/#example-3-product-catalog-as-globalktable","text":"globalTables: product_catalog: topic: product-catalog keyType: string # Product ID valueType: avro:Product pipelines: enrich_orders: from: orders via: - type: join with: product_catalog mapper: expression: value.get('productId') # Map from order to product ID resultType: string valueJoiner: expression: { \"orderId\": value1.get('orderId'), \"product\": value2, \"quantity\": value1.get('quantity') } resultType: struct # Additional processing steps... to: enriched_orders","title":"Example 3: Product Catalog as GlobalKTable"},{"location":"reference/stream-types-reference/#conclusion","text":"Understanding streams and data types is fundamental to building effective KSML applications. By choosing the right stream types, data types, and notations for your use case, you can create efficient, maintainable, and scalable stream processing pipelines. For more detailed information, refer to the KSML Language Reference and Data Types Reference .","title":"Conclusion"},{"location":"resources/","text":"Resources Welcome to the KSML Resources section! This section provides additional materials and support resources to help you get the most out of KSML. Available Resources KSML Language Specification The complete KSML language specification, including: Metadata fields like name, version, and description Definitions of streams, tables, and globalTables State store definitions Function definitions and function types Pipeline definitions Producer definitions The corresponding JSON Schema can be used for code completion in IDEs such as IntelliJ and Visual Studio Code. Examples Library A comprehensive collection of KSML examples: Categorized by complexity (beginner, intermediate, advanced) Organized by use case Fully annotated with explanations Ready to use and adapt for your own projects The examples-library is a great place to find inspiration and solutions for common stream processing patterns. Troubleshooting Guide Solutions for common issues and challenges: Diagnosing and fixing common errors Performance troubleshooting Deployment issues Integration problems Debugging techniques FAQ with solutions to frequently encountered problems Migration Guide Resources for migrating to KSML: Moving from Kafka Streams Java code to KSML Upgrading between KSML versions Best practices for migration Common migration challenges and solutions Migration case studies Community and Support Connect with the KSML community and get help: Community forums and discussion groups Official support channels Contributing to KSML Reporting bugs and requesting features Community events and webinars Professional services and training How to Use These Resources These resources are designed to complement the main documentation: When learning KSML : Start with the Getting Started section and Tutorials , then explore the Examples Library for inspiration. When building applications : Refer to the Core Concepts and Reference sections, and check the Examples Library for patterns similar to your use case. When facing challenges : Consult the Troubleshooting Guide for solutions to common issues. When upgrading or migrating : Use the Migration Guide to ensure a smooth transition. When you need help : Visit the Community and Support page to connect with other KSML users and get assistance. Contributing to Resources The KSML community is constantly growing and improving these resources. If you have suggestions, corrections, or new examples to contribute: Visit the Community and Support page to learn how to contribute Submit your contributions through the appropriate channels Help make KSML better for everyone! Your contributions, whether they're new examples, troubleshooting tips, or documentation improvements, are greatly appreciated by the KSML community.","title":"Resources"},{"location":"resources/#resources","text":"Welcome to the KSML Resources section! This section provides additional materials and support resources to help you get the most out of KSML.","title":"Resources"},{"location":"resources/#available-resources","text":"","title":"Available Resources"},{"location":"resources/#ksml-language-specification","text":"The complete KSML language specification, including: Metadata fields like name, version, and description Definitions of streams, tables, and globalTables State store definitions Function definitions and function types Pipeline definitions Producer definitions The corresponding JSON Schema can be used for code completion in IDEs such as IntelliJ and Visual Studio Code.","title":"KSML Language Specification"},{"location":"resources/#examples-library","text":"A comprehensive collection of KSML examples: Categorized by complexity (beginner, intermediate, advanced) Organized by use case Fully annotated with explanations Ready to use and adapt for your own projects The examples-library is a great place to find inspiration and solutions for common stream processing patterns.","title":"Examples Library"},{"location":"resources/#troubleshooting-guide","text":"Solutions for common issues and challenges: Diagnosing and fixing common errors Performance troubleshooting Deployment issues Integration problems Debugging techniques FAQ with solutions to frequently encountered problems","title":"Troubleshooting Guide"},{"location":"resources/#migration-guide","text":"Resources for migrating to KSML: Moving from Kafka Streams Java code to KSML Upgrading between KSML versions Best practices for migration Common migration challenges and solutions Migration case studies","title":"Migration Guide"},{"location":"resources/#community-and-support","text":"Connect with the KSML community and get help: Community forums and discussion groups Official support channels Contributing to KSML Reporting bugs and requesting features Community events and webinars Professional services and training","title":"Community and Support"},{"location":"resources/#how-to-use-these-resources","text":"These resources are designed to complement the main documentation: When learning KSML : Start with the Getting Started section and Tutorials , then explore the Examples Library for inspiration. When building applications : Refer to the Core Concepts and Reference sections, and check the Examples Library for patterns similar to your use case. When facing challenges : Consult the Troubleshooting Guide for solutions to common issues. When upgrading or migrating : Use the Migration Guide to ensure a smooth transition. When you need help : Visit the Community and Support page to connect with other KSML users and get assistance.","title":"How to Use These Resources"},{"location":"resources/#contributing-to-resources","text":"The KSML community is constantly growing and improving these resources. If you have suggestions, corrections, or new examples to contribute: Visit the Community and Support page to learn how to contribute Submit your contributions through the appropriate channels Help make KSML better for everyone! Your contributions, whether they're new examples, troubleshooting tips, or documentation improvements, are greatly appreciated by the KSML community.","title":"Contributing to Resources"},{"location":"resources/community/","text":"KSML Community and Support Welcome to the KSML community! This guide provides information on how to get help, connect with other KSML users, and contribute to the project. Getting Help Documentation The first place to look for answers is the KSML documentation: Getting Started Guide - For beginners Core Concepts - For understanding KSML fundamentals Tutorials - For hands-on learning Reference Documentation - For detailed specifications Troubleshooting Guide - For solving common issues Community Forums Join our community forums to ask questions, share your experiences, and connect with other KSML users: KSML Discussion Forum - For general questions and discussions Stack Overflow - Use the ksml tag for technical questions Issue Tracker If you believe you've found a bug or want to request a feature: Check the existing issues to see if it's already reported If not, create a new issue with: A clear, descriptive title A detailed description of the problem or feature request Steps to reproduce (for bugs) Expected vs. actual behavior (for bugs) KSML version and environment details Commercial Support For enterprise users requiring dedicated support, Axual offers commercial support packages. Contact support@axual.com for more information. Community Resources Community Calls Join our monthly community calls to: - Learn about new features and roadmap - See demos and use cases - Ask questions directly to the KSML team - Connect with other users Check the community calendar for upcoming calls. Slack Channel Join our Slack workspace for real-time discussions: - #ksml-general - General discussions about KSML - #ksml-help - Get help with specific issues - #ksml-announcements - Stay updated on new releases and events Newsletter Subscribe to our newsletter to receive: - Release announcements - Tutorial spotlights - Community highlights - Upcoming events Contributing to KSML We welcome contributions from the community! Here's how you can contribute: Code Contributions Fork the KSML repository Create a branch for your changes Make your changes following our coding standards Write tests for your changes Submit a pull request Before starting work on a significant contribution, please open an issue to discuss it with the maintainers. Documentation Contributions Help improve the KSML documentation: Fork the repository Make your documentation changes Submit a pull request We especially appreciate: - Clarifications to existing documentation - New examples and tutorials - Fixes for typos and broken links Example Contributions Share your KSML examples with the community: Create a well-documented example Ensure it follows best practices Submit it to the examples directory Reporting Issues Quality bug reports help us improve KSML. When reporting issues: Include a clear description of the problem Provide steps to reproduce Include relevant logs and error messages Share your KSML definition file (with sensitive information removed) Mention your environment (KSML version, OS, etc.) Community Guidelines To ensure a positive community experience for everyone, please follow these guidelines: Code of Conduct All community members are expected to adhere to our Code of Conduct , which promotes: Respectful and inclusive communication Constructive feedback Collaborative problem-solving A welcoming environment for everyone Communication Tips Be specific : When asking questions, provide context and details Share knowledge : Answer questions when you can help others Be patient : Remember that community members are volunteers Use appropriate channels : Post questions in the right forum or channel Search first : Check if your question has already been answered Staying Updated Release Information Release Notes - Detailed information about each release GitHub Releases - Release announcements and assets Roadmap - Upcoming features and improvements Social Media Follow us on social media for updates, tips, and community highlights: Twitter LinkedIn YouTube - Tutorials and presentations Events and Meetups Conferences Look for KSML presentations at these conferences: - Kafka Summit - Strata Data Conference - Data + AI Summit Meetups Find local KSML and Kafka Streams meetups: - Meetup.com - Eventbrite Webinars and Workshops Register for upcoming webinars and workshops: - Axual Events Page - KSML YouTube Channel Success Stories We love hearing how you're using KSML! Share your success stories: Write a blog post about your experience Submit a case study to community@axual.com Present at a community call or meetup Featured success stories: - How Company X Reduced Development Time by 50% with KSML - Building Real-time Analytics at Scale with KSML Frequently Asked Questions General Questions Q: Is KSML open source? A: Yes, KSML is open source under the Apache 2.0 license. Q: How does KSML compare to writing Kafka Streams applications in Java? A: KSML provides a higher-level abstraction that makes it easier to define stream processing applications without writing Java code. It's particularly beneficial for data engineers and analysts who may not be Java experts. Q: Can I use KSML in production? A: Yes, KSML is production-ready and used by many organizations in production environments. Technical Questions Q: Can I extend KSML with custom operations? A: Yes, you can create custom operations in Java and use them in your KSML definitions. Q: How does KSML handle schema evolution? A: KSML supports schema evolution through its integration with schema registries and flexible data handling. Q: What's the performance impact of using KSML vs. native Kafka Streams? A: KSML compiles down to native Kafka Streams code, so the runtime performance is comparable to hand-written Kafka Streams applications. Conclusion The KSML community is here to help you succeed with your stream processing projects. Whether you're just getting started or you're an experienced user, we encourage you to participate, ask questions, and share your knowledge. Remember that every community member was once a beginner. By helping others and contributing back, you help make KSML better for everyone. We look forward to seeing what you build with KSML!","title":"KSML Community and Support"},{"location":"resources/community/#ksml-community-and-support","text":"Welcome to the KSML community! This guide provides information on how to get help, connect with other KSML users, and contribute to the project.","title":"KSML Community and Support"},{"location":"resources/community/#getting-help","text":"","title":"Getting Help"},{"location":"resources/community/#documentation","text":"The first place to look for answers is the KSML documentation: Getting Started Guide - For beginners Core Concepts - For understanding KSML fundamentals Tutorials - For hands-on learning Reference Documentation - For detailed specifications Troubleshooting Guide - For solving common issues","title":"Documentation"},{"location":"resources/community/#community-forums","text":"Join our community forums to ask questions, share your experiences, and connect with other KSML users: KSML Discussion Forum - For general questions and discussions Stack Overflow - Use the ksml tag for technical questions","title":"Community Forums"},{"location":"resources/community/#issue-tracker","text":"If you believe you've found a bug or want to request a feature: Check the existing issues to see if it's already reported If not, create a new issue with: A clear, descriptive title A detailed description of the problem or feature request Steps to reproduce (for bugs) Expected vs. actual behavior (for bugs) KSML version and environment details","title":"Issue Tracker"},{"location":"resources/community/#commercial-support","text":"For enterprise users requiring dedicated support, Axual offers commercial support packages. Contact support@axual.com for more information.","title":"Commercial Support"},{"location":"resources/community/#community-resources","text":"","title":"Community Resources"},{"location":"resources/community/#community-calls","text":"Join our monthly community calls to: - Learn about new features and roadmap - See demos and use cases - Ask questions directly to the KSML team - Connect with other users Check the community calendar for upcoming calls.","title":"Community Calls"},{"location":"resources/community/#slack-channel","text":"Join our Slack workspace for real-time discussions: - #ksml-general - General discussions about KSML - #ksml-help - Get help with specific issues - #ksml-announcements - Stay updated on new releases and events","title":"Slack Channel"},{"location":"resources/community/#newsletter","text":"Subscribe to our newsletter to receive: - Release announcements - Tutorial spotlights - Community highlights - Upcoming events","title":"Newsletter"},{"location":"resources/community/#contributing-to-ksml","text":"We welcome contributions from the community! Here's how you can contribute:","title":"Contributing to KSML"},{"location":"resources/community/#code-contributions","text":"Fork the KSML repository Create a branch for your changes Make your changes following our coding standards Write tests for your changes Submit a pull request Before starting work on a significant contribution, please open an issue to discuss it with the maintainers.","title":"Code Contributions"},{"location":"resources/community/#documentation-contributions","text":"Help improve the KSML documentation: Fork the repository Make your documentation changes Submit a pull request We especially appreciate: - Clarifications to existing documentation - New examples and tutorials - Fixes for typos and broken links","title":"Documentation Contributions"},{"location":"resources/community/#example-contributions","text":"Share your KSML examples with the community: Create a well-documented example Ensure it follows best practices Submit it to the examples directory","title":"Example Contributions"},{"location":"resources/community/#reporting-issues","text":"Quality bug reports help us improve KSML. When reporting issues: Include a clear description of the problem Provide steps to reproduce Include relevant logs and error messages Share your KSML definition file (with sensitive information removed) Mention your environment (KSML version, OS, etc.)","title":"Reporting Issues"},{"location":"resources/community/#community-guidelines","text":"To ensure a positive community experience for everyone, please follow these guidelines:","title":"Community Guidelines"},{"location":"resources/community/#code-of-conduct","text":"All community members are expected to adhere to our Code of Conduct , which promotes: Respectful and inclusive communication Constructive feedback Collaborative problem-solving A welcoming environment for everyone","title":"Code of Conduct"},{"location":"resources/community/#communication-tips","text":"Be specific : When asking questions, provide context and details Share knowledge : Answer questions when you can help others Be patient : Remember that community members are volunteers Use appropriate channels : Post questions in the right forum or channel Search first : Check if your question has already been answered","title":"Communication Tips"},{"location":"resources/community/#staying-updated","text":"","title":"Staying Updated"},{"location":"resources/community/#release-information","text":"Release Notes - Detailed information about each release GitHub Releases - Release announcements and assets Roadmap - Upcoming features and improvements","title":"Release Information"},{"location":"resources/community/#social-media","text":"Follow us on social media for updates, tips, and community highlights: Twitter LinkedIn YouTube - Tutorials and presentations","title":"Social Media"},{"location":"resources/community/#events-and-meetups","text":"","title":"Events and Meetups"},{"location":"resources/community/#conferences","text":"Look for KSML presentations at these conferences: - Kafka Summit - Strata Data Conference - Data + AI Summit","title":"Conferences"},{"location":"resources/community/#meetups","text":"Find local KSML and Kafka Streams meetups: - Meetup.com - Eventbrite","title":"Meetups"},{"location":"resources/community/#webinars-and-workshops","text":"Register for upcoming webinars and workshops: - Axual Events Page - KSML YouTube Channel","title":"Webinars and Workshops"},{"location":"resources/community/#success-stories","text":"We love hearing how you're using KSML! Share your success stories: Write a blog post about your experience Submit a case study to community@axual.com Present at a community call or meetup Featured success stories: - How Company X Reduced Development Time by 50% with KSML - Building Real-time Analytics at Scale with KSML","title":"Success Stories"},{"location":"resources/community/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"resources/community/#general-questions","text":"Q: Is KSML open source? A: Yes, KSML is open source under the Apache 2.0 license. Q: How does KSML compare to writing Kafka Streams applications in Java? A: KSML provides a higher-level abstraction that makes it easier to define stream processing applications without writing Java code. It's particularly beneficial for data engineers and analysts who may not be Java experts. Q: Can I use KSML in production? A: Yes, KSML is production-ready and used by many organizations in production environments.","title":"General Questions"},{"location":"resources/community/#technical-questions","text":"Q: Can I extend KSML with custom operations? A: Yes, you can create custom operations in Java and use them in your KSML definitions. Q: How does KSML handle schema evolution? A: KSML supports schema evolution through its integration with schema registries and flexible data handling. Q: What's the performance impact of using KSML vs. native Kafka Streams? A: KSML compiles down to native Kafka Streams code, so the runtime performance is comparable to hand-written Kafka Streams applications.","title":"Technical Questions"},{"location":"resources/community/#conclusion","text":"The KSML community is here to help you succeed with your stream processing projects. Whether you're just getting started or you're an experienced user, we encourage you to participate, ask questions, and share your knowledge. Remember that every community member was once a beginner. By helping others and contributing back, you help make KSML better for everyone. We look forward to seeing what you build with KSML!","title":"Conclusion"},{"location":"resources/examples-library/","text":"KSML Examples Library This document provides a collection of ready-to-use KSML examples for common stream processing patterns and use cases. Each example includes a description, the complete KSML code, and explanations of key concepts. Basic Examples Hello World A simple pipeline that reads messages from one topic and logs them to stdout. streams: input: topic: input-topic keyType: string valueType: string output: topic: output-topic keyType: string valueType: string pipelines: hello_world: from: input via: - type: peek forEach: code: | log.info(\"Processing message: key={}, value={}\", key, value) to: output Simple Transformation Transforms JSON messages by extracting and restructuring fields and writes the results to a target topic. streams: input: topic: input-topic keyType: string valueType: json output: topic: output-topic keyType: string valueType: json pipelines: transform: from: input via: - type: mapValues mapper: code: | return { \"id\": value.get(\"user_id\"), \"name\": value.get(\"first_name\") + \" \" + value.get(\"last_name\"), \"email\": value.get(\"email\"), \"created_at\": value.get(\"signup_date\") } to: output Filtering Filters messages based on a condition. streams: input: topic: input-topic keyType: string valueType: json output: topic: output-topic keyType: string valueType: json pipelines: filter: from: input via: - type: filter if: expression: value.get(\"age\") >= 18 to: output Intermediate Examples Aggregation Counts the number of events by category. streams: input: topic: input-topic keyType: string valueType: json output: topic: output-topic keyType: json valueType: json pipelines: count_by_category: from: input via: - type: selectKey mapper: expression: value.get(\"category\") - type: groupByKey - type: count store: name: category_count type: window windowSize: 10m retention: 1h caching: false - type: toStream to: output Windowed Aggregation Calculates the average value over a time window. streams: input: topic: input-topic keyType: string valueType: json output: topic: output-topic keyType: string valueType: json pipelines: average_by_window: from: input via: - type: selectKey mapper: expression: value.get(\"category\") - type: groupByKey - type: windowByTime windowType: tumbling duration: 1h grace: 10s - type: aggregate initializer: expression: {\"sum\": 0, \"count\": 0} resultType: struct aggregator: code: | count = aggregate.get(\"count\", 0) + 1 sum = aggregate.get(\"sum\", 0) + value.get(\"amount\", 0) return { \"count\": count, \"sum\": sum, \"average\": sum / count } resultType: struct - type: toStream to: output Join Example Joins a stream of orders with a table of products. streams: orders: topic: orders-topic keyType: string valueType: json products: topic: products-topic keyType: string valueType: json enriched_orders: topic: enriched-orders-topic keyType: string valueType: json functions: enrich_order: type: mapValues code: | order = value1 product = value2 if product is None: return order return { \"order_id\": order.get(\"order_id\"), \"product_id\": order.get(\"product_id\"), \"product_name\": product.get(\"name\", \"Unknown\"), \"product_category\": product.get(\"category\", \"Unknown\"), \"quantity\": order.get(\"quantity\", 0), \"unit_price\": product.get(\"price\", 0), \"total_price\": order.get(\"quantity\", 0) * product.get(\"price\", 0), \"customer_id\": order.get(\"customer_id\"), \"order_date\": order.get(\"order_date\") } pipelines: enrich_orders: from: orders via: - type: selectKey mapper: expression: value.get(\"product_id\") - type: join with: products valueJoiner: enrich_order to: enriched_orders Advanced Examples Complex Event Processing Detects patterns in a stream of events. streams: events: topic: events-topic keyType: string valueType: json alerts: topic: alerts-topic keyType: string valueType: json functions: detect_pattern: type: valueTransformer code: | events = value # Check for a specific pattern: 3 failed login attempts within 1 minute if len(events) < 3: return None # Sort events by timestamp sorted_events = sorted(events, key=lambda e: e.get(\"timestamp\", 0)) # Check if all events are login failures all_failures = all(e.get(\"event_type\") == \"LOGIN_FAILURE\" for e in sorted_events) # Check if events occurred within 1 minute first_timestamp = sorted_events[0].get(\"timestamp\", 0) last_timestamp = sorted_events[-1].get(\"timestamp\", 0) within_timeframe = (last_timestamp - first_timestamp) <= 60000 # 1 minute in ms if all_failures and within_timeframe: return { \"alert_type\": \"POTENTIAL_BREACH\", \"user_id\": sorted_events[0].get(\"user_id\"), \"attempt_count\": len(sorted_events), \"first_attempt\": first_timestamp, \"last_attempt\": last_timestamp, \"ip_addresses\": list(set(e.get(\"ip_address\") for e in sorted_events)) } return None pipelines: security_monitoring: from: events via: - type: filter if: expression: value.get(\"event_type\") == \"LOGIN_FAILURE\" - type: selectKey mapper: expression: value.get(\"user_id\") - type: groupByKey - type: windowBySession inactivityGap: 5m # 5 minute session grace: 10s - type: aggregate initializer: expression: [] aggregator: expression: aggregatedValue + [value] - type: transformValue mapper: detect_pattern - type: filter if: expression: value is not None to: alerts Multi-Stream Processing Processes data from multiple input streams and produces multiple output streams. streams: clicks: topic: user-clicks keyType: string valueType: json purchases: topic: user-purchases keyType: string valueType: json user_profiles: topic: user-profiles keyType: string valueType: json click_stats: topic: click-statistics keyType: string valueType: json purchase_stats: topic: purchase-statistics keyType: string valueType: json user_activity: topic: user-activity keyType: string valueType: json pipelines: # Process clicks process_clicks: from: clicks via: - type: selectKey mapper: expression: value.get(\"user_id\") - type: groupByKey - type: windowByTime windowType: tumbling timeDifference: 1h - type: count - type: toStream - type: transformValue mapper: expression: | { \"user_id\": key.get(\"key\"), \"click_count\": value, \"datetime\": key.get(\"startTime\") } to: click_stats # Process purchases process_purchases: from: purchases via: - type: selectKey mapper: expression: value.get(\"user_id\") - type: groupByKey - type: aggregate initializer: expression: | {\"count\": 0, \"total\": 0} aggregator: code: | count = aggregatedValue.get(\"count\", 0) + 1 total = aggregatedValue.get(\"total\", 0) + value.get(\"amount\", 0) return { \"count\": count, \"total\": total } - type: toStream - type: transformValue mapper: code: | return { \"user_id\": key.get(\"key\"), \"purchase_count\": value.get(\"count\"), \"total_spent\": value.get(\"total\"), \"average_order\": value.get(\"total\") / value.get(\"count\") } to: purchase_stats # Combine user activity combine_activity: from: clicks via: - type: selectKey mapper: expression: value.get(\"user_id\") - type: leftJoin with: purchases valueJoiner: - type: leftJoin with: user_profiles valueJoiner: - type: mapValues mapper: code: | clicks = value if value else {} purchases = foreignValue if foreignValue else {} profile = foreignValue2 if foreignValue2 else {} return { \"user_id\": key, \"name\": profile.get(\"name\", \"Unknown\"), \"email\": profile.get(\"email\", \"Unknown\"), \"last_click\": clicks.get(\"timestamp\"), \"last_purchase\": purchases.get(\"timestamp\"), \"lifetime_value\": purchases.get(\"amount\", 0), \"user_segment\": profile.get(\"segment\", \"Unknown\") } to: user_activity Error Handling Demonstrates error handling patterns in KSML. streams: input: topic: input-topic keyType: string valueType: json output: topic: output-topic keyType: string valueType: json error_stream: topic: error-topic keyType: string valueType: json functions: process_data: type: mapValues parameters: - name: data type: object code: | # This function might throw exceptions if \"required_field\" not in data: raise ValueError(\"Missing required field\") result = { \"id\": data.get(\"id\"), \"processed_value\": data.get(\"value\") * 2, \"timestamp\": int(time.time() * 1000) } return result pipelines: robust_processing: from: input via: - type: try operations: - type: mapValues mapper: code: process_data(value) catch: - type: peek forEach: code: | log.error(\"Error processing record: {}\", exception) - type: mapValues mapper: code: | return { \"original_data\": value, \"error\": str(exception), \"timestamp\": int(time.time() * 1000) } - type: to stream: error_stream to: output Domain-Specific Examples E-commerce Processes order data for an e-commerce application. streams: orders: topic: orders keyType: string valueType: json inventory: topic: inventory keyType: string valueType: json shipments: topic: shipments keyType: string valueType: json order_updates: topic: order-updates keyType: string valueType: json pipelines: process_orders: from: orders via: - type: peek forEach: code: | log.info(\"Processing order: {}\", value.get(\"order_id\")) - type: flatMap mapper: code: | # Create a record for each item in the order result = [] for item in value.get(\"items\", []): result.append(( item.get(\"product_id\"), { \"order_id\": value.get(\"order_id\"), \"product_id\": item.get(\"product_id\"), \"quantity\": item.get(\"quantity\"), \"customer_id\": value.get(\"customer_id\"), \"order_date\": value.get(\"order_date\") } )) return result - type: join with: inventory - type: mapValues mapper: code: | order_item = value inventory_item = foreignValue # Check if item is in stock in_stock = inventory_item.get(\"quantity\", 0) >= order_item.get(\"quantity\", 0) return { \"order_id\": order_item.get(\"order_id\"), \"product_id\": order_item.get(\"product_id\"), \"product_name\": inventory_item.get(\"name\", \"Unknown\"), \"quantity\": order_item.get(\"quantity\"), \"in_stock\": in_stock, \"estimated_ship_date\": inventory_item.get(\"next_restock_date\") if not in_stock else order_item.get(\"order_date\") } to: order_updates IoT Sensor Processing Processes sensor data from IoT devices. streams: sensor_data: topic: sensor-readings keyType: string valueType: json device_metadata: topic: device-metadata keyType: string valueType: json alerts: topic: sensor-alerts keyType: string valueType: json aggregated_readings: topic: aggregated-readings keyType: string valueType: json pipelines: # Process raw sensor data process_readings: from: sensor_data via: - type: join with: device_metadata - type: mapValues mapper: code: | reading = value metadata = foreignValue # Enrich with device metadata return { \"device_id\": reading.get(\"device_id\"), \"sensor_type\": metadata.get(\"sensor_type\"), \"location\": metadata.get(\"location\"), \"reading\": reading.get(\"value\"), \"unit\": metadata.get(\"unit\"), \"timestamp\": reading.get(\"timestamp\"), \"battery_level\": reading.get(\"battery_level\") } - type: branch predicates: - code: | # Check for anomalous readings min_threshold = foreignValue.get(\"min_threshold\") max_threshold = foreignValue.get(\"max_threshold\") reading_value = value.get(\"reading\") return (reading_value < min_threshold or reading_value > max_threshold) - expression: true # All readings to: - alerts - aggregated_readings Best Practices When using these examples, consider the following best practices: Adapt to your specific use case : These examples provide a starting point. Modify them to fit your specific requirements. Test thoroughly : Always test your KSML applications with representative data before deploying to production. Monitor performance : Keep an eye on throughput, latency, and resource usage, especially for stateful operations. Handle errors gracefully : Implement proper error handling to prevent pipeline failures. Document your code : Add comments to explain complex logic and business rules. Contributing Examples We welcome contributions to this examples library! If you have a useful KSML pattern or solution to share: Document your example with clear explanations Include the complete KSML code Explain the key concepts and patterns used Submit a pull request to the KSML repository Your examples will help the community learn and apply KSML more effectively.","title":"KSML Examples Library"},{"location":"resources/examples-library/#ksml-examples-library","text":"This document provides a collection of ready-to-use KSML examples for common stream processing patterns and use cases. Each example includes a description, the complete KSML code, and explanations of key concepts.","title":"KSML Examples Library"},{"location":"resources/examples-library/#basic-examples","text":"","title":"Basic Examples"},{"location":"resources/examples-library/#hello-world","text":"A simple pipeline that reads messages from one topic and logs them to stdout. streams: input: topic: input-topic keyType: string valueType: string output: topic: output-topic keyType: string valueType: string pipelines: hello_world: from: input via: - type: peek forEach: code: | log.info(\"Processing message: key={}, value={}\", key, value) to: output","title":"Hello World"},{"location":"resources/examples-library/#simple-transformation","text":"Transforms JSON messages by extracting and restructuring fields and writes the results to a target topic. streams: input: topic: input-topic keyType: string valueType: json output: topic: output-topic keyType: string valueType: json pipelines: transform: from: input via: - type: mapValues mapper: code: | return { \"id\": value.get(\"user_id\"), \"name\": value.get(\"first_name\") + \" \" + value.get(\"last_name\"), \"email\": value.get(\"email\"), \"created_at\": value.get(\"signup_date\") } to: output","title":"Simple Transformation"},{"location":"resources/examples-library/#filtering","text":"Filters messages based on a condition. streams: input: topic: input-topic keyType: string valueType: json output: topic: output-topic keyType: string valueType: json pipelines: filter: from: input via: - type: filter if: expression: value.get(\"age\") >= 18 to: output","title":"Filtering"},{"location":"resources/examples-library/#intermediate-examples","text":"","title":"Intermediate Examples"},{"location":"resources/examples-library/#aggregation","text":"Counts the number of events by category. streams: input: topic: input-topic keyType: string valueType: json output: topic: output-topic keyType: json valueType: json pipelines: count_by_category: from: input via: - type: selectKey mapper: expression: value.get(\"category\") - type: groupByKey - type: count store: name: category_count type: window windowSize: 10m retention: 1h caching: false - type: toStream to: output","title":"Aggregation"},{"location":"resources/examples-library/#windowed-aggregation","text":"Calculates the average value over a time window. streams: input: topic: input-topic keyType: string valueType: json output: topic: output-topic keyType: string valueType: json pipelines: average_by_window: from: input via: - type: selectKey mapper: expression: value.get(\"category\") - type: groupByKey - type: windowByTime windowType: tumbling duration: 1h grace: 10s - type: aggregate initializer: expression: {\"sum\": 0, \"count\": 0} resultType: struct aggregator: code: | count = aggregate.get(\"count\", 0) + 1 sum = aggregate.get(\"sum\", 0) + value.get(\"amount\", 0) return { \"count\": count, \"sum\": sum, \"average\": sum / count } resultType: struct - type: toStream to: output","title":"Windowed Aggregation"},{"location":"resources/examples-library/#join-example","text":"Joins a stream of orders with a table of products. streams: orders: topic: orders-topic keyType: string valueType: json products: topic: products-topic keyType: string valueType: json enriched_orders: topic: enriched-orders-topic keyType: string valueType: json functions: enrich_order: type: mapValues code: | order = value1 product = value2 if product is None: return order return { \"order_id\": order.get(\"order_id\"), \"product_id\": order.get(\"product_id\"), \"product_name\": product.get(\"name\", \"Unknown\"), \"product_category\": product.get(\"category\", \"Unknown\"), \"quantity\": order.get(\"quantity\", 0), \"unit_price\": product.get(\"price\", 0), \"total_price\": order.get(\"quantity\", 0) * product.get(\"price\", 0), \"customer_id\": order.get(\"customer_id\"), \"order_date\": order.get(\"order_date\") } pipelines: enrich_orders: from: orders via: - type: selectKey mapper: expression: value.get(\"product_id\") - type: join with: products valueJoiner: enrich_order to: enriched_orders","title":"Join Example"},{"location":"resources/examples-library/#advanced-examples","text":"","title":"Advanced Examples"},{"location":"resources/examples-library/#complex-event-processing","text":"Detects patterns in a stream of events. streams: events: topic: events-topic keyType: string valueType: json alerts: topic: alerts-topic keyType: string valueType: json functions: detect_pattern: type: valueTransformer code: | events = value # Check for a specific pattern: 3 failed login attempts within 1 minute if len(events) < 3: return None # Sort events by timestamp sorted_events = sorted(events, key=lambda e: e.get(\"timestamp\", 0)) # Check if all events are login failures all_failures = all(e.get(\"event_type\") == \"LOGIN_FAILURE\" for e in sorted_events) # Check if events occurred within 1 minute first_timestamp = sorted_events[0].get(\"timestamp\", 0) last_timestamp = sorted_events[-1].get(\"timestamp\", 0) within_timeframe = (last_timestamp - first_timestamp) <= 60000 # 1 minute in ms if all_failures and within_timeframe: return { \"alert_type\": \"POTENTIAL_BREACH\", \"user_id\": sorted_events[0].get(\"user_id\"), \"attempt_count\": len(sorted_events), \"first_attempt\": first_timestamp, \"last_attempt\": last_timestamp, \"ip_addresses\": list(set(e.get(\"ip_address\") for e in sorted_events)) } return None pipelines: security_monitoring: from: events via: - type: filter if: expression: value.get(\"event_type\") == \"LOGIN_FAILURE\" - type: selectKey mapper: expression: value.get(\"user_id\") - type: groupByKey - type: windowBySession inactivityGap: 5m # 5 minute session grace: 10s - type: aggregate initializer: expression: [] aggregator: expression: aggregatedValue + [value] - type: transformValue mapper: detect_pattern - type: filter if: expression: value is not None to: alerts","title":"Complex Event Processing"},{"location":"resources/examples-library/#multi-stream-processing","text":"Processes data from multiple input streams and produces multiple output streams. streams: clicks: topic: user-clicks keyType: string valueType: json purchases: topic: user-purchases keyType: string valueType: json user_profiles: topic: user-profiles keyType: string valueType: json click_stats: topic: click-statistics keyType: string valueType: json purchase_stats: topic: purchase-statistics keyType: string valueType: json user_activity: topic: user-activity keyType: string valueType: json pipelines: # Process clicks process_clicks: from: clicks via: - type: selectKey mapper: expression: value.get(\"user_id\") - type: groupByKey - type: windowByTime windowType: tumbling timeDifference: 1h - type: count - type: toStream - type: transformValue mapper: expression: | { \"user_id\": key.get(\"key\"), \"click_count\": value, \"datetime\": key.get(\"startTime\") } to: click_stats # Process purchases process_purchases: from: purchases via: - type: selectKey mapper: expression: value.get(\"user_id\") - type: groupByKey - type: aggregate initializer: expression: | {\"count\": 0, \"total\": 0} aggregator: code: | count = aggregatedValue.get(\"count\", 0) + 1 total = aggregatedValue.get(\"total\", 0) + value.get(\"amount\", 0) return { \"count\": count, \"total\": total } - type: toStream - type: transformValue mapper: code: | return { \"user_id\": key.get(\"key\"), \"purchase_count\": value.get(\"count\"), \"total_spent\": value.get(\"total\"), \"average_order\": value.get(\"total\") / value.get(\"count\") } to: purchase_stats # Combine user activity combine_activity: from: clicks via: - type: selectKey mapper: expression: value.get(\"user_id\") - type: leftJoin with: purchases valueJoiner: - type: leftJoin with: user_profiles valueJoiner: - type: mapValues mapper: code: | clicks = value if value else {} purchases = foreignValue if foreignValue else {} profile = foreignValue2 if foreignValue2 else {} return { \"user_id\": key, \"name\": profile.get(\"name\", \"Unknown\"), \"email\": profile.get(\"email\", \"Unknown\"), \"last_click\": clicks.get(\"timestamp\"), \"last_purchase\": purchases.get(\"timestamp\"), \"lifetime_value\": purchases.get(\"amount\", 0), \"user_segment\": profile.get(\"segment\", \"Unknown\") } to: user_activity","title":"Multi-Stream Processing"},{"location":"resources/examples-library/#error-handling","text":"Demonstrates error handling patterns in KSML. streams: input: topic: input-topic keyType: string valueType: json output: topic: output-topic keyType: string valueType: json error_stream: topic: error-topic keyType: string valueType: json functions: process_data: type: mapValues parameters: - name: data type: object code: | # This function might throw exceptions if \"required_field\" not in data: raise ValueError(\"Missing required field\") result = { \"id\": data.get(\"id\"), \"processed_value\": data.get(\"value\") * 2, \"timestamp\": int(time.time() * 1000) } return result pipelines: robust_processing: from: input via: - type: try operations: - type: mapValues mapper: code: process_data(value) catch: - type: peek forEach: code: | log.error(\"Error processing record: {}\", exception) - type: mapValues mapper: code: | return { \"original_data\": value, \"error\": str(exception), \"timestamp\": int(time.time() * 1000) } - type: to stream: error_stream to: output","title":"Error Handling"},{"location":"resources/examples-library/#domain-specific-examples","text":"","title":"Domain-Specific Examples"},{"location":"resources/examples-library/#e-commerce","text":"Processes order data for an e-commerce application. streams: orders: topic: orders keyType: string valueType: json inventory: topic: inventory keyType: string valueType: json shipments: topic: shipments keyType: string valueType: json order_updates: topic: order-updates keyType: string valueType: json pipelines: process_orders: from: orders via: - type: peek forEach: code: | log.info(\"Processing order: {}\", value.get(\"order_id\")) - type: flatMap mapper: code: | # Create a record for each item in the order result = [] for item in value.get(\"items\", []): result.append(( item.get(\"product_id\"), { \"order_id\": value.get(\"order_id\"), \"product_id\": item.get(\"product_id\"), \"quantity\": item.get(\"quantity\"), \"customer_id\": value.get(\"customer_id\"), \"order_date\": value.get(\"order_date\") } )) return result - type: join with: inventory - type: mapValues mapper: code: | order_item = value inventory_item = foreignValue # Check if item is in stock in_stock = inventory_item.get(\"quantity\", 0) >= order_item.get(\"quantity\", 0) return { \"order_id\": order_item.get(\"order_id\"), \"product_id\": order_item.get(\"product_id\"), \"product_name\": inventory_item.get(\"name\", \"Unknown\"), \"quantity\": order_item.get(\"quantity\"), \"in_stock\": in_stock, \"estimated_ship_date\": inventory_item.get(\"next_restock_date\") if not in_stock else order_item.get(\"order_date\") } to: order_updates","title":"E-commerce"},{"location":"resources/examples-library/#iot-sensor-processing","text":"Processes sensor data from IoT devices. streams: sensor_data: topic: sensor-readings keyType: string valueType: json device_metadata: topic: device-metadata keyType: string valueType: json alerts: topic: sensor-alerts keyType: string valueType: json aggregated_readings: topic: aggregated-readings keyType: string valueType: json pipelines: # Process raw sensor data process_readings: from: sensor_data via: - type: join with: device_metadata - type: mapValues mapper: code: | reading = value metadata = foreignValue # Enrich with device metadata return { \"device_id\": reading.get(\"device_id\"), \"sensor_type\": metadata.get(\"sensor_type\"), \"location\": metadata.get(\"location\"), \"reading\": reading.get(\"value\"), \"unit\": metadata.get(\"unit\"), \"timestamp\": reading.get(\"timestamp\"), \"battery_level\": reading.get(\"battery_level\") } - type: branch predicates: - code: | # Check for anomalous readings min_threshold = foreignValue.get(\"min_threshold\") max_threshold = foreignValue.get(\"max_threshold\") reading_value = value.get(\"reading\") return (reading_value < min_threshold or reading_value > max_threshold) - expression: true # All readings to: - alerts - aggregated_readings","title":"IoT Sensor Processing"},{"location":"resources/examples-library/#best-practices","text":"When using these examples, consider the following best practices: Adapt to your specific use case : These examples provide a starting point. Modify them to fit your specific requirements. Test thoroughly : Always test your KSML applications with representative data before deploying to production. Monitor performance : Keep an eye on throughput, latency, and resource usage, especially for stateful operations. Handle errors gracefully : Implement proper error handling to prevent pipeline failures. Document your code : Add comments to explain complex logic and business rules.","title":"Best Practices"},{"location":"resources/examples-library/#contributing-examples","text":"We welcome contributions to this examples library! If you have a useful KSML pattern or solution to share: Document your example with clear explanations Include the complete KSML code Explain the key concepts and patterns used Submit a pull request to the KSML repository Your examples will help the community learn and apply KSML more effectively.","title":"Contributing Examples"},{"location":"resources/migration/","text":"KSML Migration Guide This document provides guidance for migrating between different versions of KSML, as well as migrating from other stream processing frameworks to KSML. Migrating Between KSML Versions Migrating from 1.x to 2.x KSML 2.x introduces several breaking changes and new features compared to KSML 1.x. This section outlines the key differences and provides guidance on how to update your KSML applications. Key Changes New Pipeline Syntax KSML 2.x introduces a more concise and expressive pipeline syntax: KSML 1.x: yaml pipelines: my_pipeline: source: stream: input_stream processors: - type: filter condition: expression: value.get(\"amount\") > 0 - type: mapValues mapper: code: transform_value(value) sink: stream: output_stream KSML 2.x: yaml pipelines: my_pipeline: from: input_stream via: - type: filter if: expression: value.get(\"amount\") > 0 - type: mapValues mapper: code: transform_value(value) to: output_stream Function Definitions KSML 2.x requires explicit function types: KSML 1.x: yaml functions: transform_value: code: | return {\"id\": value.get(\"id\"), \"amount\": value.get(\"amount\") * 2} KSML 2.x: yaml functions: transform_value: type: mapper code: | return {\"id\": value.get(\"id\"), \"amount\": value.get(\"amount\") * 2} Stream Definitions KSML 2.x uses a more structured approach for defining streams: KSML 1.x: yaml streams: input_stream: topic: input-topic key-type: string value-type: json KSML 2.x: yaml streams: input_stream: topic: input-topic keyType: string valueType: json Error Handling KSML 2.x introduces improved error handling capabilities: KSML 1.x: yaml # Limited error handling capabilities KSML 2.x: yaml pipelines: my_pipeline: from: input_stream via: - type: try operations: - type: mapValues mapper: code: process_data(value) catch: - type: mapValues mapper: code: | log.error(\"Error processing data: {}\", exception) return {\"error\": str(exception), \"original\": value} to: output_stream Migration Steps Update Stream Definitions Replace hyphenated property names with camelCase (e.g., key-type \u2192 keyType ) Review and update data type definitions Update Function Definitions Add explicit function types to all functions Review function parameters and return types Update Pipeline Definitions Replace source and sink with from and to Replace processors with via Update operation syntax (e.g., condition \u2192 if ) Update Error Handling Implement the new error handling capabilities where appropriate Test Thoroughly Run tests to ensure the migrated application behaves as expected Monitor for any performance differences Migrating from 2.x to 3.x KSML 3.x focuses on performance improvements, enhanced state management, and better integration with external systems. Key Changes State Store Management KSML 3.x introduces a more flexible state store configuration: KSML 2.x: yaml tables: user_profiles: topic: user-profiles keyType: string valueType: avro:UserProfile store: user_profiles_store KSML 3.x: yaml tables: user_profiles: topic: user-profiles keyType: string valueType: avro:UserProfile store: name: user_profiles_store type: persistent config: retention.ms: 604800000 # 7 days Enhanced Schema Management KSML 3.x provides better schema evolution support: KSML 2.x: yaml streams: sensor_data: topic: sensor-readings keyType: string valueType: avro:SensorReading schemaRegistry: http://schema-registry:8081 KSML 3.x: yaml streams: sensor_data: topic: sensor-readings keyType: string valueType: avro:SensorReading schema: registry: http://schema-registry:8081 compatibility: BACKWARD subject: sensor-readings-value Improved Metrics KSML 3.x offers more comprehensive metrics and monitoring: KSML 2.x: yaml config: metrics: reporters: - type: jmx KSML 3.x: yaml config: metrics: reporters: - type: jmx - type: prometheus port: 8080 path: /metrics tags: application: \"ksml-app\" environment: \"${ENV:-dev}\" Migration Steps Update State Store Configurations Expand state store configurations with the new options Consider retention periods and cleanup policies Update Schema Management Enhance schema configurations with compatibility settings Review subject naming strategies Enhance Metrics Configuration Add additional metrics reporters as needed Configure tags for better metrics organization Test Thoroughly Verify state store behavior Check schema evolution handling Monitor metrics collection Migrating from Other Frameworks to KSML Migrating from Kafka Streams Java API If you're currently using the Kafka Streams Java API directly, migrating to KSML can significantly reduce code complexity while maintaining the same processing capabilities. Example Migration Kafka Streams Java API: // Example Java code (simplified for illustration) StreamsBuilder builder = new StreamsBuilder(); // Define streams KStream<String, Order> orders = builder.stream( \"orders\", Consumed.with(Serdes.String(), orderSerde) ); // Filter and transform KStream<String, EnrichedOrder> processedOrders = orders .filter((key, order) -> order.getAmount() > 0) .mapValues(order -> { EnrichedOrder enriched = new EnrichedOrder(); enriched.setOrderId(order.getOrderId()); enriched.setAmount(order.getAmount()); enriched.setProcessedTime(System.currentTimeMillis()); return enriched; }); // Write to output topic processedOrders.to( \"processed-orders\", Produced.with(Serdes.String(), enrichedOrderSerde) ); Equivalent KSML: streams: orders: topic: orders keyType: string valueType: avro:Order processed_orders: topic: processed-orders keyType: string valueType: avro:EnrichedOrder functions: enrich_order: type: mapper code: | return { \"orderId\": value.get(\"orderId\"), \"amount\": value.get(\"amount\"), \"processedTime\": int(time.time() * 1000) } pipelines: process_orders: from: orders via: - type: filter if: expression: value.get(\"amount\") > 0 - type: mapValues mapper: code: enrich_order(key, value) to: processed_orders Migration Steps Identify Streams and Tables Map each KStream and KTable to KSML stream and table definitions Determine key and value types Extract Processing Logic Convert stateless operations (filter, map, etc.) to KSML operations Extract complex logic into KSML functions Define Pipelines Create KSML pipelines that connect streams and operations Ensure the processing order is maintained Configure Serialization Set up appropriate serialization formats in KSML Configure schema registry if using AVRO Test and Validate Compare the output of both implementations Verify performance characteristics Migrating from Kafka Connect Transformations If you're using Kafka Connect with SMTs (Single Message Transformations), you can migrate some of this logic to KSML for more flexible processing. Example Migration Kafka Connect SMT Configuration: transforms=extractField,filterNull transforms.extractField.type=org.apache.kafka.connect.transforms.ExtractField$Value transforms.extractField.field=payload transforms.filterNull.type=org.apache.kafka.connect.transforms.Filter transforms.filterNull.predicate=isNull transforms.filterNull.negate=true Equivalent KSML: streams: input_stream: topic: connect-input keyType: string valueType: json output_stream: topic: processed-output keyType: string valueType: json pipelines: process_connect_data: from: input_stream via: - type: mapValues mapper: expression: value.get(\"payload\") - type: filter if: expression: value is not None to: output_stream Migration Steps Identify Connect Pipelines Map Kafka Connect source and sink connectors to KSML streams Identify the transformations being applied Convert Transformations to KSML Operations Map each SMT to an equivalent KSML operation Combine multiple SMTs into a single KSML pipeline Enhance with KSML Capabilities Add additional processing that wasn't possible with SMTs Implement stateful processing if needed Test and Validate Verify that the KSML pipeline produces the same output as the Connect pipeline Check for any performance differences Migrating from Apache Flink Migrating from Apache Flink to KSML involves mapping Flink's DataStream operations to KSML operations. Example Migration Apache Flink: // Example Java code (simplified for illustration) StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // Create a stream from a Kafka source DataStream<Order> orders = env .addSource(new FlinkKafkaConsumer<>(\"orders\", new OrderDeserializationSchema(), properties)); // Process the stream DataStream<EnrichedOrder> processedOrders = orders .filter(order -> order.getAmount() > 0) .map(order -> { EnrichedOrder enriched = new EnrichedOrder(); enriched.setOrderId(order.getOrderId()); enriched.setAmount(order.getAmount()); enriched.setProcessedTime(System.currentTimeMillis()); return enriched; }); // Write to Kafka sink processedOrders .addSink(new FlinkKafkaProducer<>(\"processed-orders\", new EnrichedOrderSerializationSchema(), properties)); Equivalent KSML: streams: orders: topic: orders keyType: string valueType: avro:Order processed_orders: topic: processed-orders keyType: string valueType: avro:EnrichedOrder functions: enrich_order: type: mapper code: | return { \"orderId\": value.get(\"orderId\"), \"amount\": value.get(\"amount\"), \"processedTime\": int(time.time() * 1000) } pipelines: process_orders: from: orders via: - type: filter if: expression: value.get(\"amount\") > 0 - type: mapValues mapper: code: enrich_order(key, value) to: processed_orders Migration Steps Identify Stream Sources and Sinks Map Flink sources to KSML stream definitions Map Flink sinks to KSML stream destinations Convert DataStream Operations Map Flink's DataStream operations to KSML operations Extract complex logic into KSML functions Handle State Differently Redesign stateful operations to use Kafka Streams state stores Consider windowing differences between Flink and Kafka Streams Adjust for Processing Guarantees Configure KSML for appropriate processing guarantees Be aware of the differences in exactly-once semantics Test and Validate Compare the output of both implementations Monitor performance differences Best Practices for Migration Incremental Migration Migrate one pipeline or component at a time Run old and new implementations in parallel initially Thorough Testing Create comprehensive test cases before migration Verify that the migrated application produces the same results Performance Monitoring Monitor performance metrics before and after migration Tune configurations as needed Documentation Document the migration process and decisions Update operational procedures for the new implementation Training Provide training for team members on KSML Highlight the differences from the previous framework Related Resources KSML Language Reference Operations Reference Functions Reference Configuration Reference Troubleshooting Guide","title":"KSML Migration Guide"},{"location":"resources/migration/#ksml-migration-guide","text":"This document provides guidance for migrating between different versions of KSML, as well as migrating from other stream processing frameworks to KSML.","title":"KSML Migration Guide"},{"location":"resources/migration/#migrating-between-ksml-versions","text":"","title":"Migrating Between KSML Versions"},{"location":"resources/migration/#migrating-from-1x-to-2x","text":"KSML 2.x introduces several breaking changes and new features compared to KSML 1.x. This section outlines the key differences and provides guidance on how to update your KSML applications.","title":"Migrating from 1.x to 2.x"},{"location":"resources/migration/#key-changes","text":"New Pipeline Syntax KSML 2.x introduces a more concise and expressive pipeline syntax: KSML 1.x: yaml pipelines: my_pipeline: source: stream: input_stream processors: - type: filter condition: expression: value.get(\"amount\") > 0 - type: mapValues mapper: code: transform_value(value) sink: stream: output_stream KSML 2.x: yaml pipelines: my_pipeline: from: input_stream via: - type: filter if: expression: value.get(\"amount\") > 0 - type: mapValues mapper: code: transform_value(value) to: output_stream Function Definitions KSML 2.x requires explicit function types: KSML 1.x: yaml functions: transform_value: code: | return {\"id\": value.get(\"id\"), \"amount\": value.get(\"amount\") * 2} KSML 2.x: yaml functions: transform_value: type: mapper code: | return {\"id\": value.get(\"id\"), \"amount\": value.get(\"amount\") * 2} Stream Definitions KSML 2.x uses a more structured approach for defining streams: KSML 1.x: yaml streams: input_stream: topic: input-topic key-type: string value-type: json KSML 2.x: yaml streams: input_stream: topic: input-topic keyType: string valueType: json Error Handling KSML 2.x introduces improved error handling capabilities: KSML 1.x: yaml # Limited error handling capabilities KSML 2.x: yaml pipelines: my_pipeline: from: input_stream via: - type: try operations: - type: mapValues mapper: code: process_data(value) catch: - type: mapValues mapper: code: | log.error(\"Error processing data: {}\", exception) return {\"error\": str(exception), \"original\": value} to: output_stream","title":"Key Changes"},{"location":"resources/migration/#migration-steps","text":"Update Stream Definitions Replace hyphenated property names with camelCase (e.g., key-type \u2192 keyType ) Review and update data type definitions Update Function Definitions Add explicit function types to all functions Review function parameters and return types Update Pipeline Definitions Replace source and sink with from and to Replace processors with via Update operation syntax (e.g., condition \u2192 if ) Update Error Handling Implement the new error handling capabilities where appropriate Test Thoroughly Run tests to ensure the migrated application behaves as expected Monitor for any performance differences","title":"Migration Steps"},{"location":"resources/migration/#migrating-from-2x-to-3x","text":"KSML 3.x focuses on performance improvements, enhanced state management, and better integration with external systems.","title":"Migrating from 2.x to 3.x"},{"location":"resources/migration/#key-changes_1","text":"State Store Management KSML 3.x introduces a more flexible state store configuration: KSML 2.x: yaml tables: user_profiles: topic: user-profiles keyType: string valueType: avro:UserProfile store: user_profiles_store KSML 3.x: yaml tables: user_profiles: topic: user-profiles keyType: string valueType: avro:UserProfile store: name: user_profiles_store type: persistent config: retention.ms: 604800000 # 7 days Enhanced Schema Management KSML 3.x provides better schema evolution support: KSML 2.x: yaml streams: sensor_data: topic: sensor-readings keyType: string valueType: avro:SensorReading schemaRegistry: http://schema-registry:8081 KSML 3.x: yaml streams: sensor_data: topic: sensor-readings keyType: string valueType: avro:SensorReading schema: registry: http://schema-registry:8081 compatibility: BACKWARD subject: sensor-readings-value Improved Metrics KSML 3.x offers more comprehensive metrics and monitoring: KSML 2.x: yaml config: metrics: reporters: - type: jmx KSML 3.x: yaml config: metrics: reporters: - type: jmx - type: prometheus port: 8080 path: /metrics tags: application: \"ksml-app\" environment: \"${ENV:-dev}\"","title":"Key Changes"},{"location":"resources/migration/#migration-steps_1","text":"Update State Store Configurations Expand state store configurations with the new options Consider retention periods and cleanup policies Update Schema Management Enhance schema configurations with compatibility settings Review subject naming strategies Enhance Metrics Configuration Add additional metrics reporters as needed Configure tags for better metrics organization Test Thoroughly Verify state store behavior Check schema evolution handling Monitor metrics collection","title":"Migration Steps"},{"location":"resources/migration/#migrating-from-other-frameworks-to-ksml","text":"","title":"Migrating from Other Frameworks to KSML"},{"location":"resources/migration/#migrating-from-kafka-streams-java-api","text":"If you're currently using the Kafka Streams Java API directly, migrating to KSML can significantly reduce code complexity while maintaining the same processing capabilities.","title":"Migrating from Kafka Streams Java API"},{"location":"resources/migration/#example-migration","text":"Kafka Streams Java API: // Example Java code (simplified for illustration) StreamsBuilder builder = new StreamsBuilder(); // Define streams KStream<String, Order> orders = builder.stream( \"orders\", Consumed.with(Serdes.String(), orderSerde) ); // Filter and transform KStream<String, EnrichedOrder> processedOrders = orders .filter((key, order) -> order.getAmount() > 0) .mapValues(order -> { EnrichedOrder enriched = new EnrichedOrder(); enriched.setOrderId(order.getOrderId()); enriched.setAmount(order.getAmount()); enriched.setProcessedTime(System.currentTimeMillis()); return enriched; }); // Write to output topic processedOrders.to( \"processed-orders\", Produced.with(Serdes.String(), enrichedOrderSerde) ); Equivalent KSML: streams: orders: topic: orders keyType: string valueType: avro:Order processed_orders: topic: processed-orders keyType: string valueType: avro:EnrichedOrder functions: enrich_order: type: mapper code: | return { \"orderId\": value.get(\"orderId\"), \"amount\": value.get(\"amount\"), \"processedTime\": int(time.time() * 1000) } pipelines: process_orders: from: orders via: - type: filter if: expression: value.get(\"amount\") > 0 - type: mapValues mapper: code: enrich_order(key, value) to: processed_orders","title":"Example Migration"},{"location":"resources/migration/#migration-steps_2","text":"Identify Streams and Tables Map each KStream and KTable to KSML stream and table definitions Determine key and value types Extract Processing Logic Convert stateless operations (filter, map, etc.) to KSML operations Extract complex logic into KSML functions Define Pipelines Create KSML pipelines that connect streams and operations Ensure the processing order is maintained Configure Serialization Set up appropriate serialization formats in KSML Configure schema registry if using AVRO Test and Validate Compare the output of both implementations Verify performance characteristics","title":"Migration Steps"},{"location":"resources/migration/#migrating-from-kafka-connect-transformations","text":"If you're using Kafka Connect with SMTs (Single Message Transformations), you can migrate some of this logic to KSML for more flexible processing.","title":"Migrating from Kafka Connect Transformations"},{"location":"resources/migration/#example-migration_1","text":"Kafka Connect SMT Configuration: transforms=extractField,filterNull transforms.extractField.type=org.apache.kafka.connect.transforms.ExtractField$Value transforms.extractField.field=payload transforms.filterNull.type=org.apache.kafka.connect.transforms.Filter transforms.filterNull.predicate=isNull transforms.filterNull.negate=true Equivalent KSML: streams: input_stream: topic: connect-input keyType: string valueType: json output_stream: topic: processed-output keyType: string valueType: json pipelines: process_connect_data: from: input_stream via: - type: mapValues mapper: expression: value.get(\"payload\") - type: filter if: expression: value is not None to: output_stream","title":"Example Migration"},{"location":"resources/migration/#migration-steps_3","text":"Identify Connect Pipelines Map Kafka Connect source and sink connectors to KSML streams Identify the transformations being applied Convert Transformations to KSML Operations Map each SMT to an equivalent KSML operation Combine multiple SMTs into a single KSML pipeline Enhance with KSML Capabilities Add additional processing that wasn't possible with SMTs Implement stateful processing if needed Test and Validate Verify that the KSML pipeline produces the same output as the Connect pipeline Check for any performance differences","title":"Migration Steps"},{"location":"resources/migration/#migrating-from-apache-flink","text":"Migrating from Apache Flink to KSML involves mapping Flink's DataStream operations to KSML operations.","title":"Migrating from Apache Flink"},{"location":"resources/migration/#example-migration_2","text":"Apache Flink: // Example Java code (simplified for illustration) StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // Create a stream from a Kafka source DataStream<Order> orders = env .addSource(new FlinkKafkaConsumer<>(\"orders\", new OrderDeserializationSchema(), properties)); // Process the stream DataStream<EnrichedOrder> processedOrders = orders .filter(order -> order.getAmount() > 0) .map(order -> { EnrichedOrder enriched = new EnrichedOrder(); enriched.setOrderId(order.getOrderId()); enriched.setAmount(order.getAmount()); enriched.setProcessedTime(System.currentTimeMillis()); return enriched; }); // Write to Kafka sink processedOrders .addSink(new FlinkKafkaProducer<>(\"processed-orders\", new EnrichedOrderSerializationSchema(), properties)); Equivalent KSML: streams: orders: topic: orders keyType: string valueType: avro:Order processed_orders: topic: processed-orders keyType: string valueType: avro:EnrichedOrder functions: enrich_order: type: mapper code: | return { \"orderId\": value.get(\"orderId\"), \"amount\": value.get(\"amount\"), \"processedTime\": int(time.time() * 1000) } pipelines: process_orders: from: orders via: - type: filter if: expression: value.get(\"amount\") > 0 - type: mapValues mapper: code: enrich_order(key, value) to: processed_orders","title":"Example Migration"},{"location":"resources/migration/#migration-steps_4","text":"Identify Stream Sources and Sinks Map Flink sources to KSML stream definitions Map Flink sinks to KSML stream destinations Convert DataStream Operations Map Flink's DataStream operations to KSML operations Extract complex logic into KSML functions Handle State Differently Redesign stateful operations to use Kafka Streams state stores Consider windowing differences between Flink and Kafka Streams Adjust for Processing Guarantees Configure KSML for appropriate processing guarantees Be aware of the differences in exactly-once semantics Test and Validate Compare the output of both implementations Monitor performance differences","title":"Migration Steps"},{"location":"resources/migration/#best-practices-for-migration","text":"Incremental Migration Migrate one pipeline or component at a time Run old and new implementations in parallel initially Thorough Testing Create comprehensive test cases before migration Verify that the migrated application produces the same results Performance Monitoring Monitor performance metrics before and after migration Tune configurations as needed Documentation Document the migration process and decisions Update operational procedures for the new implementation Training Provide training for team members on KSML Highlight the differences from the previous framework","title":"Best Practices for Migration"},{"location":"resources/migration/#related-resources","text":"KSML Language Reference Operations Reference Functions Reference Configuration Reference Troubleshooting Guide","title":"Related Resources"},{"location":"resources/troubleshooting/","text":"KSML Troubleshooting Guide This guide provides solutions for common issues you might encounter when working with KSML. It covers problems related to development, deployment, and runtime behavior. Common Error Messages Syntax Errors YAML Parsing Errors Error : Error parsing YAML: mapping values are not allowed here Solution : Check your YAML indentation. YAML is sensitive to spaces and tabs. Make sure you're using consistent indentation (preferably spaces) throughout your KSML file. # Incorrect streams: my_stream: topic: my-topic # Wrong indentation (topic should be indented under my_stream) # Correct streams: my_stream: topic: my-topic # Proper indentation (topic is properly indented under my_stream) Error : Error parsing YAML: found character that cannot start any token Solution : Look for special characters that might be causing issues. Common culprits include: - Unescaped quotes within strings - Special characters in keys or values - Tab characters instead of spaces Function Definition Errors Error : Function 'process_data' is missing required parameter 'type' Solution : Ensure all functions have the required type parameter: functions: process_data: # Missing type parameter parameters: - name: value type: object code: | return value Correct version: functions: process_data: type: mapValues # Added type parameter parameters: - name: value type: object code: | return value Pipeline Definition Errors Error : Pipeline 'my_pipeline' is missing required parameter 'from' Solution : Ensure all pipelines have the required from and to parameters: pipelines: my_pipeline: # Missing 'from' parameter via: - type: mapValues mapper: code: transform_data(value) to: output_stream Correct version: pipelines: my_pipeline: from: input_stream # Added 'from' parameter via: - type: mapValues mapper: code: transform_data(value) to: output_stream Runtime Errors Serialization/Deserialization Errors Error : Failed to deserialize key/value for topic 'my-topic' Solution : 1. Check that the keyType and valueType in your stream definition match the actual data format in the Kafka topic 2. Verify that your data conforms to the expected schema 3. For Avro or other schema-based formats, ensure the schema is correctly registered and accessible streams: my_stream: topic: my-topic keyType: string valueType: json # Make sure this matches the actual data format Python Code Errors Error : AttributeError: 'NoneType' object has no attribute 'get' Solution : Add null checks before accessing object properties: # Incorrect result = value.get(\"field\").get(\"nested_field\") # Correct result = None if value is not None: field_value = value.get(\"field\") if field_value is not None: result = field_value.get(\"nested_field\") Error : NameError: name 'some_variable' is not defined Solution : Ensure all variables are defined before use. Common issues include: - Typos in variable names - Using variables from outside the function scope - Forgetting to import required modules Join Operation Errors Error : Stream 'reference_data' not found for join operation Solution : Verify that all streams referenced in join operations are defined: streams: input_stream: topic: input-topic keyType: string valueType: json # Missing reference_data stream definition pipelines: join_pipeline: from: input_stream via: - type: join with: reference_data # This stream is not defined to: output_stream Error : Join operation requires keys of the same type Solution : Ensure that the key types of streams being joined are compatible: streams: orders: topic: orders keyType: string valueType: json products: topic: products keyType: long # Different key type valueType: json Correct version: streams: orders: topic: orders keyType: string valueType: json products: topic: products keyType: string # Same key type as orders valueType: json Deployment Issues Configuration Problems Error : Failed to start KSML application: Could not connect to Kafka broker Solution : 1. Verify that the Kafka brokers are running and accessible 2. Check the bootstrap.servers configuration 3. Ensure network connectivity between the KSML application and Kafka brokers 4. Check firewall rules that might be blocking connections configuration: bootstrap.servers: kafka:9092 # Make sure this is correct Error : Topic 'my-topic' not found Solution : 1. Verify that the topic exists in your Kafka cluster 2. Check if you have the correct permissions to access the topic 3. Ensure the topic name is spelled correctly in your KSML definition Resource Constraints Error : OutOfMemoryError: Java heap space Solution : 1. Increase the JVM heap size for the KSML runner 2. Review your stateful operations (joins, aggregations) for potential memory leaks 3. Consider using smaller time windows for windowed operations 4. Implement proper cleanup for state stores when they're no longer needed # Example of increasing heap size java -Xmx2g -jar ksml-runner.jar Performance Issues High Latency Symptom : Messages take a long time to process through the pipeline Solutions : 1. Optimize Python Code : Review your Python functions for inefficiencies 2. Reduce State Size : Minimize the amount of data stored in stateful operations 3. Adjust Parallelism : Increase the number of partitions for input topics 4. Monitor Resource Usage : Check CPU, memory, and network utilization 5. Simplify Complex Operations : Break down complex operations into simpler ones Low Throughput Symptom : Pipeline processes fewer messages per second than expected Solutions : 1. Batch Processing : Use batch processing where appropriate 2. Optimize Serialization : Choose efficient serialization formats (e.g., Avro instead of JSON) 3. Reduce External Calls : Minimize calls to external systems 4. Caching : Implement caching for frequently accessed data 5. Horizontal Scaling : Run multiple instances of your KSML application Debugging Techniques Logging Add logging statements to your KSML functions to track execution: - type: peek forEach: code: | log.info(\"Processing record: key={}, value={}\", key, value) For more detailed logging, you can include specific fields: - type: peek forEach: code: | log.info(\"Order details: id={}, amount={}, customer={}\", value.get(\"order_id\"), value.get(\"amount\"), value.get(\"customer_id\")) Dead Letter Queues Implement dead letter queues to capture and analyze failed records: streams: input: topic: input-topic keyType: string valueType: json output: topic: output-topic keyType: string valueType: json errors: topic: error-topic keyType: string valueType: json pipelines: main_pipeline: from: input via: - type: mapValues mapper: code: process_data(value) onError: sendTo: errors withKey: key withValue: {\"original\": value, \"error\": exception.getMessage()} to: output Testing with Sample Data Create test pipelines that process a small sample of data: streams: test_input: topic: test-input keyType: string valueType: json test_output: topic: test-output keyType: string valueType: json pipelines: test_pipeline: from: test_input via: - type: peek forEach: code: | log.info(\"Input: {}\", value) - type: mapValues mapper: code: process_data(value) - type: peek forEach: code: | log.info(\"Output: {}\", value) to: test_output Common Patterns and Anti-Patterns Recommended Patterns Proper Error Handling Always implement error handling to prevent pipeline failures: - type: try operations: - type: mapValues mapper: code: process_data(value) catch: - type: mapValues mapper: code: | log.error(\"Failed to process data: {}\", exception) return {\"error\": str(exception), \"original\": value} Defensive Programming Always check for null values and handle edge cases: def process_data(value): if value is None: return {\"error\": \"Null input\"} user_id = value.get(\"user_id\") if user_id is None: return {\"error\": \"Missing user_id\"} # Process valid data return {\"user_id\": user_id, \"processed\": True} Modular Functions Break down complex logic into smaller, reusable functions: functions: validate_input: type: mapValues parameters: - name: value type: object code: | # Validation logic transform_data: type: mapValues parameters: - name: value type: object code: | # Transformation logic enrich_data: type: mapValues parameters: - name: value type: object - name: reference type: object code: | # Enrichment logic pipelines: main_pipeline: from: input via: - type: mapValues mapper: code: validate_input(value) - type: mapValues mapper: code: transform_data(value) - type: join with: reference_data - type: mapValues mapper: code: enrich_data(value, foreignValue) to: output Anti-Patterns to Avoid Excessive State Avoid storing large amounts of data in state stores: # Anti-pattern: Storing entire message history - type: aggregate initializer: expression: [] aggregator: code: | if aggregate is None: return [value] else: return aggregate + [value] # This will grow unbounded! Better approach: # Better: Store only what you need - type: aggregate initializer: expression: {\"count\": 0, \"sum\": 0} aggregator: code: | if aggregate is None: return {\"count\": 1, \"sum\": value.get(\"amount\", 0)} else: return { \"count\": aggregate.get(\"count\", 0) + 1, \"sum\": aggregate.get(\"sum\", 0) + value.get(\"amount\", 0) } Complex Single Functions Avoid putting too much logic in a single function: # Anti-pattern: Monolithic function functions: do_everything: type: mapValues parameters: - name: value type: object code: | # 100+ lines of code that does validation, transformation, # business logic, error handling, etc. Ignoring Errors Don't ignore exceptions without proper handling: # Anti-pattern: Swallowing exceptions - type: mapValues mapper: code: | try: # Complex processing return process_result except: # Silently return empty result return {} Getting Help If you're still experiencing issues after trying the solutions in this guide: Check the Documentation : Review the KSML Language Reference and Operations Reference Search the Community Forums : Look for similar issues in the community forums Examine Logs : Check the KSML runner logs for detailed error information Create a Minimal Example : Create a simplified version of your pipeline that reproduces the issue Contact Support : Reach out to the KSML support team with: A clear description of the issue Steps to reproduce Relevant logs and error messages Your KSML definition file (with sensitive information removed) Conclusion Troubleshooting KSML applications involves understanding both the KSML language itself and the underlying Kafka Streams concepts. By following the patterns and solutions in this guide, you can resolve common issues and build more robust stream processing applications. Remember that proper logging, error handling, and testing are key to identifying and resolving issues quickly. When in doubt, start with a simplified version of your pipeline and gradually add complexity while testing at each step.","title":"KSML Troubleshooting Guide"},{"location":"resources/troubleshooting/#ksml-troubleshooting-guide","text":"This guide provides solutions for common issues you might encounter when working with KSML. It covers problems related to development, deployment, and runtime behavior.","title":"KSML Troubleshooting Guide"},{"location":"resources/troubleshooting/#common-error-messages","text":"","title":"Common Error Messages"},{"location":"resources/troubleshooting/#syntax-errors","text":"","title":"Syntax Errors"},{"location":"resources/troubleshooting/#yaml-parsing-errors","text":"Error : Error parsing YAML: mapping values are not allowed here Solution : Check your YAML indentation. YAML is sensitive to spaces and tabs. Make sure you're using consistent indentation (preferably spaces) throughout your KSML file. # Incorrect streams: my_stream: topic: my-topic # Wrong indentation (topic should be indented under my_stream) # Correct streams: my_stream: topic: my-topic # Proper indentation (topic is properly indented under my_stream) Error : Error parsing YAML: found character that cannot start any token Solution : Look for special characters that might be causing issues. Common culprits include: - Unescaped quotes within strings - Special characters in keys or values - Tab characters instead of spaces","title":"YAML Parsing Errors"},{"location":"resources/troubleshooting/#function-definition-errors","text":"Error : Function 'process_data' is missing required parameter 'type' Solution : Ensure all functions have the required type parameter: functions: process_data: # Missing type parameter parameters: - name: value type: object code: | return value Correct version: functions: process_data: type: mapValues # Added type parameter parameters: - name: value type: object code: | return value","title":"Function Definition Errors"},{"location":"resources/troubleshooting/#pipeline-definition-errors","text":"Error : Pipeline 'my_pipeline' is missing required parameter 'from' Solution : Ensure all pipelines have the required from and to parameters: pipelines: my_pipeline: # Missing 'from' parameter via: - type: mapValues mapper: code: transform_data(value) to: output_stream Correct version: pipelines: my_pipeline: from: input_stream # Added 'from' parameter via: - type: mapValues mapper: code: transform_data(value) to: output_stream","title":"Pipeline Definition Errors"},{"location":"resources/troubleshooting/#runtime-errors","text":"","title":"Runtime Errors"},{"location":"resources/troubleshooting/#serializationdeserialization-errors","text":"Error : Failed to deserialize key/value for topic 'my-topic' Solution : 1. Check that the keyType and valueType in your stream definition match the actual data format in the Kafka topic 2. Verify that your data conforms to the expected schema 3. For Avro or other schema-based formats, ensure the schema is correctly registered and accessible streams: my_stream: topic: my-topic keyType: string valueType: json # Make sure this matches the actual data format","title":"Serialization/Deserialization Errors"},{"location":"resources/troubleshooting/#python-code-errors","text":"Error : AttributeError: 'NoneType' object has no attribute 'get' Solution : Add null checks before accessing object properties: # Incorrect result = value.get(\"field\").get(\"nested_field\") # Correct result = None if value is not None: field_value = value.get(\"field\") if field_value is not None: result = field_value.get(\"nested_field\") Error : NameError: name 'some_variable' is not defined Solution : Ensure all variables are defined before use. Common issues include: - Typos in variable names - Using variables from outside the function scope - Forgetting to import required modules","title":"Python Code Errors"},{"location":"resources/troubleshooting/#join-operation-errors","text":"Error : Stream 'reference_data' not found for join operation Solution : Verify that all streams referenced in join operations are defined: streams: input_stream: topic: input-topic keyType: string valueType: json # Missing reference_data stream definition pipelines: join_pipeline: from: input_stream via: - type: join with: reference_data # This stream is not defined to: output_stream Error : Join operation requires keys of the same type Solution : Ensure that the key types of streams being joined are compatible: streams: orders: topic: orders keyType: string valueType: json products: topic: products keyType: long # Different key type valueType: json Correct version: streams: orders: topic: orders keyType: string valueType: json products: topic: products keyType: string # Same key type as orders valueType: json","title":"Join Operation Errors"},{"location":"resources/troubleshooting/#deployment-issues","text":"","title":"Deployment Issues"},{"location":"resources/troubleshooting/#configuration-problems","text":"Error : Failed to start KSML application: Could not connect to Kafka broker Solution : 1. Verify that the Kafka brokers are running and accessible 2. Check the bootstrap.servers configuration 3. Ensure network connectivity between the KSML application and Kafka brokers 4. Check firewall rules that might be blocking connections configuration: bootstrap.servers: kafka:9092 # Make sure this is correct Error : Topic 'my-topic' not found Solution : 1. Verify that the topic exists in your Kafka cluster 2. Check if you have the correct permissions to access the topic 3. Ensure the topic name is spelled correctly in your KSML definition","title":"Configuration Problems"},{"location":"resources/troubleshooting/#resource-constraints","text":"Error : OutOfMemoryError: Java heap space Solution : 1. Increase the JVM heap size for the KSML runner 2. Review your stateful operations (joins, aggregations) for potential memory leaks 3. Consider using smaller time windows for windowed operations 4. Implement proper cleanup for state stores when they're no longer needed # Example of increasing heap size java -Xmx2g -jar ksml-runner.jar","title":"Resource Constraints"},{"location":"resources/troubleshooting/#performance-issues","text":"","title":"Performance Issues"},{"location":"resources/troubleshooting/#high-latency","text":"Symptom : Messages take a long time to process through the pipeline Solutions : 1. Optimize Python Code : Review your Python functions for inefficiencies 2. Reduce State Size : Minimize the amount of data stored in stateful operations 3. Adjust Parallelism : Increase the number of partitions for input topics 4. Monitor Resource Usage : Check CPU, memory, and network utilization 5. Simplify Complex Operations : Break down complex operations into simpler ones","title":"High Latency"},{"location":"resources/troubleshooting/#low-throughput","text":"Symptom : Pipeline processes fewer messages per second than expected Solutions : 1. Batch Processing : Use batch processing where appropriate 2. Optimize Serialization : Choose efficient serialization formats (e.g., Avro instead of JSON) 3. Reduce External Calls : Minimize calls to external systems 4. Caching : Implement caching for frequently accessed data 5. Horizontal Scaling : Run multiple instances of your KSML application","title":"Low Throughput"},{"location":"resources/troubleshooting/#debugging-techniques","text":"","title":"Debugging Techniques"},{"location":"resources/troubleshooting/#logging","text":"Add logging statements to your KSML functions to track execution: - type: peek forEach: code: | log.info(\"Processing record: key={}, value={}\", key, value) For more detailed logging, you can include specific fields: - type: peek forEach: code: | log.info(\"Order details: id={}, amount={}, customer={}\", value.get(\"order_id\"), value.get(\"amount\"), value.get(\"customer_id\"))","title":"Logging"},{"location":"resources/troubleshooting/#dead-letter-queues","text":"Implement dead letter queues to capture and analyze failed records: streams: input: topic: input-topic keyType: string valueType: json output: topic: output-topic keyType: string valueType: json errors: topic: error-topic keyType: string valueType: json pipelines: main_pipeline: from: input via: - type: mapValues mapper: code: process_data(value) onError: sendTo: errors withKey: key withValue: {\"original\": value, \"error\": exception.getMessage()} to: output","title":"Dead Letter Queues"},{"location":"resources/troubleshooting/#testing-with-sample-data","text":"Create test pipelines that process a small sample of data: streams: test_input: topic: test-input keyType: string valueType: json test_output: topic: test-output keyType: string valueType: json pipelines: test_pipeline: from: test_input via: - type: peek forEach: code: | log.info(\"Input: {}\", value) - type: mapValues mapper: code: process_data(value) - type: peek forEach: code: | log.info(\"Output: {}\", value) to: test_output","title":"Testing with Sample Data"},{"location":"resources/troubleshooting/#common-patterns-and-anti-patterns","text":"","title":"Common Patterns and Anti-Patterns"},{"location":"resources/troubleshooting/#recommended-patterns","text":"","title":"Recommended Patterns"},{"location":"resources/troubleshooting/#proper-error-handling","text":"Always implement error handling to prevent pipeline failures: - type: try operations: - type: mapValues mapper: code: process_data(value) catch: - type: mapValues mapper: code: | log.error(\"Failed to process data: {}\", exception) return {\"error\": str(exception), \"original\": value}","title":"Proper Error Handling"},{"location":"resources/troubleshooting/#defensive-programming","text":"Always check for null values and handle edge cases: def process_data(value): if value is None: return {\"error\": \"Null input\"} user_id = value.get(\"user_id\") if user_id is None: return {\"error\": \"Missing user_id\"} # Process valid data return {\"user_id\": user_id, \"processed\": True}","title":"Defensive Programming"},{"location":"resources/troubleshooting/#modular-functions","text":"Break down complex logic into smaller, reusable functions: functions: validate_input: type: mapValues parameters: - name: value type: object code: | # Validation logic transform_data: type: mapValues parameters: - name: value type: object code: | # Transformation logic enrich_data: type: mapValues parameters: - name: value type: object - name: reference type: object code: | # Enrichment logic pipelines: main_pipeline: from: input via: - type: mapValues mapper: code: validate_input(value) - type: mapValues mapper: code: transform_data(value) - type: join with: reference_data - type: mapValues mapper: code: enrich_data(value, foreignValue) to: output","title":"Modular Functions"},{"location":"resources/troubleshooting/#anti-patterns-to-avoid","text":"","title":"Anti-Patterns to Avoid"},{"location":"resources/troubleshooting/#excessive-state","text":"Avoid storing large amounts of data in state stores: # Anti-pattern: Storing entire message history - type: aggregate initializer: expression: [] aggregator: code: | if aggregate is None: return [value] else: return aggregate + [value] # This will grow unbounded! Better approach: # Better: Store only what you need - type: aggregate initializer: expression: {\"count\": 0, \"sum\": 0} aggregator: code: | if aggregate is None: return {\"count\": 1, \"sum\": value.get(\"amount\", 0)} else: return { \"count\": aggregate.get(\"count\", 0) + 1, \"sum\": aggregate.get(\"sum\", 0) + value.get(\"amount\", 0) }","title":"Excessive State"},{"location":"resources/troubleshooting/#complex-single-functions","text":"Avoid putting too much logic in a single function: # Anti-pattern: Monolithic function functions: do_everything: type: mapValues parameters: - name: value type: object code: | # 100+ lines of code that does validation, transformation, # business logic, error handling, etc.","title":"Complex Single Functions"},{"location":"resources/troubleshooting/#ignoring-errors","text":"Don't ignore exceptions without proper handling: # Anti-pattern: Swallowing exceptions - type: mapValues mapper: code: | try: # Complex processing return process_result except: # Silently return empty result return {}","title":"Ignoring Errors"},{"location":"resources/troubleshooting/#getting-help","text":"If you're still experiencing issues after trying the solutions in this guide: Check the Documentation : Review the KSML Language Reference and Operations Reference Search the Community Forums : Look for similar issues in the community forums Examine Logs : Check the KSML runner logs for detailed error information Create a Minimal Example : Create a simplified version of your pipeline that reproduces the issue Contact Support : Reach out to the KSML support team with: A clear description of the issue Steps to reproduce Relevant logs and error messages Your KSML definition file (with sensitive information removed)","title":"Getting Help"},{"location":"resources/troubleshooting/#conclusion","text":"Troubleshooting KSML applications involves understanding both the KSML language itself and the underlying Kafka Streams concepts. By following the patterns and solutions in this guide, you can resolve common issues and build more robust stream processing applications. Remember that proper logging, error handling, and testing are key to identifying and resolving issues quickly. When in doubt, start with a simplified version of your pipeline and gradually add complexity while testing at each step.","title":"Conclusion"},{"location":"tutorials/","text":"Tutorials Welcome to the KSML tutorials and guides section. This section provides a progressive learning path from beginner to advanced topics, as well as practical use case guides. Learning Paths Getting Started This tutorial will help you understand what KSML is, how to set it up, and how to create your first KSML application. Beginner Tutorials When you've installed KSML and are ready to develop, these tutorials cover the basics of building simple data pipelines, filtering and transforming data, and implementing logging and monitoring. Intermediate Tutorials Once you're comfortable with the basics, move on to these tutorials to learn about aggregations, joins, windowed operations, and error handling. Advanced Tutorials These tutorials cover complex topics such as complex event processing, custom state stores, performance optimization, and external integration. How to Use These Tutorials Each tutorial builds on concepts from previous tutorials, so we recommend following them in order within each learning path. Each tutorial includes: Objectives : What you'll learn Prerequisites : What you need to know before starting Step-by-step instructions : Detailed walkthrough with explanations Complete examples : Working code you can run and modify Next steps : Suggestions for what to learn next Getting Help If you encounter any issues while working through these tutorials, check the Troubleshooting Guide or reach out to the KSML Community for assistance.","title":"Tutorials"},{"location":"tutorials/#tutorials","text":"Welcome to the KSML tutorials and guides section. This section provides a progressive learning path from beginner to advanced topics, as well as practical use case guides.","title":"Tutorials"},{"location":"tutorials/#learning-paths","text":"","title":"Learning Paths"},{"location":"tutorials/#getting-started","text":"This tutorial will help you understand what KSML is, how to set it up, and how to create your first KSML application.","title":"Getting Started"},{"location":"tutorials/#beginner-tutorials","text":"When you've installed KSML and are ready to develop, these tutorials cover the basics of building simple data pipelines, filtering and transforming data, and implementing logging and monitoring.","title":"Beginner Tutorials"},{"location":"tutorials/#intermediate-tutorials","text":"Once you're comfortable with the basics, move on to these tutorials to learn about aggregations, joins, windowed operations, and error handling.","title":"Intermediate Tutorials"},{"location":"tutorials/#advanced-tutorials","text":"These tutorials cover complex topics such as complex event processing, custom state stores, performance optimization, and external integration.","title":"Advanced Tutorials"},{"location":"tutorials/#how-to-use-these-tutorials","text":"Each tutorial builds on concepts from previous tutorials, so we recommend following them in order within each learning path. Each tutorial includes: Objectives : What you'll learn Prerequisites : What you need to know before starting Step-by-step instructions : Detailed walkthrough with explanations Complete examples : Working code you can run and modify Next steps : Suggestions for what to learn next","title":"How to Use These Tutorials"},{"location":"tutorials/#getting-help","text":"If you encounter any issues while working through these tutorials, check the Troubleshooting Guide or reach out to the KSML Community for assistance.","title":"Getting Help"},{"location":"tutorials/advanced/","text":"Advanced Tutorials Welcome to the KSML advanced tutorials! These tutorials are designed for users who have mastered the beginner and intermediate concepts and are ready to explore the most powerful and complex features of KSML. These tutorials will help you build sophisticated, high-performance data processing applications and integrate KSML with other systems and technologies. Available Tutorials Complex Event Processing Learn how to implement complex event processing patterns with KSML: Pattern detection across multiple streams Temporal pattern matching Event correlation and enrichment Building event-driven applications Custom State Stores This tutorial covers advanced state management techniques: Implementing custom state stores Optimizing state access patterns Managing state store size and performance Implementing custom serialization/deserialization Performance Optimization Learn how to optimize your KSML applications for maximum performance: Identifying bottlenecks Optimizing serialization/deserialization Tuning Kafka Streams configurations Scaling strategies Monitoring performance metrics Integration with External Systems This tutorial focuses on integrating KSML with other systems: Connecting to databases Integrating with REST APIs Working with message queues Implementing the Kafka Connect pattern in KSML Advanced Error Handling Learn sophisticated error handling techniques: Circuit breaker patterns Retry strategies Error recovery workflows Handling partial failures in distributed systems Learning Path These advanced tutorials can be approached based on your specific needs and interests. Each tutorial is self-contained but may reference concepts from other advanced or intermediate tutorials. After completing these tutorials, you'll have a comprehensive understanding of KSML's capabilities and be able to implement complex, production-grade stream processing applications. Additional Resources Core Concepts: Advanced Topics - Detailed explanations of advanced KSML concepts Reference: Configuration Options - Complete reference for tuning and configuring KSML Examples Library: Advanced Examples - Ready-to-use examples for advanced patterns Troubleshooting Guide - Solutions for common issues in complex applications","title":"Advanced Tutorials"},{"location":"tutorials/advanced/#advanced-tutorials","text":"Welcome to the KSML advanced tutorials! These tutorials are designed for users who have mastered the beginner and intermediate concepts and are ready to explore the most powerful and complex features of KSML. These tutorials will help you build sophisticated, high-performance data processing applications and integrate KSML with other systems and technologies.","title":"Advanced Tutorials"},{"location":"tutorials/advanced/#available-tutorials","text":"","title":"Available Tutorials"},{"location":"tutorials/advanced/#complex-event-processing","text":"Learn how to implement complex event processing patterns with KSML: Pattern detection across multiple streams Temporal pattern matching Event correlation and enrichment Building event-driven applications","title":"Complex Event Processing"},{"location":"tutorials/advanced/#custom-state-stores","text":"This tutorial covers advanced state management techniques: Implementing custom state stores Optimizing state access patterns Managing state store size and performance Implementing custom serialization/deserialization","title":"Custom State Stores"},{"location":"tutorials/advanced/#performance-optimization","text":"Learn how to optimize your KSML applications for maximum performance: Identifying bottlenecks Optimizing serialization/deserialization Tuning Kafka Streams configurations Scaling strategies Monitoring performance metrics","title":"Performance Optimization"},{"location":"tutorials/advanced/#integration-with-external-systems","text":"This tutorial focuses on integrating KSML with other systems: Connecting to databases Integrating with REST APIs Working with message queues Implementing the Kafka Connect pattern in KSML","title":"Integration with External Systems"},{"location":"tutorials/advanced/#advanced-error-handling","text":"Learn sophisticated error handling techniques: Circuit breaker patterns Retry strategies Error recovery workflows Handling partial failures in distributed systems","title":"Advanced Error Handling"},{"location":"tutorials/advanced/#learning-path","text":"These advanced tutorials can be approached based on your specific needs and interests. Each tutorial is self-contained but may reference concepts from other advanced or intermediate tutorials. After completing these tutorials, you'll have a comprehensive understanding of KSML's capabilities and be able to implement complex, production-grade stream processing applications.","title":"Learning Path"},{"location":"tutorials/advanced/#additional-resources","text":"Core Concepts: Advanced Topics - Detailed explanations of advanced KSML concepts Reference: Configuration Options - Complete reference for tuning and configuring KSML Examples Library: Advanced Examples - Ready-to-use examples for advanced patterns Troubleshooting Guide - Solutions for common issues in complex applications","title":"Additional Resources"},{"location":"tutorials/advanced/complex-event-processing/","text":"Complex Event Processing in KSML This tutorial explores how to implement complex event processing (CEP) patterns in KSML, allowing you to detect meaningful patterns across multiple events and streams in real-time. Introduction to Complex Event Processing Complex Event Processing (CEP) is a method of tracking and analyzing streams of data about things that happen (events), and deriving conclusions from them. In streaming contexts, CEP allows you to: Detect patterns across multiple events Identify sequences of events that occur over time Correlate events from different sources Derive higher-level insights from lower-level events KSML provides powerful capabilities for implementing CEP patterns through its combination of stateful processing, windowing operations, and Python functions. Prerequisites Before starting this tutorial, you should: Understand intermediate KSML concepts (streams, functions, pipelines) Have completed the Windowed Operations tutorial Be familiar with Joins and Aggregations Have a basic understanding of state management in stream processing Key CEP Patterns in KSML 1. Pattern Detection Pattern detection involves identifying specific sequences or combinations of events within a stream: functions: detect_pattern: type: valueTransformer code: | # Check if this event completes a pattern if value.get(\"event_type\") == \"C\" and state_store.get(key + \"_has_A\") and state_store.get(key + \"_has_B\"): # Pattern A -> B -> C detected result = { \"pattern_detected\": \"A_B_C\", \"completion_time\": value.get(\"timestamp\"), \"key\": key } # Reset pattern tracking state_store.delete(key + \"_has_A\") state_store.delete(key + \"_has_B\") return result # Track events that are part of the pattern if value.get(\"event_type\") == \"A\": state_store.put(key + \"_has_A\", True) elif value.get(\"event_type\") == \"B\": state_store.put(key + \"_has_B\", True) # Don't emit anything for partial patterns return None stores: - pattern_state_store pipelines: detect_abc_pattern: from: input_events mapValues: detect_pattern filter: is_not_null # Only pass through completed patterns to: detected_patterns 2. Temporal Pattern Matching Temporal pattern matching adds time constraints to pattern detection: functions: detect_temporal_pattern: type: valueTransformer code: | current_time = value.get(\"timestamp\", int(time.time() * 1000)) if value.get(\"event_type\") == \"A\": # Start tracking a new potential pattern state_store.put(key + \"_pattern_start\", current_time) state_store.put(key + \"_has_A\", True) return None if value.get(\"event_type\") == \"B\" and state_store.get(key + \"_has_A\"): # Check if B occurred within 5 minutes of A pattern_start = state_store.get(key + \"_pattern_start\") if pattern_start and (current_time - pattern_start) <= 5 * 60 * 1000: state_store.put(key + \"_has_B\", True) state_store.put(key + \"_B_time\", current_time) return None if value.get(\"event_type\") == \"C\" and state_store.get(key + \"_has_B\"): # Check if C occurred within 2 minutes of B b_time = state_store.get(key + \"_B_time\") if b_time and (current_time - b_time) <= 2 * 60 * 1000: # Pattern A -> B -> C detected within time constraints pattern_start = state_store.get(key + \"_pattern_start\") result = { \"pattern_detected\": \"A_B_C\", \"start_time\": pattern_start, \"end_time\": current_time, \"duration_ms\": current_time - pattern_start, \"key\": key } # Reset pattern tracking state_store.delete(key + \"_has_A\") state_store.delete(key + \"_has_B\") state_store.delete(key + \"_pattern_start\") state_store.delete(key + \"_B_time\") return result return None stores: - temporal_pattern_store 3. Event Correlation and Enrichment Event correlation involves combining related events from different streams: streams: user_logins: topic: user_login_events keyType: string # User ID valueType: json # Login details user_actions: topic: user_action_events keyType: string # User ID valueType: json # Action details user_logouts: topic: user_logout_events keyType: string # User ID valueType: json # Logout details user_sessions: topic: user_session_events keyType: string # User ID valueType: json # Complete session information functions: correlate_session_events: type: valueTransformer code: | event_type = value.get(\"event_type\") # Get current session state session = state_store.get(key + \"_session\") if session is None: session = {\"events\": []} # Add this event to the session event_copy = value.copy() event_copy[\"processed_time\"] = int(time.time() * 1000) session[\"events\"].append(event_copy) # Update session based on event type if event_type == \"login\": session[\"login_time\"] = value.get(\"timestamp\") session[\"device\"] = value.get(\"device\") session[\"ip_address\"] = value.get(\"ip_address\") session[\"status\"] = \"active\" elif event_type == \"action\": session[\"last_activity_time\"] = value.get(\"timestamp\") session[\"last_action\"] = value.get(\"action_type\") elif event_type == \"logout\": session[\"logout_time\"] = value.get(\"timestamp\") session[\"status\"] = \"completed\" session[\"duration_ms\"] = session[\"logout_time\"] - session.get(\"login_time\", 0) # Return the complete session and clear state result = session.copy() state_store.delete(key + \"_session\") return result # Update session state and don't emit for incomplete sessions state_store.put(key + \"_session\", session) return None stores: - session_state_store pipelines: process_login_events: from: user_logins mapValues: correlate_session_events filter: is_not_null to: user_sessions process_action_events: from: user_actions mapValues: correlate_session_events filter: is_not_null to: user_sessions process_logout_events: from: user_logouts mapValues: correlate_session_events filter: is_not_null to: user_sessions 4. Anomaly Detection Anomaly detection identifies unusual patterns or deviations from normal behavior: functions: detect_anomalies: type: valueTransformer code: | # Get historical values for this key history = state_store.get(key + \"_history\") if history is None: history = {\"values\": [], \"sum\": 0, \"count\": 0} # Extract the value to monitor metric_value = value.get(\"metric_value\", 0) # Update history history[\"values\"].append(metric_value) history[\"sum\"] += metric_value history[\"count\"] += 1 # Keep only the last 10 values if len(history[\"values\"]) > 10: removed_value = history[\"values\"].pop(0) history[\"sum\"] -= removed_value history[\"count\"] -= 1 # Calculate statistics avg = history[\"sum\"] / history[\"count\"] if history[\"count\"] > 0 else 0 # Calculate standard deviation variance_sum = sum((x - avg) ** 2 for x in history[\"values\"]) std_dev = (variance_sum / history[\"count\"]) ** 0.5 if history[\"count\"] > 0 else 0 # Check for anomaly (value more than 3 standard deviations from mean) is_anomaly = False if std_dev > 0 and abs(metric_value - avg) > 3 * std_dev: is_anomaly = True # Update state state_store.put(key + \"_history\", history) # Return anomaly information if detected if is_anomaly: return { \"key\": key, \"timestamp\": value.get(\"timestamp\"), \"metric_value\": metric_value, \"average\": avg, \"std_dev\": std_dev, \"deviation\": abs(metric_value - avg) / std_dev if std_dev > 0 else 0, \"original_event\": value } return None stores: - anomaly_detection_store Practical Example: Fraud Detection System Let's build a complete example that implements a real-time fraud detection system using CEP patterns: streams: credit_card_transactions: topic: cc_transactions keyType: string # Card number valueType: json # Transaction details location_changes: topic: location_events keyType: string # User ID valueType: json # Location information authentication_events: topic: auth_events keyType: string # User ID valueType: json # Authentication details fraud_alerts: topic: fraud_alerts keyType: string # Alert ID valueType: json # Alert details stores: transaction_history_store: type: keyValue keyType: string valueType: json persistent: true user_profile_store: type: keyValue keyType: string valueType: json persistent: true alert_state_store: type: keyValue keyType: string valueType: json persistent: false functions: detect_transaction_anomalies: type: keyValueTransformer code: | card_number = key transaction = value current_time = transaction.get(\"timestamp\", int(time.time() * 1000)) # Get transaction history history = transaction_history_store.get(card_number + \"_history\") if history is None: history = { \"transactions\": [], \"avg_amount\": 0, \"max_amount\": 0, \"locations\": set(), \"merchants\": set(), \"last_transaction_time\": 0 } # Calculate time since last transaction time_since_last = current_time - history.get(\"last_transaction_time\", 0) # Extract transaction details amount = transaction.get(\"amount\", 0) location = transaction.get(\"location\", \"unknown\") merchant = transaction.get(\"merchant\", \"unknown\") merchant_category = transaction.get(\"merchant_category\", \"unknown\") # Check for anomalies anomalies = [] # 1. Unusual amount if amount > 3 * history.get(\"avg_amount\", 0) and amount > 100: anomalies.append(\"unusual_amount\") # 2. New location if location not in history.get(\"locations\", set()) and len(history.get(\"locations\", set())) > 0: anomalies.append(\"new_location\") # 3. Rapid succession (multiple transactions in short time) if time_since_last < 5 * 60 * 1000 and time_since_last > 0: # Less than 5 minutes anomalies.append(\"rapid_succession\") # 4. High-risk merchant category if merchant_category in [\"gambling\", \"cryptocurrency\", \"money_transfer\"]: anomalies.append(\"high_risk_category\") # Update history history[\"transactions\"].append({ \"timestamp\": current_time, \"amount\": amount, \"location\": location, \"merchant\": merchant }) # Keep only last 20 transactions if len(history[\"transactions\"]) > 20: history[\"transactions\"] = history[\"transactions\"][-20:] # Update statistics history[\"avg_amount\"] = sum(t.get(\"amount\", 0) for t in history[\"transactions\"]) / len(history[\"transactions\"]) history[\"max_amount\"] = max(history[\"max_amount\"], amount) history[\"locations\"].add(location) history[\"merchants\"].add(merchant) history[\"last_transaction_time\"] = current_time # Store updated history transaction_history_store.put(card_number + \"_history\", history) # Generate alert if anomalies detected if anomalies: alert_id = str(uuid.uuid4()) alert = { \"alert_id\": alert_id, \"card_number\": card_number, \"timestamp\": current_time, \"transaction\": transaction, \"anomalies\": anomalies, \"risk_score\": len(anomalies) * 25, # Simple scoring: 25 points per anomaly \"status\": \"new\" } return (alert_id, alert) return None stores: - transaction_history_store correlate_with_location: type: valueTransformer code: | if value is None: return None alert = value card_number = alert.get(\"card_number\") transaction = alert.get(\"transaction\", {}) # Get user profile user_id = transaction.get(\"user_id\") if user_id: user_profile = user_profile_store.get(user_id) if user_profile: # Check for impossible travel last_known_location = user_profile.get(\"last_known_location\") current_location = transaction.get(\"location\") if last_known_location and current_location and last_known_location != current_location: last_location_time = user_profile.get(\"last_location_time\", 0) current_time = transaction.get(\"timestamp\", 0) # Simple check: if locations changed too quickly, flag as impossible travel if current_time - last_location_time < 3 * 60 * 60 * 1000: # Less than 3 hours alert[\"anomalies\"].append(\"impossible_travel\") alert[\"risk_score\"] += 50 # Higher score for impossible travel # Add location context alert[\"location_context\"] = { \"previous_location\": last_known_location, \"previous_location_time\": last_location_time, \"current_location\": current_location, \"travel_time_ms\": current_time - last_location_time } return alert stores: - user_profile_store enrich_with_user_data: type: valueTransformer code: | if value is None: return None alert = value transaction = alert.get(\"transaction\", {}) user_id = transaction.get(\"user_id\") if user_id: user_profile = user_profile_store.get(user_id) if user_profile: # Add user context to alert alert[\"user_context\"] = { \"user_id\": user_id, \"account_age_days\": user_profile.get(\"account_age_days\"), \"previous_fraud_alerts\": user_profile.get(\"fraud_alert_count\", 0) } # Adjust risk score based on user history if user_profile.get(\"fraud_alert_count\", 0) > 0: alert[\"risk_score\"] += 25 # Increase risk for users with previous alerts if user_profile.get(\"account_age_days\", 0) < 30: alert[\"risk_score\"] += 15 # Increase risk for new accounts # Categorize risk level if alert[\"risk_score\"] >= 90: alert[\"risk_level\"] = \"high\" elif alert[\"risk_score\"] >= 60: alert[\"risk_level\"] = \"medium\" else: alert[\"risk_level\"] = \"low\" return alert stores: - user_profile_store pipelines: # Process credit card transactions - first stage detect_transaction_anomalies: from: credit_card_transactions transformKeyValue: detect_transaction_anomalies filter: is_not_null to: potential_fraud_alerts # Process credit card transactions - second stage correlate_location_data: from: potential_fraud_alerts mapValues: correlate_with_location to: location_correlated_alerts # Process credit card transactions - final stage enrich_and_score_alerts: from: location_correlated_alerts mapValues: enrich_with_user_data to: fraud_alerts # Update user profiles with location data track_locations: from: location_changes mapValues: update_user_location to: updated_user_profiles This example: 1. Processes credit card transactions in real-time 2. Detects anomalies based on transaction amount, location, frequency, and merchant category 3. Correlates transactions with user location data to detect impossible travel patterns 4. Enriches alerts with user context and history 5. Calculates a risk score and categorizes alerts by risk level Advanced CEP Techniques State Management for Long-Running Patterns For patterns that span long periods, consider using persistent state stores: stores: long_term_pattern_store: type: keyValue keyType: string valueType: json persistent: true historyRetention: 7d # Keep state for 7 days Handling Out-of-Order Events Use windowing with grace periods to handle events that arrive out of order: pipelines: handle_out_of_order: from: input_stream groupByKey: windowByTime: size: 1h advanceBy: 1h grace: 15m # Allow events up to 15 minutes late aggregate: initializer: initialize_pattern_state aggregator: update_pattern_state to: detected_patterns Hierarchical Pattern Detection Implement hierarchical patterns by building higher-level patterns from lower-level ones: pipelines: # Detect basic patterns detect_basic_patterns: from: raw_events mapValues: detect_basic_pattern to: basic_patterns # Detect composite patterns from basic patterns detect_composite_patterns: from: basic_patterns mapValues: detect_composite_pattern to: composite_patterns Best Practices for Complex Event Processing Performance Considerations State Size : CEP often requires maintaining state. Monitor state store sizes and use windowing to limit state growth. Computation Complexity : Complex pattern detection can be CPU-intensive. Keep pattern matching logic efficient. Event Volume : High-volume streams may require pre-filtering to focus on relevant events. Design Patterns Pattern Decomposition : Break complex patterns into simpler sub-patterns that can be detected independently. Incremental Processing : Update pattern state incrementally as events arrive rather than reprocessing all events. Hierarchical Patterns : Build complex patterns by combining simpler patterns. Error Handling Implement robust error handling to prevent pattern detection failures: functions: robust_pattern_detection: type: valueTransformer code: | try: # Pattern detection logic return detected_pattern except Exception as e: log.error(\"Error in pattern detection: {}\", str(e)) # Return None to avoid emitting erroneous patterns return None Conclusion Complex Event Processing in KSML allows you to detect sophisticated patterns across multiple events and streams. By combining stateful processing, windowing operations, and custom Python functions, you can implement powerful CEP applications that derive meaningful insights from streaming data. In the next tutorial, we'll explore Custom State Stores to learn how to implement and optimize state management for advanced stream processing applications. Further Reading Core Concepts: Operations Core Concepts: Functions Intermediate Tutorial: Windowed Operations Reference: State Stores","title":"Complex Event Processing in KSML"},{"location":"tutorials/advanced/complex-event-processing/#complex-event-processing-in-ksml","text":"This tutorial explores how to implement complex event processing (CEP) patterns in KSML, allowing you to detect meaningful patterns across multiple events and streams in real-time.","title":"Complex Event Processing in KSML"},{"location":"tutorials/advanced/complex-event-processing/#introduction-to-complex-event-processing","text":"Complex Event Processing (CEP) is a method of tracking and analyzing streams of data about things that happen (events), and deriving conclusions from them. In streaming contexts, CEP allows you to: Detect patterns across multiple events Identify sequences of events that occur over time Correlate events from different sources Derive higher-level insights from lower-level events KSML provides powerful capabilities for implementing CEP patterns through its combination of stateful processing, windowing operations, and Python functions.","title":"Introduction to Complex Event Processing"},{"location":"tutorials/advanced/complex-event-processing/#prerequisites","text":"Before starting this tutorial, you should: Understand intermediate KSML concepts (streams, functions, pipelines) Have completed the Windowed Operations tutorial Be familiar with Joins and Aggregations Have a basic understanding of state management in stream processing","title":"Prerequisites"},{"location":"tutorials/advanced/complex-event-processing/#key-cep-patterns-in-ksml","text":"","title":"Key CEP Patterns in KSML"},{"location":"tutorials/advanced/complex-event-processing/#1-pattern-detection","text":"Pattern detection involves identifying specific sequences or combinations of events within a stream: functions: detect_pattern: type: valueTransformer code: | # Check if this event completes a pattern if value.get(\"event_type\") == \"C\" and state_store.get(key + \"_has_A\") and state_store.get(key + \"_has_B\"): # Pattern A -> B -> C detected result = { \"pattern_detected\": \"A_B_C\", \"completion_time\": value.get(\"timestamp\"), \"key\": key } # Reset pattern tracking state_store.delete(key + \"_has_A\") state_store.delete(key + \"_has_B\") return result # Track events that are part of the pattern if value.get(\"event_type\") == \"A\": state_store.put(key + \"_has_A\", True) elif value.get(\"event_type\") == \"B\": state_store.put(key + \"_has_B\", True) # Don't emit anything for partial patterns return None stores: - pattern_state_store pipelines: detect_abc_pattern: from: input_events mapValues: detect_pattern filter: is_not_null # Only pass through completed patterns to: detected_patterns","title":"1. Pattern Detection"},{"location":"tutorials/advanced/complex-event-processing/#2-temporal-pattern-matching","text":"Temporal pattern matching adds time constraints to pattern detection: functions: detect_temporal_pattern: type: valueTransformer code: | current_time = value.get(\"timestamp\", int(time.time() * 1000)) if value.get(\"event_type\") == \"A\": # Start tracking a new potential pattern state_store.put(key + \"_pattern_start\", current_time) state_store.put(key + \"_has_A\", True) return None if value.get(\"event_type\") == \"B\" and state_store.get(key + \"_has_A\"): # Check if B occurred within 5 minutes of A pattern_start = state_store.get(key + \"_pattern_start\") if pattern_start and (current_time - pattern_start) <= 5 * 60 * 1000: state_store.put(key + \"_has_B\", True) state_store.put(key + \"_B_time\", current_time) return None if value.get(\"event_type\") == \"C\" and state_store.get(key + \"_has_B\"): # Check if C occurred within 2 minutes of B b_time = state_store.get(key + \"_B_time\") if b_time and (current_time - b_time) <= 2 * 60 * 1000: # Pattern A -> B -> C detected within time constraints pattern_start = state_store.get(key + \"_pattern_start\") result = { \"pattern_detected\": \"A_B_C\", \"start_time\": pattern_start, \"end_time\": current_time, \"duration_ms\": current_time - pattern_start, \"key\": key } # Reset pattern tracking state_store.delete(key + \"_has_A\") state_store.delete(key + \"_has_B\") state_store.delete(key + \"_pattern_start\") state_store.delete(key + \"_B_time\") return result return None stores: - temporal_pattern_store","title":"2. Temporal Pattern Matching"},{"location":"tutorials/advanced/complex-event-processing/#3-event-correlation-and-enrichment","text":"Event correlation involves combining related events from different streams: streams: user_logins: topic: user_login_events keyType: string # User ID valueType: json # Login details user_actions: topic: user_action_events keyType: string # User ID valueType: json # Action details user_logouts: topic: user_logout_events keyType: string # User ID valueType: json # Logout details user_sessions: topic: user_session_events keyType: string # User ID valueType: json # Complete session information functions: correlate_session_events: type: valueTransformer code: | event_type = value.get(\"event_type\") # Get current session state session = state_store.get(key + \"_session\") if session is None: session = {\"events\": []} # Add this event to the session event_copy = value.copy() event_copy[\"processed_time\"] = int(time.time() * 1000) session[\"events\"].append(event_copy) # Update session based on event type if event_type == \"login\": session[\"login_time\"] = value.get(\"timestamp\") session[\"device\"] = value.get(\"device\") session[\"ip_address\"] = value.get(\"ip_address\") session[\"status\"] = \"active\" elif event_type == \"action\": session[\"last_activity_time\"] = value.get(\"timestamp\") session[\"last_action\"] = value.get(\"action_type\") elif event_type == \"logout\": session[\"logout_time\"] = value.get(\"timestamp\") session[\"status\"] = \"completed\" session[\"duration_ms\"] = session[\"logout_time\"] - session.get(\"login_time\", 0) # Return the complete session and clear state result = session.copy() state_store.delete(key + \"_session\") return result # Update session state and don't emit for incomplete sessions state_store.put(key + \"_session\", session) return None stores: - session_state_store pipelines: process_login_events: from: user_logins mapValues: correlate_session_events filter: is_not_null to: user_sessions process_action_events: from: user_actions mapValues: correlate_session_events filter: is_not_null to: user_sessions process_logout_events: from: user_logouts mapValues: correlate_session_events filter: is_not_null to: user_sessions","title":"3. Event Correlation and Enrichment"},{"location":"tutorials/advanced/complex-event-processing/#4-anomaly-detection","text":"Anomaly detection identifies unusual patterns or deviations from normal behavior: functions: detect_anomalies: type: valueTransformer code: | # Get historical values for this key history = state_store.get(key + \"_history\") if history is None: history = {\"values\": [], \"sum\": 0, \"count\": 0} # Extract the value to monitor metric_value = value.get(\"metric_value\", 0) # Update history history[\"values\"].append(metric_value) history[\"sum\"] += metric_value history[\"count\"] += 1 # Keep only the last 10 values if len(history[\"values\"]) > 10: removed_value = history[\"values\"].pop(0) history[\"sum\"] -= removed_value history[\"count\"] -= 1 # Calculate statistics avg = history[\"sum\"] / history[\"count\"] if history[\"count\"] > 0 else 0 # Calculate standard deviation variance_sum = sum((x - avg) ** 2 for x in history[\"values\"]) std_dev = (variance_sum / history[\"count\"]) ** 0.5 if history[\"count\"] > 0 else 0 # Check for anomaly (value more than 3 standard deviations from mean) is_anomaly = False if std_dev > 0 and abs(metric_value - avg) > 3 * std_dev: is_anomaly = True # Update state state_store.put(key + \"_history\", history) # Return anomaly information if detected if is_anomaly: return { \"key\": key, \"timestamp\": value.get(\"timestamp\"), \"metric_value\": metric_value, \"average\": avg, \"std_dev\": std_dev, \"deviation\": abs(metric_value - avg) / std_dev if std_dev > 0 else 0, \"original_event\": value } return None stores: - anomaly_detection_store","title":"4. Anomaly Detection"},{"location":"tutorials/advanced/complex-event-processing/#practical-example-fraud-detection-system","text":"Let's build a complete example that implements a real-time fraud detection system using CEP patterns: streams: credit_card_transactions: topic: cc_transactions keyType: string # Card number valueType: json # Transaction details location_changes: topic: location_events keyType: string # User ID valueType: json # Location information authentication_events: topic: auth_events keyType: string # User ID valueType: json # Authentication details fraud_alerts: topic: fraud_alerts keyType: string # Alert ID valueType: json # Alert details stores: transaction_history_store: type: keyValue keyType: string valueType: json persistent: true user_profile_store: type: keyValue keyType: string valueType: json persistent: true alert_state_store: type: keyValue keyType: string valueType: json persistent: false functions: detect_transaction_anomalies: type: keyValueTransformer code: | card_number = key transaction = value current_time = transaction.get(\"timestamp\", int(time.time() * 1000)) # Get transaction history history = transaction_history_store.get(card_number + \"_history\") if history is None: history = { \"transactions\": [], \"avg_amount\": 0, \"max_amount\": 0, \"locations\": set(), \"merchants\": set(), \"last_transaction_time\": 0 } # Calculate time since last transaction time_since_last = current_time - history.get(\"last_transaction_time\", 0) # Extract transaction details amount = transaction.get(\"amount\", 0) location = transaction.get(\"location\", \"unknown\") merchant = transaction.get(\"merchant\", \"unknown\") merchant_category = transaction.get(\"merchant_category\", \"unknown\") # Check for anomalies anomalies = [] # 1. Unusual amount if amount > 3 * history.get(\"avg_amount\", 0) and amount > 100: anomalies.append(\"unusual_amount\") # 2. New location if location not in history.get(\"locations\", set()) and len(history.get(\"locations\", set())) > 0: anomalies.append(\"new_location\") # 3. Rapid succession (multiple transactions in short time) if time_since_last < 5 * 60 * 1000 and time_since_last > 0: # Less than 5 minutes anomalies.append(\"rapid_succession\") # 4. High-risk merchant category if merchant_category in [\"gambling\", \"cryptocurrency\", \"money_transfer\"]: anomalies.append(\"high_risk_category\") # Update history history[\"transactions\"].append({ \"timestamp\": current_time, \"amount\": amount, \"location\": location, \"merchant\": merchant }) # Keep only last 20 transactions if len(history[\"transactions\"]) > 20: history[\"transactions\"] = history[\"transactions\"][-20:] # Update statistics history[\"avg_amount\"] = sum(t.get(\"amount\", 0) for t in history[\"transactions\"]) / len(history[\"transactions\"]) history[\"max_amount\"] = max(history[\"max_amount\"], amount) history[\"locations\"].add(location) history[\"merchants\"].add(merchant) history[\"last_transaction_time\"] = current_time # Store updated history transaction_history_store.put(card_number + \"_history\", history) # Generate alert if anomalies detected if anomalies: alert_id = str(uuid.uuid4()) alert = { \"alert_id\": alert_id, \"card_number\": card_number, \"timestamp\": current_time, \"transaction\": transaction, \"anomalies\": anomalies, \"risk_score\": len(anomalies) * 25, # Simple scoring: 25 points per anomaly \"status\": \"new\" } return (alert_id, alert) return None stores: - transaction_history_store correlate_with_location: type: valueTransformer code: | if value is None: return None alert = value card_number = alert.get(\"card_number\") transaction = alert.get(\"transaction\", {}) # Get user profile user_id = transaction.get(\"user_id\") if user_id: user_profile = user_profile_store.get(user_id) if user_profile: # Check for impossible travel last_known_location = user_profile.get(\"last_known_location\") current_location = transaction.get(\"location\") if last_known_location and current_location and last_known_location != current_location: last_location_time = user_profile.get(\"last_location_time\", 0) current_time = transaction.get(\"timestamp\", 0) # Simple check: if locations changed too quickly, flag as impossible travel if current_time - last_location_time < 3 * 60 * 60 * 1000: # Less than 3 hours alert[\"anomalies\"].append(\"impossible_travel\") alert[\"risk_score\"] += 50 # Higher score for impossible travel # Add location context alert[\"location_context\"] = { \"previous_location\": last_known_location, \"previous_location_time\": last_location_time, \"current_location\": current_location, \"travel_time_ms\": current_time - last_location_time } return alert stores: - user_profile_store enrich_with_user_data: type: valueTransformer code: | if value is None: return None alert = value transaction = alert.get(\"transaction\", {}) user_id = transaction.get(\"user_id\") if user_id: user_profile = user_profile_store.get(user_id) if user_profile: # Add user context to alert alert[\"user_context\"] = { \"user_id\": user_id, \"account_age_days\": user_profile.get(\"account_age_days\"), \"previous_fraud_alerts\": user_profile.get(\"fraud_alert_count\", 0) } # Adjust risk score based on user history if user_profile.get(\"fraud_alert_count\", 0) > 0: alert[\"risk_score\"] += 25 # Increase risk for users with previous alerts if user_profile.get(\"account_age_days\", 0) < 30: alert[\"risk_score\"] += 15 # Increase risk for new accounts # Categorize risk level if alert[\"risk_score\"] >= 90: alert[\"risk_level\"] = \"high\" elif alert[\"risk_score\"] >= 60: alert[\"risk_level\"] = \"medium\" else: alert[\"risk_level\"] = \"low\" return alert stores: - user_profile_store pipelines: # Process credit card transactions - first stage detect_transaction_anomalies: from: credit_card_transactions transformKeyValue: detect_transaction_anomalies filter: is_not_null to: potential_fraud_alerts # Process credit card transactions - second stage correlate_location_data: from: potential_fraud_alerts mapValues: correlate_with_location to: location_correlated_alerts # Process credit card transactions - final stage enrich_and_score_alerts: from: location_correlated_alerts mapValues: enrich_with_user_data to: fraud_alerts # Update user profiles with location data track_locations: from: location_changes mapValues: update_user_location to: updated_user_profiles This example: 1. Processes credit card transactions in real-time 2. Detects anomalies based on transaction amount, location, frequency, and merchant category 3. Correlates transactions with user location data to detect impossible travel patterns 4. Enriches alerts with user context and history 5. Calculates a risk score and categorizes alerts by risk level","title":"Practical Example: Fraud Detection System"},{"location":"tutorials/advanced/complex-event-processing/#advanced-cep-techniques","text":"","title":"Advanced CEP Techniques"},{"location":"tutorials/advanced/complex-event-processing/#state-management-for-long-running-patterns","text":"For patterns that span long periods, consider using persistent state stores: stores: long_term_pattern_store: type: keyValue keyType: string valueType: json persistent: true historyRetention: 7d # Keep state for 7 days","title":"State Management for Long-Running Patterns"},{"location":"tutorials/advanced/complex-event-processing/#handling-out-of-order-events","text":"Use windowing with grace periods to handle events that arrive out of order: pipelines: handle_out_of_order: from: input_stream groupByKey: windowByTime: size: 1h advanceBy: 1h grace: 15m # Allow events up to 15 minutes late aggregate: initializer: initialize_pattern_state aggregator: update_pattern_state to: detected_patterns","title":"Handling Out-of-Order Events"},{"location":"tutorials/advanced/complex-event-processing/#hierarchical-pattern-detection","text":"Implement hierarchical patterns by building higher-level patterns from lower-level ones: pipelines: # Detect basic patterns detect_basic_patterns: from: raw_events mapValues: detect_basic_pattern to: basic_patterns # Detect composite patterns from basic patterns detect_composite_patterns: from: basic_patterns mapValues: detect_composite_pattern to: composite_patterns","title":"Hierarchical Pattern Detection"},{"location":"tutorials/advanced/complex-event-processing/#best-practices-for-complex-event-processing","text":"","title":"Best Practices for Complex Event Processing"},{"location":"tutorials/advanced/complex-event-processing/#performance-considerations","text":"State Size : CEP often requires maintaining state. Monitor state store sizes and use windowing to limit state growth. Computation Complexity : Complex pattern detection can be CPU-intensive. Keep pattern matching logic efficient. Event Volume : High-volume streams may require pre-filtering to focus on relevant events.","title":"Performance Considerations"},{"location":"tutorials/advanced/complex-event-processing/#design-patterns","text":"Pattern Decomposition : Break complex patterns into simpler sub-patterns that can be detected independently. Incremental Processing : Update pattern state incrementally as events arrive rather than reprocessing all events. Hierarchical Patterns : Build complex patterns by combining simpler patterns.","title":"Design Patterns"},{"location":"tutorials/advanced/complex-event-processing/#error-handling","text":"Implement robust error handling to prevent pattern detection failures: functions: robust_pattern_detection: type: valueTransformer code: | try: # Pattern detection logic return detected_pattern except Exception as e: log.error(\"Error in pattern detection: {}\", str(e)) # Return None to avoid emitting erroneous patterns return None","title":"Error Handling"},{"location":"tutorials/advanced/complex-event-processing/#conclusion","text":"Complex Event Processing in KSML allows you to detect sophisticated patterns across multiple events and streams. By combining stateful processing, windowing operations, and custom Python functions, you can implement powerful CEP applications that derive meaningful insights from streaming data. In the next tutorial, we'll explore Custom State Stores to learn how to implement and optimize state management for advanced stream processing applications.","title":"Conclusion"},{"location":"tutorials/advanced/complex-event-processing/#further-reading","text":"Core Concepts: Operations Core Concepts: Functions Intermediate Tutorial: Windowed Operations Reference: State Stores","title":"Further Reading"},{"location":"tutorials/advanced/custom-state-stores/","text":"Custom State Stores in KSML This tutorial explores how to implement and optimize custom state stores in KSML, allowing you to maintain and manage state in your stream processing applications with greater flexibility and control. Introduction to State Stores State stores are a critical component of stateful stream processing applications. They allow your application to: Maintain data across multiple messages and events Track historical information for context-aware processing Implement stateful operations like aggregations and joins Build sophisticated business logic that depends on previous events KSML provides built-in state store capabilities, but for advanced use cases, you may need to customize how state is stored, accessed, and managed. Prerequisites Before starting this tutorial, you should: Understand intermediate KSML concepts (streams, functions, pipelines) Have completed the Aggregations and Joins tutorials Be familiar with Stateful Operations Have a basic understanding of Kafka Streams state stores State Store Fundamentals Types of State Stores in KSML KSML supports several types of state stores: Key-Value Stores : Simple stores that map keys to values Window Stores : Stores that organize data by time windows Session Stores : Stores that organize data by session windows Each type has different characteristics and is suitable for different use cases. State Store Configuration State stores in KSML are defined in the stores section of your KSML definition file: stores: user_profile_store: type: keyValue keyType: string valueType: json persistent: true historyRetention: 7d caching: true logging: false Key configuration options include: type : The type of store (keyValue, window, session) keyType/valueType : The data types for keys and values persistent : Whether the store should persist data to disk historyRetention : How long to retain historical data caching : Whether to enable caching for faster access logging : Whether to enable change logging Implementing Custom State Stores 1. Custom Serialization and Deserialization For complex data types or special performance requirements, you can implement custom serialization: stores: custom_serialized_store: type: keyValue keyType: string valueType: custom persistent: true serdes: key: org.example.CustomKeySerializer value: org.example.CustomValueSerializer In your Python functions, you can work with the deserialized objects directly: functions: process_with_custom_store: type: valueTransformer code: | # Get data from custom store stored_value = custom_serialized_store.get(key) # Process with custom object result = process_data(value, stored_value) # Update store with new value custom_serialized_store.put(key, result) return result stores: - custom_serialized_store 2. Custom State Store Implementations For advanced use cases, you can implement custom state store classes in Java and reference them in KSML: stores: specialized_store: type: custom implementation: org.example.SpecializedStateStore config: customParam1: value1 customParam2: value2 This approach allows you to implement specialized functionality like: Custom indexing for faster lookups Compression strategies for large states Integration with external systems Special eviction policies 3. Partitioned State Stores For better scalability, you can implement partitioned state stores that distribute state across multiple instances: stores: partitioned_store: type: keyValue keyType: string valueType: json persistent: true partitioned: true numPartitions: 10 This approach helps manage large state sizes by distributing the load across multiple partitions. Optimizing State Store Performance 1. Caching Strategies Configure caching to balance between performance and memory usage: stores: optimized_store: type: keyValue keyType: string valueType: json persistent: true caching: true cacheSizeBytes: 10485760 # 10MB cache In your functions, organize access patterns to maximize cache hits: functions: cache_optimized_function: type: valueTransformer code: | # Batch similar lookups together keys_to_lookup = extract_related_keys(value) cached_values = {} for k in keys_to_lookup: cached_values[k] = optimized_store.get(k) # Process with cached values result = process_with_cached_data(value, cached_values) return result stores: - optimized_store 2. Memory Management Implement strategies to control memory usage: functions: memory_efficient_function: type: valueTransformer code: | # Use compact data structures current_state = optimized_store.get(key) if current_state is None: current_state = {\"c\": 0, \"s\": 0} # Use short keys # Update state with minimal memory footprint current_state[\"c\"] += 1 # count current_state[\"s\"] += value.get(\"amount\", 0) # sum # Store only what's needed optimized_store.put(key, current_state) return {\"count\": current_state[\"c\"], \"sum\": current_state[\"s\"]} stores: - optimized_store 3. Retention Policies Configure appropriate retention policies to limit state size: stores: time_limited_store: type: window keyType: string valueType: json persistent: true historyRetention: 24h # Only keep 24 hours of history retainDuplicates: false # Don't store duplicates 4. Compaction and Cleanup Implement periodic cleanup to manage state size: functions: cleanup_old_data: type: valueTransformer code: | current_time = int(time.time() * 1000) stored_data = time_limited_store.get(key) if stored_data and \"timestamp\" in stored_data: # Check if data is older than 7 days if current_time - stored_data[\"timestamp\"] > 7 * 24 * 60 * 60 * 1000: # Delete old data time_limited_store.delete(key) return None return value stores: - time_limited_store Practical Example: Advanced User Behavior Tracking Let's build a complete example that implements sophisticated user behavior tracking with optimized state stores: streams: user_events: topic: user_activity_events keyType: string # User ID valueType: json # Event details user_profiles: topic: user_profile_updates keyType: string # User ID valueType: json # Profile details user_insights: topic: user_behavior_insights keyType: string # User ID valueType: json # Behavior insights stores: # Store for recent user events (last 100 events per user) recent_events_store: type: keyValue keyType: string valueType: json persistent: true caching: true cacheSizeBytes: 52428800 # 50MB cache # Store for user profiles user_profile_store: type: keyValue keyType: string valueType: json persistent: true caching: true # Store for behavior metrics (windowed) behavior_metrics_store: type: window keyType: string valueType: json windowSize: 30d # 30-day windows persistent: true historyRetention: 90d # Keep 90 days of history functions: track_user_events: type: valueTransformer code: | user_id = key event = value current_time = event.get(\"timestamp\", int(time.time() * 1000)) # Get recent events for this user recent_events = recent_events_store.get(user_id) if recent_events is None: recent_events = { \"events\": [], \"event_counts\": {}, \"last_updated\": current_time } # Add new event to the list event_type = event.get(\"event_type\", \"unknown\") recent_events[\"events\"].append({ \"timestamp\": current_time, \"type\": event_type, \"properties\": event.get(\"properties\", {}) }) # Update event counts if event_type in recent_events[\"event_counts\"]: recent_events[\"event_counts\"][event_type] += 1 else: recent_events[\"event_counts\"][event_type] = 1 # Keep only the most recent 100 events if len(recent_events[\"events\"]) > 100: recent_events[\"events\"] = recent_events[\"events\"][-100:] # Update last updated timestamp recent_events[\"last_updated\"] = current_time # Store updated events recent_events_store.put(user_id, recent_events) # Don't emit anything from this function return None stores: - recent_events_store update_user_profile: type: valueTransformer code: | user_id = key profile_update = value # Get existing profile existing_profile = user_profile_store.get(user_id) if existing_profile is None: existing_profile = { \"created_at\": int(time.time() * 1000), \"attributes\": {} } # Update profile with new data if \"attributes\" in profile_update: for attr_key, attr_value in profile_update[\"attributes\"].items(): existing_profile[\"attributes\"][attr_key] = attr_value # Add metadata existing_profile[\"last_updated\"] = int(time.time() * 1000) # Store updated profile user_profile_store.put(user_id, existing_profile) # Don't emit anything from this function return None stores: - user_profile_store generate_user_insights: type: valueTransformer code: | user_id = key trigger_event = value # This could be any event that triggers insight generation current_time = int(time.time() * 1000) # Get user's recent events recent_events = recent_events_store.get(user_id) if recent_events is None or not recent_events.get(\"events\"): return None # No events to analyze # Get user profile user_profile = user_profile_store.get(user_id) if user_profile is None: user_profile = {\"attributes\": {}} # Get behavior metrics behavior_metrics = behavior_metrics_store.get(user_id) if behavior_metrics is None: behavior_metrics = { \"session_count\": 0, \"total_time_spent\": 0, \"average_session_length\": 0, \"first_seen\": current_time, \"last_seen\": current_time } # Analyze events to generate insights events = recent_events.get(\"events\", []) event_counts = recent_events.get(\"event_counts\", {}) # Calculate recency (days since last activity) last_activity = max(event[\"timestamp\"] for event in events) if events else current_time recency_days = (current_time - last_activity) / (24 * 60 * 60 * 1000) # Calculate frequency (number of events in last 30 days) thirty_days_ago = current_time - (30 * 24 * 60 * 60 * 1000) recent_event_count = sum(1 for event in events if event[\"timestamp\"] > thirty_days_ago) # Calculate engagement score engagement_score = calculate_engagement_score(events, user_profile) # Update behavior metrics behavior_metrics[\"last_seen\"] = last_activity behavior_metrics[\"event_count_30d\"] = recent_event_count behavior_metrics[\"recency_days\"] = recency_days behavior_metrics[\"engagement_score\"] = engagement_score # Store updated metrics behavior_metrics_store.put(user_id, behavior_metrics) # Generate insights insights = { \"user_id\": user_id, \"timestamp\": current_time, \"recency_days\": recency_days, \"frequency_30d\": recent_event_count, \"engagement_score\": engagement_score, \"top_activities\": get_top_activities(event_counts), \"user_segment\": determine_user_segment(recency_days, recent_event_count, engagement_score), \"recommendations\": generate_recommendations(events, user_profile) } return insights stores: - recent_events_store - user_profile_store - behavior_metrics_store globalCode: | def calculate_engagement_score(events, user_profile): # Implementation of engagement scoring algorithm if not events: return 0 # Simple scoring based on recency and frequency recent_events = [e for e in events if e[\"timestamp\"] > (int(time.time() * 1000) - 7 * 24 * 60 * 60 * 1000)] score = len(recent_events) * 10 # Bonus for high-value activities for event in recent_events: if event[\"type\"] in [\"purchase\", \"subscription\", \"share\"]: score += 20 return min(100, score) def get_top_activities(event_counts): # Return top 5 activities by count sorted_activities = sorted(event_counts.items(), key=lambda x: x[1], reverse=True) return dict(sorted_activities[:5]) def determine_user_segment(recency, frequency, engagement): # Segment users based on RFE (Recency, Frequency, Engagement) if recency < 7 and frequency > 10 and engagement > 70: return \"highly_engaged\" elif recency < 30 and frequency > 5 and engagement > 40: return \"engaged\" elif recency < 90: return \"casual\" else: return \"inactive\" def generate_recommendations(events, user_profile): # Simple recommendation engine based on user activity recommendations = [] event_types = set(event[\"type\"] for event in events) if \"view_product\" in event_types and \"add_to_cart\" not in event_types: recommendations.append(\"complete_purchase\") if \"subscription_view\" in event_types and \"subscription\" not in event_types: recommendations.append(\"subscribe\") return recommendations[:3] # Return top 3 recommendations pipelines: # Process user events track_events: from: user_events mapValues: track_user_events filter: is_not_null to: tracked_events # Process profile updates update_profiles: from: user_profiles mapValues: update_user_profile filter: is_not_null to: updated_profiles # Generate insights (triggered by specific events) filter_trigger_events: from: user_events filter: is_insight_trigger_event to: insight_trigger_events # Process trigger events to generate insights generate_insights: from: insight_trigger_events mapValues: generate_user_insights filter: is_not_null to: user_insights This example: 1. Tracks user events in a memory-efficient way, keeping only the 100 most recent events 2. Maintains user profiles with attribute updates 3. Generates behavioral insights using data from multiple state stores 4. Implements sophisticated analytics like engagement scoring and user segmentation 5. Uses optimized state stores with appropriate caching and retention policies Advanced State Store Patterns 1. Tiered Storage Implement tiered storage for different access patterns: stores: hot_data_store: type: keyValue keyType: string valueType: json persistent: false # In-memory only caching: true warm_data_store: type: keyValue keyType: string valueType: json persistent: true caching: true cold_data_store: type: keyValue keyType: string valueType: json persistent: true caching: false 2. Read-Through/Write-Through Caching Implement patterns to synchronize state with external systems: functions: cache_with_external_system: type: valueTransformer code: | # Try to get from local cache first cached_value = local_cache_store.get(key) if cached_value is None: # Cache miss - fetch from external system external_value = fetch_from_external_system(key) # Update local cache if external_value is not None: local_cache_store.put(key, external_value) return process_data(value, external_value) else: # Cache hit return process_data(value, cached_value) stores: - local_cache_store 3. Composite State Patterns Combine multiple state stores for complex use cases: functions: use_composite_state: type: valueTransformer code: | # Get data from multiple stores recent_data = recent_store.get(key) historical_data = historical_store.get(key) metadata = metadata_store.get(key) # Combine data for processing combined_state = { \"recent\": recent_data, \"historical\": historical_data, \"metadata\": metadata } # Process with combined state result = process_with_combined_state(value, combined_state) # Update stores as needed recent_store.put(key, result.get(\"recent\")) metadata_store.put(key, result.get(\"metadata\")) return result.get(\"output\") stores: - recent_store - historical_store - metadata_store Best Practices for Custom State Stores Performance Considerations Memory Management : Monitor and control memory usage, especially for large state stores Serialization Overhead : Be aware of serialization/deserialization costs for frequent state access Caching Strategy : Configure appropriate cache sizes based on access patterns Persistence Impact : Understand the performance implications of persistent vs. in-memory stores Design Patterns State Partitioning : Partition large states to improve scalability Minimal State : Store only what's necessary to reduce resource usage Batched Operations : Group state operations to improve efficiency Incremental Updates : Update state incrementally rather than replacing entire objects Error Handling Implement robust error handling for state store operations: functions: robust_state_access: type: valueTransformer code: | try: # Access state store stored_value = my_store.get(key) # Process with state result = process_with_state(value, stored_value) # Update state my_store.put(key, updated_state) return result except Exception as e: log.error(\"Error accessing state store: {}\", str(e)) # Return fallback value or original input return value stores: - my_store Conclusion Custom state stores in KSML provide powerful capabilities for implementing sophisticated stateful stream processing applications. By understanding how to implement, optimize, and manage state stores, you can build applications that efficiently maintain state across events and deliver complex business logic. In the next tutorial, we'll explore Performance Optimization to learn how to tune and optimize your KSML applications for maximum throughput and minimal resource usage. Further Reading Core Concepts: Operations Core Concepts: Functions Intermediate Tutorial: Aggregations Reference: State Stores","title":"Custom State Stores in KSML"},{"location":"tutorials/advanced/custom-state-stores/#custom-state-stores-in-ksml","text":"This tutorial explores how to implement and optimize custom state stores in KSML, allowing you to maintain and manage state in your stream processing applications with greater flexibility and control.","title":"Custom State Stores in KSML"},{"location":"tutorials/advanced/custom-state-stores/#introduction-to-state-stores","text":"State stores are a critical component of stateful stream processing applications. They allow your application to: Maintain data across multiple messages and events Track historical information for context-aware processing Implement stateful operations like aggregations and joins Build sophisticated business logic that depends on previous events KSML provides built-in state store capabilities, but for advanced use cases, you may need to customize how state is stored, accessed, and managed.","title":"Introduction to State Stores"},{"location":"tutorials/advanced/custom-state-stores/#prerequisites","text":"Before starting this tutorial, you should: Understand intermediate KSML concepts (streams, functions, pipelines) Have completed the Aggregations and Joins tutorials Be familiar with Stateful Operations Have a basic understanding of Kafka Streams state stores","title":"Prerequisites"},{"location":"tutorials/advanced/custom-state-stores/#state-store-fundamentals","text":"","title":"State Store Fundamentals"},{"location":"tutorials/advanced/custom-state-stores/#types-of-state-stores-in-ksml","text":"KSML supports several types of state stores: Key-Value Stores : Simple stores that map keys to values Window Stores : Stores that organize data by time windows Session Stores : Stores that organize data by session windows Each type has different characteristics and is suitable for different use cases.","title":"Types of State Stores in KSML"},{"location":"tutorials/advanced/custom-state-stores/#state-store-configuration","text":"State stores in KSML are defined in the stores section of your KSML definition file: stores: user_profile_store: type: keyValue keyType: string valueType: json persistent: true historyRetention: 7d caching: true logging: false Key configuration options include: type : The type of store (keyValue, window, session) keyType/valueType : The data types for keys and values persistent : Whether the store should persist data to disk historyRetention : How long to retain historical data caching : Whether to enable caching for faster access logging : Whether to enable change logging","title":"State Store Configuration"},{"location":"tutorials/advanced/custom-state-stores/#implementing-custom-state-stores","text":"","title":"Implementing Custom State Stores"},{"location":"tutorials/advanced/custom-state-stores/#1-custom-serialization-and-deserialization","text":"For complex data types or special performance requirements, you can implement custom serialization: stores: custom_serialized_store: type: keyValue keyType: string valueType: custom persistent: true serdes: key: org.example.CustomKeySerializer value: org.example.CustomValueSerializer In your Python functions, you can work with the deserialized objects directly: functions: process_with_custom_store: type: valueTransformer code: | # Get data from custom store stored_value = custom_serialized_store.get(key) # Process with custom object result = process_data(value, stored_value) # Update store with new value custom_serialized_store.put(key, result) return result stores: - custom_serialized_store","title":"1. Custom Serialization and Deserialization"},{"location":"tutorials/advanced/custom-state-stores/#2-custom-state-store-implementations","text":"For advanced use cases, you can implement custom state store classes in Java and reference them in KSML: stores: specialized_store: type: custom implementation: org.example.SpecializedStateStore config: customParam1: value1 customParam2: value2 This approach allows you to implement specialized functionality like: Custom indexing for faster lookups Compression strategies for large states Integration with external systems Special eviction policies","title":"2. Custom State Store Implementations"},{"location":"tutorials/advanced/custom-state-stores/#3-partitioned-state-stores","text":"For better scalability, you can implement partitioned state stores that distribute state across multiple instances: stores: partitioned_store: type: keyValue keyType: string valueType: json persistent: true partitioned: true numPartitions: 10 This approach helps manage large state sizes by distributing the load across multiple partitions.","title":"3. Partitioned State Stores"},{"location":"tutorials/advanced/custom-state-stores/#optimizing-state-store-performance","text":"","title":"Optimizing State Store Performance"},{"location":"tutorials/advanced/custom-state-stores/#1-caching-strategies","text":"Configure caching to balance between performance and memory usage: stores: optimized_store: type: keyValue keyType: string valueType: json persistent: true caching: true cacheSizeBytes: 10485760 # 10MB cache In your functions, organize access patterns to maximize cache hits: functions: cache_optimized_function: type: valueTransformer code: | # Batch similar lookups together keys_to_lookup = extract_related_keys(value) cached_values = {} for k in keys_to_lookup: cached_values[k] = optimized_store.get(k) # Process with cached values result = process_with_cached_data(value, cached_values) return result stores: - optimized_store","title":"1. Caching Strategies"},{"location":"tutorials/advanced/custom-state-stores/#2-memory-management","text":"Implement strategies to control memory usage: functions: memory_efficient_function: type: valueTransformer code: | # Use compact data structures current_state = optimized_store.get(key) if current_state is None: current_state = {\"c\": 0, \"s\": 0} # Use short keys # Update state with minimal memory footprint current_state[\"c\"] += 1 # count current_state[\"s\"] += value.get(\"amount\", 0) # sum # Store only what's needed optimized_store.put(key, current_state) return {\"count\": current_state[\"c\"], \"sum\": current_state[\"s\"]} stores: - optimized_store","title":"2. Memory Management"},{"location":"tutorials/advanced/custom-state-stores/#3-retention-policies","text":"Configure appropriate retention policies to limit state size: stores: time_limited_store: type: window keyType: string valueType: json persistent: true historyRetention: 24h # Only keep 24 hours of history retainDuplicates: false # Don't store duplicates","title":"3. Retention Policies"},{"location":"tutorials/advanced/custom-state-stores/#4-compaction-and-cleanup","text":"Implement periodic cleanup to manage state size: functions: cleanup_old_data: type: valueTransformer code: | current_time = int(time.time() * 1000) stored_data = time_limited_store.get(key) if stored_data and \"timestamp\" in stored_data: # Check if data is older than 7 days if current_time - stored_data[\"timestamp\"] > 7 * 24 * 60 * 60 * 1000: # Delete old data time_limited_store.delete(key) return None return value stores: - time_limited_store","title":"4. Compaction and Cleanup"},{"location":"tutorials/advanced/custom-state-stores/#practical-example-advanced-user-behavior-tracking","text":"Let's build a complete example that implements sophisticated user behavior tracking with optimized state stores: streams: user_events: topic: user_activity_events keyType: string # User ID valueType: json # Event details user_profiles: topic: user_profile_updates keyType: string # User ID valueType: json # Profile details user_insights: topic: user_behavior_insights keyType: string # User ID valueType: json # Behavior insights stores: # Store for recent user events (last 100 events per user) recent_events_store: type: keyValue keyType: string valueType: json persistent: true caching: true cacheSizeBytes: 52428800 # 50MB cache # Store for user profiles user_profile_store: type: keyValue keyType: string valueType: json persistent: true caching: true # Store for behavior metrics (windowed) behavior_metrics_store: type: window keyType: string valueType: json windowSize: 30d # 30-day windows persistent: true historyRetention: 90d # Keep 90 days of history functions: track_user_events: type: valueTransformer code: | user_id = key event = value current_time = event.get(\"timestamp\", int(time.time() * 1000)) # Get recent events for this user recent_events = recent_events_store.get(user_id) if recent_events is None: recent_events = { \"events\": [], \"event_counts\": {}, \"last_updated\": current_time } # Add new event to the list event_type = event.get(\"event_type\", \"unknown\") recent_events[\"events\"].append({ \"timestamp\": current_time, \"type\": event_type, \"properties\": event.get(\"properties\", {}) }) # Update event counts if event_type in recent_events[\"event_counts\"]: recent_events[\"event_counts\"][event_type] += 1 else: recent_events[\"event_counts\"][event_type] = 1 # Keep only the most recent 100 events if len(recent_events[\"events\"]) > 100: recent_events[\"events\"] = recent_events[\"events\"][-100:] # Update last updated timestamp recent_events[\"last_updated\"] = current_time # Store updated events recent_events_store.put(user_id, recent_events) # Don't emit anything from this function return None stores: - recent_events_store update_user_profile: type: valueTransformer code: | user_id = key profile_update = value # Get existing profile existing_profile = user_profile_store.get(user_id) if existing_profile is None: existing_profile = { \"created_at\": int(time.time() * 1000), \"attributes\": {} } # Update profile with new data if \"attributes\" in profile_update: for attr_key, attr_value in profile_update[\"attributes\"].items(): existing_profile[\"attributes\"][attr_key] = attr_value # Add metadata existing_profile[\"last_updated\"] = int(time.time() * 1000) # Store updated profile user_profile_store.put(user_id, existing_profile) # Don't emit anything from this function return None stores: - user_profile_store generate_user_insights: type: valueTransformer code: | user_id = key trigger_event = value # This could be any event that triggers insight generation current_time = int(time.time() * 1000) # Get user's recent events recent_events = recent_events_store.get(user_id) if recent_events is None or not recent_events.get(\"events\"): return None # No events to analyze # Get user profile user_profile = user_profile_store.get(user_id) if user_profile is None: user_profile = {\"attributes\": {}} # Get behavior metrics behavior_metrics = behavior_metrics_store.get(user_id) if behavior_metrics is None: behavior_metrics = { \"session_count\": 0, \"total_time_spent\": 0, \"average_session_length\": 0, \"first_seen\": current_time, \"last_seen\": current_time } # Analyze events to generate insights events = recent_events.get(\"events\", []) event_counts = recent_events.get(\"event_counts\", {}) # Calculate recency (days since last activity) last_activity = max(event[\"timestamp\"] for event in events) if events else current_time recency_days = (current_time - last_activity) / (24 * 60 * 60 * 1000) # Calculate frequency (number of events in last 30 days) thirty_days_ago = current_time - (30 * 24 * 60 * 60 * 1000) recent_event_count = sum(1 for event in events if event[\"timestamp\"] > thirty_days_ago) # Calculate engagement score engagement_score = calculate_engagement_score(events, user_profile) # Update behavior metrics behavior_metrics[\"last_seen\"] = last_activity behavior_metrics[\"event_count_30d\"] = recent_event_count behavior_metrics[\"recency_days\"] = recency_days behavior_metrics[\"engagement_score\"] = engagement_score # Store updated metrics behavior_metrics_store.put(user_id, behavior_metrics) # Generate insights insights = { \"user_id\": user_id, \"timestamp\": current_time, \"recency_days\": recency_days, \"frequency_30d\": recent_event_count, \"engagement_score\": engagement_score, \"top_activities\": get_top_activities(event_counts), \"user_segment\": determine_user_segment(recency_days, recent_event_count, engagement_score), \"recommendations\": generate_recommendations(events, user_profile) } return insights stores: - recent_events_store - user_profile_store - behavior_metrics_store globalCode: | def calculate_engagement_score(events, user_profile): # Implementation of engagement scoring algorithm if not events: return 0 # Simple scoring based on recency and frequency recent_events = [e for e in events if e[\"timestamp\"] > (int(time.time() * 1000) - 7 * 24 * 60 * 60 * 1000)] score = len(recent_events) * 10 # Bonus for high-value activities for event in recent_events: if event[\"type\"] in [\"purchase\", \"subscription\", \"share\"]: score += 20 return min(100, score) def get_top_activities(event_counts): # Return top 5 activities by count sorted_activities = sorted(event_counts.items(), key=lambda x: x[1], reverse=True) return dict(sorted_activities[:5]) def determine_user_segment(recency, frequency, engagement): # Segment users based on RFE (Recency, Frequency, Engagement) if recency < 7 and frequency > 10 and engagement > 70: return \"highly_engaged\" elif recency < 30 and frequency > 5 and engagement > 40: return \"engaged\" elif recency < 90: return \"casual\" else: return \"inactive\" def generate_recommendations(events, user_profile): # Simple recommendation engine based on user activity recommendations = [] event_types = set(event[\"type\"] for event in events) if \"view_product\" in event_types and \"add_to_cart\" not in event_types: recommendations.append(\"complete_purchase\") if \"subscription_view\" in event_types and \"subscription\" not in event_types: recommendations.append(\"subscribe\") return recommendations[:3] # Return top 3 recommendations pipelines: # Process user events track_events: from: user_events mapValues: track_user_events filter: is_not_null to: tracked_events # Process profile updates update_profiles: from: user_profiles mapValues: update_user_profile filter: is_not_null to: updated_profiles # Generate insights (triggered by specific events) filter_trigger_events: from: user_events filter: is_insight_trigger_event to: insight_trigger_events # Process trigger events to generate insights generate_insights: from: insight_trigger_events mapValues: generate_user_insights filter: is_not_null to: user_insights This example: 1. Tracks user events in a memory-efficient way, keeping only the 100 most recent events 2. Maintains user profiles with attribute updates 3. Generates behavioral insights using data from multiple state stores 4. Implements sophisticated analytics like engagement scoring and user segmentation 5. Uses optimized state stores with appropriate caching and retention policies","title":"Practical Example: Advanced User Behavior Tracking"},{"location":"tutorials/advanced/custom-state-stores/#advanced-state-store-patterns","text":"","title":"Advanced State Store Patterns"},{"location":"tutorials/advanced/custom-state-stores/#1-tiered-storage","text":"Implement tiered storage for different access patterns: stores: hot_data_store: type: keyValue keyType: string valueType: json persistent: false # In-memory only caching: true warm_data_store: type: keyValue keyType: string valueType: json persistent: true caching: true cold_data_store: type: keyValue keyType: string valueType: json persistent: true caching: false","title":"1. Tiered Storage"},{"location":"tutorials/advanced/custom-state-stores/#2-read-throughwrite-through-caching","text":"Implement patterns to synchronize state with external systems: functions: cache_with_external_system: type: valueTransformer code: | # Try to get from local cache first cached_value = local_cache_store.get(key) if cached_value is None: # Cache miss - fetch from external system external_value = fetch_from_external_system(key) # Update local cache if external_value is not None: local_cache_store.put(key, external_value) return process_data(value, external_value) else: # Cache hit return process_data(value, cached_value) stores: - local_cache_store","title":"2. Read-Through/Write-Through Caching"},{"location":"tutorials/advanced/custom-state-stores/#3-composite-state-patterns","text":"Combine multiple state stores for complex use cases: functions: use_composite_state: type: valueTransformer code: | # Get data from multiple stores recent_data = recent_store.get(key) historical_data = historical_store.get(key) metadata = metadata_store.get(key) # Combine data for processing combined_state = { \"recent\": recent_data, \"historical\": historical_data, \"metadata\": metadata } # Process with combined state result = process_with_combined_state(value, combined_state) # Update stores as needed recent_store.put(key, result.get(\"recent\")) metadata_store.put(key, result.get(\"metadata\")) return result.get(\"output\") stores: - recent_store - historical_store - metadata_store","title":"3. Composite State Patterns"},{"location":"tutorials/advanced/custom-state-stores/#best-practices-for-custom-state-stores","text":"","title":"Best Practices for Custom State Stores"},{"location":"tutorials/advanced/custom-state-stores/#performance-considerations","text":"Memory Management : Monitor and control memory usage, especially for large state stores Serialization Overhead : Be aware of serialization/deserialization costs for frequent state access Caching Strategy : Configure appropriate cache sizes based on access patterns Persistence Impact : Understand the performance implications of persistent vs. in-memory stores","title":"Performance Considerations"},{"location":"tutorials/advanced/custom-state-stores/#design-patterns","text":"State Partitioning : Partition large states to improve scalability Minimal State : Store only what's necessary to reduce resource usage Batched Operations : Group state operations to improve efficiency Incremental Updates : Update state incrementally rather than replacing entire objects","title":"Design Patterns"},{"location":"tutorials/advanced/custom-state-stores/#error-handling","text":"Implement robust error handling for state store operations: functions: robust_state_access: type: valueTransformer code: | try: # Access state store stored_value = my_store.get(key) # Process with state result = process_with_state(value, stored_value) # Update state my_store.put(key, updated_state) return result except Exception as e: log.error(\"Error accessing state store: {}\", str(e)) # Return fallback value or original input return value stores: - my_store","title":"Error Handling"},{"location":"tutorials/advanced/custom-state-stores/#conclusion","text":"Custom state stores in KSML provide powerful capabilities for implementing sophisticated stateful stream processing applications. By understanding how to implement, optimize, and manage state stores, you can build applications that efficiently maintain state across events and deliver complex business logic. In the next tutorial, we'll explore Performance Optimization to learn how to tune and optimize your KSML applications for maximum throughput and minimal resource usage.","title":"Conclusion"},{"location":"tutorials/advanced/custom-state-stores/#further-reading","text":"Core Concepts: Operations Core Concepts: Functions Intermediate Tutorial: Aggregations Reference: State Stores","title":"Further Reading"},{"location":"tutorials/advanced/external-integration/","text":"Integration with External Systems in KSML This tutorial explores how to integrate KSML applications with external systems such as databases, REST APIs, message queues, and other services, allowing you to build comprehensive data processing solutions that connect to your entire technology ecosystem. Introduction to External Integration While Kafka Streams and KSML excel at processing data within Kafka, real-world applications often need to interact with external systems to: Enrich streaming data with reference information from databases Persist processed results to data warehouses or databases Call external APIs to trigger actions or fetch additional data Integrate with legacy systems or third-party services Implement the Kappa architecture pattern (using Kafka as the system of record) This tutorial covers various approaches to integrate KSML applications with external systems while maintaining the reliability, scalability, and fault-tolerance of your stream processing applications. Prerequisites Before starting this tutorial, you should: Understand intermediate KSML concepts (streams, functions, pipelines) Have completed the Custom State Stores tutorial Be familiar with Error Handling techniques Have a basic understanding of external systems you want to integrate with Integration Patterns 1. Request-Response Pattern The request-response pattern involves making synchronous calls to external systems from within your KSML functions: functions: enrich_with_api_data: type: valueTransformer globalCode: | import requests import json # Configure API client API_BASE_URL = \"https://api.example.com/v1\" API_KEY = \"your-api-key\" def get_api_data(entity_id): try: response = requests.get( f\"{API_BASE_URL}/entities/{entity_id}\", headers={\"Authorization\": f\"Bearer {API_KEY}\"}, timeout=2.0 # Set a reasonable timeout ) response.raise_for_status() # Raise exception for HTTP errors return response.json() except Exception as e: log.warn(\"API request failed: {}\", str(e)) return None code: | # Extract entity ID from the message entity_id = value.get(\"entity_id\") if not entity_id: return value # No enrichment possible # Call the API to get additional data api_data = get_api_data(entity_id) # Enrich the message with API data if api_data: value[\"enriched_data\"] = api_data value[\"enrichment_status\"] = \"success\" else: value[\"enrichment_status\"] = \"failed\" return value Best Practices for Request-Response Implement timeouts : Prevent blocking the processing pipeline Use connection pooling : Reuse connections for better performance Implement circuit breakers : Prevent cascading failures when external systems are down Cache responses : Reduce the load on external systems Handle errors gracefully : Implement fallback strategies when external systems fail 2. Lookup Pattern The lookup pattern uses state stores to cache reference data from external systems: functions: load_reference_data: type: valueTransformer globalCode: | import psycopg2 import json from datetime import datetime # Database connection parameters DB_PARAMS = { \"host\": \"db.example.com\", \"database\": \"reference_data\", \"user\": \"app_user\", \"password\": \"app_password\" } def load_product_data(): \"\"\"Load product data from database into a dictionary\"\"\" products = {} try: conn = psycopg2.connect(**DB_PARAMS) cursor = conn.cursor() cursor.execute(\"SELECT product_id, name, category, price FROM products\") for row in cursor.fetchall(): product_id, name, category, price = row products[product_id] = { \"name\": name, \"category\": category, \"price\": float(price), \"loaded_at\": datetime.now().isoformat() } cursor.close() conn.close() return products except Exception as e: log.error(\"Failed to load product data: {}\", str(e)) return {} code: | # This function runs once when the topology is built # Load reference data into the state store products = load_product_data() log.info(\"Loaded {} products into reference data store\", len(products)) for product_id, product_data in products.items(): reference_data_store.put(product_id, product_data) # Return the input unchanged - this function is just for initialization return value stores: - reference_data_store enrich_with_product_data: type: valueTransformer code: | # Extract product ID from the message product_id = value.get(\"product_id\") if not product_id: return value # No enrichment possible # Look up product data from the state store product_data = reference_data_store.get(product_id) # Enrich the message with product data if product_data: value[\"product_name\"] = product_data.get(\"name\") value[\"product_category\"] = product_data.get(\"category\") value[\"product_price\"] = product_data.get(\"price\") value[\"enrichment_status\"] = \"success\" else: value[\"enrichment_status\"] = \"not_found\" return value stores: - reference_data_store Best Practices for Lookup Pattern Periodic refreshes : Implement a mechanism to periodically refresh the cached data Incremental updates : Consider using CDC (Change Data Capture) to keep the cache up-to-date Memory management : Be mindful of the size of the cached data Fallback strategies : Implement fallback strategies when data is not found in the cache 3. Async Integration Pattern The async integration pattern uses separate Kafka topics for communication with external systems: streams: input_events: topic: app_events keyType: string valueType: json db_write_requests: topic: db_write_requests keyType: string valueType: json db_write_responses: topic: db_write_responses keyType: string valueType: json functions: prepare_db_write: type: keyValueTransformer code: | # Create a write request for the database service request_id = str(uuid.uuid4()) write_request = { \"request_id\": request_id, \"table\": \"user_activities\", \"operation\": \"insert\", \"timestamp\": int(time.time() * 1000), \"data\": { \"user_id\": key, \"activity_type\": value.get(\"type\"), \"activity_data\": value } } # Return the request with the request ID as the key return (request_id, write_request) process_db_response: type: valueTransformer code: | # Process the database write response request_id = value.get(\"request_id\") status = value.get(\"status\") if status == \"success\": log.info(\"Database write successful for request {}\", request_id) else: log.error(\"Database write failed for request {}: {}\", request_id, value.get(\"error\")) # This could update a state store with the status if needed return value pipelines: # Send write requests to the database service send_to_database: from: input_events transformKeyValue: prepare_db_write to: db_write_requests # Process responses from the database service process_responses: from: db_write_responses mapValues: process_db_response to: db_write_status This pattern requires a separate service that: Consumes from the db_write_requests topic Performs the database operations Produces results to the db_write_responses topic Best Practices for Async Integration Correlation IDs : Use unique IDs to correlate requests and responses Idempotent operations : Ensure that operations can be safely retried Dead letter queues : Implement DLQs for failed operations Monitoring : Monitor the request and response topics for backpressure 4. Database Integration with JDBC For direct database integration, you can use JDBC within your KSML functions: functions: persist_to_database: type: forEach globalCode: | import java.sql.Connection import java.sql.DriverManager import java.sql.PreparedStatement import java.sql.SQLException # JDBC connection parameters JDBC_URL = \"jdbc:postgresql://db.example.com:5432/app_db\" DB_USER = \"app_user\" DB_PASSWORD = \"app_password\" # Connection pool connection_pool = None def get_connection(): try: return DriverManager.getConnection(JDBC_URL, DB_USER, DB_PASSWORD) except SQLException as e: log.error(\"Database connection error: {}\", str(e)) raise e def insert_event(conn, user_id, event_type, event_data): try: sql = \"\"\" INSERT INTO user_events (user_id, event_type, event_data, created_at) VALUES (?, ?, ?, NOW()) \"\"\" stmt = conn.prepareStatement(sql) stmt.setString(1, user_id) stmt.setString(2, event_type) stmt.setString(3, json.dumps(event_data)) return stmt.executeUpdate() except SQLException as e: log.error(\"Database insert error: {}\", str(e)) raise e code: | try: # Get database connection conn = get_connection() # Insert the event into the database user_id = key event_type = value.get(\"type\", \"unknown\") rows_affected = insert_event(conn, user_id, event_type, value) log.info(\"Inserted event into database: {} rows affected\", rows_affected) # Close the connection conn.close() except Exception as e: log.error(\"Failed to persist event to database: {}\", str(e)) # Consider adding to a retry queue or dead letter queue Best Practices for JDBC Integration Use connection pooling : Manage database connections efficiently Batch operations : Group multiple operations for better performance Prepare statements : Use prepared statements to prevent SQL injection Transaction management : Use transactions for atomicity Error handling : Implement proper error handling and retries Practical Example: Multi-System Integration Let's build a complete example that integrates with multiple external systems: streams: order_events: topic: ecommerce_orders keyType: string # Order ID valueType: json # Order details payment_requests: topic: payment_requests keyType: string # Payment request ID valueType: json # Payment details payment_responses: topic: payment_responses keyType: string # Payment request ID valueType: json # Payment response inventory_requests: topic: inventory_requests keyType: string # Inventory request ID valueType: json # Inventory details inventory_responses: topic: inventory_responses keyType: string # Inventory request ID valueType: json # Inventory response order_status_updates: topic: order_status_updates keyType: string # Order ID valueType: json # Status update stores: # Store for product data product_reference_store: type: keyValue keyType: string # Product ID valueType: json # Product details persistent: true caching: true # Store for order status order_status_store: type: keyValue keyType: string # Order ID valueType: json # Order status persistent: true caching: true # Store for payment requests payment_request_store: type: keyValue keyType: string # Order ID valueType: json # Payment request details persistent: true caching: true functions: # Load product reference data from database load_product_data: type: valueTransformer globalCode: | import psycopg2 import json # Database connection parameters DB_PARAMS = { \"host\": \"db.example.com\", \"database\": \"product_catalog\", \"user\": \"app_user\", \"password\": \"app_password\" } def load_products(): products = {} try: conn = psycopg2.connect(**DB_PARAMS) cursor = conn.cursor() cursor.execute(\"\"\" SELECT product_id, name, price, stock_level, category FROM products WHERE active = TRUE \"\"\") for row in cursor.fetchall(): product_id, name, price, stock_level, category = row products[product_id] = { \"name\": name, \"price\": float(price), \"stock_level\": stock_level, \"category\": category, \"loaded_at\": int(time.time() * 1000) } cursor.close() conn.close() return products except Exception as e: log.error(\"Failed to load product data: {}\", str(e)) return {} code: | # Load product data into the reference store products = load_products() log.info(\"Loaded {} products into reference store\", len(products)) for product_id, product_data in products.items(): product_reference_store.put(product_id, product_data) # Return the input unchanged return value stores: - product_reference_store # Process new order process_order: type: keyValueTransformer code: | order_id = key order = value # Validate order items against product reference data valid_items = [] invalid_items = [] total_amount = 0 for item in order.get(\"items\", []): product_id = item.get(\"product_id\") quantity = item.get(\"quantity\", 0) if not product_id or quantity <= 0: invalid_items.append({ \"product_id\": product_id, \"reason\": \"Invalid product ID or quantity\" }) continue # Look up product details product = product_reference_store.get(product_id) if not product: invalid_items.append({ \"product_id\": product_id, \"reason\": \"Product not found\" }) continue # Calculate item price item_price = product.get(\"price\", 0) * quantity total_amount += item_price # Add to valid items valid_items.append({ \"product_id\": product_id, \"product_name\": product.get(\"name\"), \"quantity\": quantity, \"unit_price\": product.get(\"price\", 0), \"total_price\": item_price }) # Create validated order validated_order = { \"order_id\": order_id, \"customer_id\": order.get(\"customer_id\"), \"order_date\": order.get(\"order_date\"), \"items\": valid_items, \"invalid_items\": invalid_items, \"total_amount\": total_amount, \"status\": \"validated\" if valid_items else \"invalid\", \"validation_timestamp\": int(time.time() * 1000) } # Store order status order_status_store.put(order_id, { \"status\": validated_order[\"status\"], \"timestamp\": validated_order[\"validation_timestamp\"] }) return (order_id, validated_order) stores: - product_reference_store - order_status_store # Create payment request create_payment_request: type: keyValueTransformer code: | order_id = key order = value # Only process validated orders with valid items if order.get(\"status\") != \"validated\" or not order.get(\"items\"): return None # Create payment request payment_request_id = str(uuid.uuid4()) payment_request = { \"request_id\": payment_request_id, \"order_id\": order_id, \"customer_id\": order.get(\"customer_id\"), \"amount\": order.get(\"total_amount\"), \"currency\": \"USD\", \"timestamp\": int(time.time() * 1000) } # Store payment request for correlation payment_request_store.put(order_id, { \"payment_request_id\": payment_request_id, \"timestamp\": payment_request[\"timestamp\"] }) # Update order status order_status = order_status_store.get(order_id) order_status[\"status\"] = \"payment_pending\" order_status[\"payment_request_id\"] = payment_request_id order_status_store.put(order_id, order_status) return (payment_request_id, payment_request) stores: - payment_request_store - order_status_store # Process payment response process_payment_response: type: keyValueTransformer code: | payment_request_id = key payment_response = value # Extract data from payment response order_id = payment_response.get(\"order_id\") status = payment_response.get(\"status\") if not order_id: log.error(\"Payment response missing order ID: {}\", payment_request_id) return None # Get current order status order_status = order_status_store.get(order_id) if not order_status: log.error(\"Order not found for payment response: {}\", order_id) return None # Update order status based on payment result if status == \"approved\": order_status[\"status\"] = \"paid\" order_status[\"payment_timestamp\"] = payment_response.get(\"timestamp\") order_status[\"payment_reference\"] = payment_response.get(\"reference\") else: order_status[\"status\"] = \"payment_failed\" order_status[\"payment_error\"] = payment_response.get(\"error\") # Store updated status order_status_store.put(order_id, order_status) # Create order status update message status_update = { \"order_id\": order_id, \"status\": order_status[\"status\"], \"timestamp\": int(time.time() * 1000), \"payment_status\": status, \"payment_reference\": payment_response.get(\"reference\"), \"payment_error\": payment_response.get(\"error\") } return (order_id, status_update) stores: - order_status_store # Create inventory request for paid orders create_inventory_request: type: keyValueTransformer code: | order_id = key status_update = value # Only process paid orders if status_update.get(\"status\") != \"paid\": return None # Get the original order # In a real system, you might need to fetch this from a database or state store # For simplicity, we'll assume we have the order details in a state store order = order_status_store.get(order_id) if not order or \"items\" not in order: log.error(\"Order details not found for inventory request: {}\", order_id) return None # Create inventory request inventory_request_id = str(uuid.uuid4()) inventory_request = { \"request_id\": inventory_request_id, \"order_id\": order_id, \"items\": order.get(\"items\", []), \"timestamp\": int(time.time() * 1000) } # Update order status order_status = order_status_store.get(order_id) order_status[\"status\"] = \"inventory_pending\" order_status[\"inventory_request_id\"] = inventory_request_id order_status_store.put(order_id, order_status) return (inventory_request_id, inventory_request) stores: - order_status_store # Process inventory response process_inventory_response: type: keyValueTransformer code: | inventory_request_id = key inventory_response = value # Extract data from inventory response order_id = inventory_response.get(\"order_id\") status = inventory_response.get(\"status\") if not order_id: log.error(\"Inventory response missing order ID: {}\", inventory_request_id) return None # Get current order status order_status = order_status_store.get(order_id) if not order_status: log.error(\"Order not found for inventory response: {}\", order_id) return None # Update order status based on inventory result if status == \"fulfilled\": order_status[\"status\"] = \"ready_for_shipment\" order_status[\"inventory_timestamp\"] = inventory_response.get(\"timestamp\") order_status[\"warehouse_id\"] = inventory_response.get(\"warehouse_id\") else: order_status[\"status\"] = \"inventory_failed\" order_status[\"inventory_error\"] = inventory_response.get(\"error\") # Store updated status order_status_store.put(order_id, order_status) # Create order status update message status_update = { \"order_id\": order_id, \"status\": order_status[\"status\"], \"timestamp\": int(time.time() * 1000), \"inventory_status\": status, \"warehouse_id\": inventory_response.get(\"warehouse_id\"), \"inventory_error\": inventory_response.get(\"error\") } return (order_id, status_update) stores: - order_status_store # Send shipment notification via REST API send_shipment_notification: type: forEach globalCode: | import requests import json # API configuration NOTIFICATION_API_URL = \"https://notifications.example.com/api/v1/shipment\" API_KEY = \"your-api-key\" def send_notification(order_id, customer_id, status): try: payload = { \"order_id\": order_id, \"customer_id\": customer_id, \"status\": status, \"timestamp\": int(time.time() * 1000) } response = requests.post( NOTIFICATION_API_URL, headers={ \"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {API_KEY}\" }, data=json.dumps(payload), timeout=5.0 ) response.raise_for_status() return True except Exception as e: log.error(\"Failed to send notification for order {}: {}\", order_id, str(e)) return False code: | order_id = key status_update = value # Only send notifications for orders ready for shipment if status_update.get(\"status\") != \"ready_for_shipment\": return # Send notification customer_id = status_update.get(\"customer_id\") success = send_notification(order_id, customer_id, \"ready_for_shipment\") if success: log.info(\"Sent shipment notification for order {}\", order_id) else: log.warn(\"Failed to send shipment notification for order {}\", order_id) pipelines: # Initialize reference data load_reference_data: from: reference_data_trigger mapValues: load_product_data to: reference_data_loaded # Process new orders process_orders: from: order_events transformKeyValue: process_order to: validated_orders # Create payment requests request_payments: from: validated_orders transformKeyValue: create_payment_request filter: is_not_null to: payment_requests # Process payment responses handle_payments: from: payment_responses transformKeyValue: process_payment_response filter: is_not_null to: order_status_updates # Create inventory requests request_inventory: from: order_status_updates transformKeyValue: create_inventory_request filter: is_not_null to: inventory_requests # Process inventory responses handle_inventory: from: inventory_responses transformKeyValue: process_inventory_response filter: is_not_null to: order_status_updates # Send notifications send_notifications: from: order_status_updates forEach: send_shipment_notification This example: Loads product reference data from a database Processes incoming orders and validates them against the reference data Creates payment requests and sends them to a payment service Processes payment responses and updates order status Creates inventory requests for paid orders Processes inventory responses and updates order status Sends shipment notifications via a REST API for orders ready for shipment Integration with Specific External Systems 1. Relational Databases For integrating with relational databases: functions: database_integration: type: valueTransformer globalCode: | import psycopg2 from psycopg2.extras import RealDictCursor import json from contextlib import contextmanager # Database connection parameters DB_PARAMS = { \"host\": \"db.example.com\", \"database\": \"app_db\", \"user\": \"app_user\", \"password\": \"app_password\" } @contextmanager def get_db_connection(): \"\"\"Context manager for database connections\"\"\" conn = None try: conn = psycopg2.connect(**DB_PARAMS) yield conn except Exception as e: log.error(\"Database connection error: {}\", str(e)) raise finally: if conn is not None: conn.close() def query_user_data(user_id): \"\"\"Query user data from the database\"\"\" with get_db_connection() as conn: with conn.cursor(cursor_factory=RealDictCursor) as cursor: cursor.execute(\"\"\" SELECT user_id, name, email, account_type, created_at FROM users WHERE user_id = %s \"\"\", (user_id,)) result = cursor.fetchone() return dict(result) if result else None def insert_user_activity(user_id, activity_type, activity_data): \"\"\"Insert user activity into the database\"\"\" with get_db_connection() as conn: with conn.cursor() as cursor: cursor.execute(\"\"\" INSERT INTO user_activities (user_id, activity_type, activity_data, created_at) VALUES (%s, %s, %s, NOW()) RETURNING id \"\"\", (user_id, activity_type, json.dumps(activity_data))) activity_id = cursor.fetchone()[0] conn.commit() return activity_id code: | user_id = key # Query user data user_data = query_user_data(user_id) if not user_data: log.warn(\"User not found: {}\", user_id) return value # Enrich message with user data value[\"user_name\"] = user_data.get(\"name\") value[\"user_email\"] = user_data.get(\"email\") value[\"user_account_type\"] = user_data.get(\"account_type\") # Record activity in database try: activity_id = insert_user_activity( user_id, value.get(\"activity_type\", \"unknown\"), value ) value[\"activity_id\"] = activity_id except Exception as e: log.error(\"Failed to record activity: {}\", str(e)) return value 2. NoSQL Databases For integrating with NoSQL databases like MongoDB: functions: mongodb_integration: type: valueTransformer globalCode: | from pymongo import MongoClient import json # MongoDB connection parameters MONGO_URI = \"mongodb://user:password@mongodb.example.com:27017/app_db\" # Create a MongoDB client client = MongoClient(MONGO_URI) db = client.get_database() def get_document(collection, query): \"\"\"Get a document from MongoDB\"\"\" try: result = db[collection].find_one(query) return result except Exception as e: log.error(\"MongoDB query error: {}\", str(e)) return None def insert_document(collection, document): \"\"\"Insert a document into MongoDB\"\"\" try: result = db[collection].insert_one(document) return str(result.inserted_id) except Exception as e: log.error(\"MongoDB insert error: {}\", str(e)) return None code: | # Get product data from MongoDB product_id = value.get(\"product_id\") if product_id: product = get_document(\"products\", {\"_id\": product_id}) if product: # Enrich message with product data value[\"product_name\"] = product.get(\"name\") value[\"product_category\"] = product.get(\"category\") value[\"product_price\"] = product.get(\"price\") # Store the event in MongoDB event_id = insert_document(\"events\", { \"user_id\": key, \"event_type\": value.get(\"type\"), \"timestamp\": value.get(\"timestamp\"), \"data\": value }) if event_id: value[\"event_id\"] = event_id return value 3. REST APIs For integrating with REST APIs: functions: rest_api_integration: type: valueTransformer globalCode: | import requests import json from cachetools import TTLCache # API configuration API_BASE_URL = \"https://api.example.com/v1\" API_KEY = \"your-api-key\" # Create a cache with TTL of 5 minutes cache = TTLCache(maxsize=1000, ttl=300) def get_api_data(endpoint, params=None): \"\"\"Get data from the API with caching\"\"\" # Create cache key cache_key = f\"{endpoint}:{json.dumps(params or {})}\" # Check cache if cache_key in cache: return cache[cache_key] # Make API request try: response = requests.get( f\"{API_BASE_URL}/{endpoint}\", params=params, headers={ \"Authorization\": f\"Bearer {API_KEY}\", \"Accept\": \"application/json\" }, timeout=5.0 ) response.raise_for_status() result = response.json() # Cache the result cache[cache_key] = result return result except Exception as e: log.error(\"API request failed: {}\", str(e)) return None def post_api_data(endpoint, data): \"\"\"Post data to the API\"\"\" try: response = requests.post( f\"{API_BASE_URL}/{endpoint}\", json=data, headers={ \"Authorization\": f\"Bearer {API_KEY}\", \"Content-Type\": \"application/json\" }, timeout=5.0 ) response.raise_for_status() return response.json() except Exception as e: log.error(\"API post failed: {}\", str(e)) return None code: | # Get weather data for the user's location location = value.get(\"location\") if location: weather_data = get_api_data(\"weather\", { \"lat\": location.get(\"latitude\"), \"lon\": location.get(\"longitude\") }) if weather_data: value[\"weather\"] = { \"temperature\": weather_data.get(\"temperature\"), \"conditions\": weather_data.get(\"conditions\"), \"forecast\": weather_data.get(\"forecast\") } # Send analytics event to API analytics_result = post_api_data(\"analytics/events\", { \"user_id\": key, \"event_type\": value.get(\"type\"), \"timestamp\": value.get(\"timestamp\"), \"properties\": value }) if analytics_result: value[\"analytics_id\"] = analytics_result.get(\"id\") return value 4. Message Queues For integrating with message queues like RabbitMQ: functions: rabbitmq_integration: type: forEach globalCode: | import pika import json # RabbitMQ connection parameters RABBITMQ_PARAMS = { \"host\": \"rabbitmq.example.com\", \"port\": 5672, \"virtual_host\": \"/\", \"credentials\": pika.PlainCredentials(\"user\", \"password\") } # Create a connection and channel connection = None channel = None def get_channel(): global connection, channel if connection is None or not connection.is_open: connection = pika.BlockingConnection( pika.ConnectionParameters(**RABBITMQ_PARAMS) ) channel = connection.channel() # Declare queues channel.queue_declare(queue=\"notifications\", durable=True) channel.queue_declare(queue=\"alerts\", durable=True) return channel def send_to_queue(queue, message): \"\"\"Send a message to a RabbitMQ queue\"\"\" try: ch = get_channel() ch.basic_publish( exchange=\"\", routing_key=queue, body=json.dumps(message), properties=pika.BasicProperties( delivery_mode=2, # Make message persistent content_type=\"application/json\" ) ) return True except Exception as e: log.error(\"Failed to send message to RabbitMQ: {}\", str(e)) return False code: | # Determine which queue to use based on message type message_type = value.get(\"type\", \"unknown\") if message_type in [\"alert\", \"error\", \"warning\"]: queue = \"alerts\" else: queue = \"notifications\" # Prepare message message = { \"user_id\": key, \"type\": message_type, \"timestamp\": value.get(\"timestamp\", int(time.time() * 1000)), \"content\": value.get(\"content\"), \"priority\": value.get(\"priority\", \"normal\") } # Send to RabbitMQ success = send_to_queue(queue, message) if success: log.info(\"Sent message to RabbitMQ queue {}: {}\", queue, message_type) else: log.error(\"Failed to send message to RabbitMQ queue {}: {}\", queue, message_type) Best Practices for External Integration Performance and Reliability Connection pooling : Reuse connections to external systems Timeouts : Implement appropriate timeouts for external calls Circuit breakers : Prevent cascading failures when external systems are down Retries with backoff : Implement retries with exponential backoff for transient failures Idempotent operations : Ensure that operations can be safely retried Data Consistency Transactions : Use transactions when appropriate to ensure data consistency Correlation IDs : Use correlation IDs to track operations across systems Idempotency keys : Use idempotency keys to prevent duplicate processing Compensating transactions : Implement compensating transactions for rollbacks Security Secure credentials : Store credentials securely and rotate them regularly TLS/SSL : Use secure connections for all external communication Authentication : Implement proper authentication for all external systems Authorization : Ensure proper authorization for all operations Data encryption : Encrypt sensitive data in transit and at rest Monitoring and Observability Logging : Log all external interactions with appropriate context Metrics : Track performance metrics for external calls Tracing : Implement distributed tracing for end-to-end visibility Alerting : Set up alerts for abnormal patterns or failures Conclusion Integrating KSML applications with external systems allows you to build comprehensive data processing solutions that connect to your entire technology ecosystem. By using the patterns and techniques covered in this tutorial, you can implement reliable, scalable, and maintainable integrations with databases, APIs, message queues, and other external systems. In the next tutorial, we'll explore Advanced Error Handling to learn sophisticated techniques for handling errors and implementing recovery mechanisms in complex KSML applications. Further Reading Core Concepts: Operations Core Concepts: Functions Intermediate Tutorial: Error Handling Reference: Configuration Options","title":"Integration with External Systems in KSML"},{"location":"tutorials/advanced/external-integration/#integration-with-external-systems-in-ksml","text":"This tutorial explores how to integrate KSML applications with external systems such as databases, REST APIs, message queues, and other services, allowing you to build comprehensive data processing solutions that connect to your entire technology ecosystem.","title":"Integration with External Systems in KSML"},{"location":"tutorials/advanced/external-integration/#introduction-to-external-integration","text":"While Kafka Streams and KSML excel at processing data within Kafka, real-world applications often need to interact with external systems to: Enrich streaming data with reference information from databases Persist processed results to data warehouses or databases Call external APIs to trigger actions or fetch additional data Integrate with legacy systems or third-party services Implement the Kappa architecture pattern (using Kafka as the system of record) This tutorial covers various approaches to integrate KSML applications with external systems while maintaining the reliability, scalability, and fault-tolerance of your stream processing applications.","title":"Introduction to External Integration"},{"location":"tutorials/advanced/external-integration/#prerequisites","text":"Before starting this tutorial, you should: Understand intermediate KSML concepts (streams, functions, pipelines) Have completed the Custom State Stores tutorial Be familiar with Error Handling techniques Have a basic understanding of external systems you want to integrate with","title":"Prerequisites"},{"location":"tutorials/advanced/external-integration/#integration-patterns","text":"","title":"Integration Patterns"},{"location":"tutorials/advanced/external-integration/#1-request-response-pattern","text":"The request-response pattern involves making synchronous calls to external systems from within your KSML functions: functions: enrich_with_api_data: type: valueTransformer globalCode: | import requests import json # Configure API client API_BASE_URL = \"https://api.example.com/v1\" API_KEY = \"your-api-key\" def get_api_data(entity_id): try: response = requests.get( f\"{API_BASE_URL}/entities/{entity_id}\", headers={\"Authorization\": f\"Bearer {API_KEY}\"}, timeout=2.0 # Set a reasonable timeout ) response.raise_for_status() # Raise exception for HTTP errors return response.json() except Exception as e: log.warn(\"API request failed: {}\", str(e)) return None code: | # Extract entity ID from the message entity_id = value.get(\"entity_id\") if not entity_id: return value # No enrichment possible # Call the API to get additional data api_data = get_api_data(entity_id) # Enrich the message with API data if api_data: value[\"enriched_data\"] = api_data value[\"enrichment_status\"] = \"success\" else: value[\"enrichment_status\"] = \"failed\" return value","title":"1. Request-Response Pattern"},{"location":"tutorials/advanced/external-integration/#best-practices-for-request-response","text":"Implement timeouts : Prevent blocking the processing pipeline Use connection pooling : Reuse connections for better performance Implement circuit breakers : Prevent cascading failures when external systems are down Cache responses : Reduce the load on external systems Handle errors gracefully : Implement fallback strategies when external systems fail","title":"Best Practices for Request-Response"},{"location":"tutorials/advanced/external-integration/#2-lookup-pattern","text":"The lookup pattern uses state stores to cache reference data from external systems: functions: load_reference_data: type: valueTransformer globalCode: | import psycopg2 import json from datetime import datetime # Database connection parameters DB_PARAMS = { \"host\": \"db.example.com\", \"database\": \"reference_data\", \"user\": \"app_user\", \"password\": \"app_password\" } def load_product_data(): \"\"\"Load product data from database into a dictionary\"\"\" products = {} try: conn = psycopg2.connect(**DB_PARAMS) cursor = conn.cursor() cursor.execute(\"SELECT product_id, name, category, price FROM products\") for row in cursor.fetchall(): product_id, name, category, price = row products[product_id] = { \"name\": name, \"category\": category, \"price\": float(price), \"loaded_at\": datetime.now().isoformat() } cursor.close() conn.close() return products except Exception as e: log.error(\"Failed to load product data: {}\", str(e)) return {} code: | # This function runs once when the topology is built # Load reference data into the state store products = load_product_data() log.info(\"Loaded {} products into reference data store\", len(products)) for product_id, product_data in products.items(): reference_data_store.put(product_id, product_data) # Return the input unchanged - this function is just for initialization return value stores: - reference_data_store enrich_with_product_data: type: valueTransformer code: | # Extract product ID from the message product_id = value.get(\"product_id\") if not product_id: return value # No enrichment possible # Look up product data from the state store product_data = reference_data_store.get(product_id) # Enrich the message with product data if product_data: value[\"product_name\"] = product_data.get(\"name\") value[\"product_category\"] = product_data.get(\"category\") value[\"product_price\"] = product_data.get(\"price\") value[\"enrichment_status\"] = \"success\" else: value[\"enrichment_status\"] = \"not_found\" return value stores: - reference_data_store","title":"2. Lookup Pattern"},{"location":"tutorials/advanced/external-integration/#best-practices-for-lookup-pattern","text":"Periodic refreshes : Implement a mechanism to periodically refresh the cached data Incremental updates : Consider using CDC (Change Data Capture) to keep the cache up-to-date Memory management : Be mindful of the size of the cached data Fallback strategies : Implement fallback strategies when data is not found in the cache","title":"Best Practices for Lookup Pattern"},{"location":"tutorials/advanced/external-integration/#3-async-integration-pattern","text":"The async integration pattern uses separate Kafka topics for communication with external systems: streams: input_events: topic: app_events keyType: string valueType: json db_write_requests: topic: db_write_requests keyType: string valueType: json db_write_responses: topic: db_write_responses keyType: string valueType: json functions: prepare_db_write: type: keyValueTransformer code: | # Create a write request for the database service request_id = str(uuid.uuid4()) write_request = { \"request_id\": request_id, \"table\": \"user_activities\", \"operation\": \"insert\", \"timestamp\": int(time.time() * 1000), \"data\": { \"user_id\": key, \"activity_type\": value.get(\"type\"), \"activity_data\": value } } # Return the request with the request ID as the key return (request_id, write_request) process_db_response: type: valueTransformer code: | # Process the database write response request_id = value.get(\"request_id\") status = value.get(\"status\") if status == \"success\": log.info(\"Database write successful for request {}\", request_id) else: log.error(\"Database write failed for request {}: {}\", request_id, value.get(\"error\")) # This could update a state store with the status if needed return value pipelines: # Send write requests to the database service send_to_database: from: input_events transformKeyValue: prepare_db_write to: db_write_requests # Process responses from the database service process_responses: from: db_write_responses mapValues: process_db_response to: db_write_status This pattern requires a separate service that: Consumes from the db_write_requests topic Performs the database operations Produces results to the db_write_responses topic","title":"3. Async Integration Pattern"},{"location":"tutorials/advanced/external-integration/#best-practices-for-async-integration","text":"Correlation IDs : Use unique IDs to correlate requests and responses Idempotent operations : Ensure that operations can be safely retried Dead letter queues : Implement DLQs for failed operations Monitoring : Monitor the request and response topics for backpressure","title":"Best Practices for Async Integration"},{"location":"tutorials/advanced/external-integration/#4-database-integration-with-jdbc","text":"For direct database integration, you can use JDBC within your KSML functions: functions: persist_to_database: type: forEach globalCode: | import java.sql.Connection import java.sql.DriverManager import java.sql.PreparedStatement import java.sql.SQLException # JDBC connection parameters JDBC_URL = \"jdbc:postgresql://db.example.com:5432/app_db\" DB_USER = \"app_user\" DB_PASSWORD = \"app_password\" # Connection pool connection_pool = None def get_connection(): try: return DriverManager.getConnection(JDBC_URL, DB_USER, DB_PASSWORD) except SQLException as e: log.error(\"Database connection error: {}\", str(e)) raise e def insert_event(conn, user_id, event_type, event_data): try: sql = \"\"\" INSERT INTO user_events (user_id, event_type, event_data, created_at) VALUES (?, ?, ?, NOW()) \"\"\" stmt = conn.prepareStatement(sql) stmt.setString(1, user_id) stmt.setString(2, event_type) stmt.setString(3, json.dumps(event_data)) return stmt.executeUpdate() except SQLException as e: log.error(\"Database insert error: {}\", str(e)) raise e code: | try: # Get database connection conn = get_connection() # Insert the event into the database user_id = key event_type = value.get(\"type\", \"unknown\") rows_affected = insert_event(conn, user_id, event_type, value) log.info(\"Inserted event into database: {} rows affected\", rows_affected) # Close the connection conn.close() except Exception as e: log.error(\"Failed to persist event to database: {}\", str(e)) # Consider adding to a retry queue or dead letter queue","title":"4. Database Integration with JDBC"},{"location":"tutorials/advanced/external-integration/#best-practices-for-jdbc-integration","text":"Use connection pooling : Manage database connections efficiently Batch operations : Group multiple operations for better performance Prepare statements : Use prepared statements to prevent SQL injection Transaction management : Use transactions for atomicity Error handling : Implement proper error handling and retries","title":"Best Practices for JDBC Integration"},{"location":"tutorials/advanced/external-integration/#practical-example-multi-system-integration","text":"Let's build a complete example that integrates with multiple external systems: streams: order_events: topic: ecommerce_orders keyType: string # Order ID valueType: json # Order details payment_requests: topic: payment_requests keyType: string # Payment request ID valueType: json # Payment details payment_responses: topic: payment_responses keyType: string # Payment request ID valueType: json # Payment response inventory_requests: topic: inventory_requests keyType: string # Inventory request ID valueType: json # Inventory details inventory_responses: topic: inventory_responses keyType: string # Inventory request ID valueType: json # Inventory response order_status_updates: topic: order_status_updates keyType: string # Order ID valueType: json # Status update stores: # Store for product data product_reference_store: type: keyValue keyType: string # Product ID valueType: json # Product details persistent: true caching: true # Store for order status order_status_store: type: keyValue keyType: string # Order ID valueType: json # Order status persistent: true caching: true # Store for payment requests payment_request_store: type: keyValue keyType: string # Order ID valueType: json # Payment request details persistent: true caching: true functions: # Load product reference data from database load_product_data: type: valueTransformer globalCode: | import psycopg2 import json # Database connection parameters DB_PARAMS = { \"host\": \"db.example.com\", \"database\": \"product_catalog\", \"user\": \"app_user\", \"password\": \"app_password\" } def load_products(): products = {} try: conn = psycopg2.connect(**DB_PARAMS) cursor = conn.cursor() cursor.execute(\"\"\" SELECT product_id, name, price, stock_level, category FROM products WHERE active = TRUE \"\"\") for row in cursor.fetchall(): product_id, name, price, stock_level, category = row products[product_id] = { \"name\": name, \"price\": float(price), \"stock_level\": stock_level, \"category\": category, \"loaded_at\": int(time.time() * 1000) } cursor.close() conn.close() return products except Exception as e: log.error(\"Failed to load product data: {}\", str(e)) return {} code: | # Load product data into the reference store products = load_products() log.info(\"Loaded {} products into reference store\", len(products)) for product_id, product_data in products.items(): product_reference_store.put(product_id, product_data) # Return the input unchanged return value stores: - product_reference_store # Process new order process_order: type: keyValueTransformer code: | order_id = key order = value # Validate order items against product reference data valid_items = [] invalid_items = [] total_amount = 0 for item in order.get(\"items\", []): product_id = item.get(\"product_id\") quantity = item.get(\"quantity\", 0) if not product_id or quantity <= 0: invalid_items.append({ \"product_id\": product_id, \"reason\": \"Invalid product ID or quantity\" }) continue # Look up product details product = product_reference_store.get(product_id) if not product: invalid_items.append({ \"product_id\": product_id, \"reason\": \"Product not found\" }) continue # Calculate item price item_price = product.get(\"price\", 0) * quantity total_amount += item_price # Add to valid items valid_items.append({ \"product_id\": product_id, \"product_name\": product.get(\"name\"), \"quantity\": quantity, \"unit_price\": product.get(\"price\", 0), \"total_price\": item_price }) # Create validated order validated_order = { \"order_id\": order_id, \"customer_id\": order.get(\"customer_id\"), \"order_date\": order.get(\"order_date\"), \"items\": valid_items, \"invalid_items\": invalid_items, \"total_amount\": total_amount, \"status\": \"validated\" if valid_items else \"invalid\", \"validation_timestamp\": int(time.time() * 1000) } # Store order status order_status_store.put(order_id, { \"status\": validated_order[\"status\"], \"timestamp\": validated_order[\"validation_timestamp\"] }) return (order_id, validated_order) stores: - product_reference_store - order_status_store # Create payment request create_payment_request: type: keyValueTransformer code: | order_id = key order = value # Only process validated orders with valid items if order.get(\"status\") != \"validated\" or not order.get(\"items\"): return None # Create payment request payment_request_id = str(uuid.uuid4()) payment_request = { \"request_id\": payment_request_id, \"order_id\": order_id, \"customer_id\": order.get(\"customer_id\"), \"amount\": order.get(\"total_amount\"), \"currency\": \"USD\", \"timestamp\": int(time.time() * 1000) } # Store payment request for correlation payment_request_store.put(order_id, { \"payment_request_id\": payment_request_id, \"timestamp\": payment_request[\"timestamp\"] }) # Update order status order_status = order_status_store.get(order_id) order_status[\"status\"] = \"payment_pending\" order_status[\"payment_request_id\"] = payment_request_id order_status_store.put(order_id, order_status) return (payment_request_id, payment_request) stores: - payment_request_store - order_status_store # Process payment response process_payment_response: type: keyValueTransformer code: | payment_request_id = key payment_response = value # Extract data from payment response order_id = payment_response.get(\"order_id\") status = payment_response.get(\"status\") if not order_id: log.error(\"Payment response missing order ID: {}\", payment_request_id) return None # Get current order status order_status = order_status_store.get(order_id) if not order_status: log.error(\"Order not found for payment response: {}\", order_id) return None # Update order status based on payment result if status == \"approved\": order_status[\"status\"] = \"paid\" order_status[\"payment_timestamp\"] = payment_response.get(\"timestamp\") order_status[\"payment_reference\"] = payment_response.get(\"reference\") else: order_status[\"status\"] = \"payment_failed\" order_status[\"payment_error\"] = payment_response.get(\"error\") # Store updated status order_status_store.put(order_id, order_status) # Create order status update message status_update = { \"order_id\": order_id, \"status\": order_status[\"status\"], \"timestamp\": int(time.time() * 1000), \"payment_status\": status, \"payment_reference\": payment_response.get(\"reference\"), \"payment_error\": payment_response.get(\"error\") } return (order_id, status_update) stores: - order_status_store # Create inventory request for paid orders create_inventory_request: type: keyValueTransformer code: | order_id = key status_update = value # Only process paid orders if status_update.get(\"status\") != \"paid\": return None # Get the original order # In a real system, you might need to fetch this from a database or state store # For simplicity, we'll assume we have the order details in a state store order = order_status_store.get(order_id) if not order or \"items\" not in order: log.error(\"Order details not found for inventory request: {}\", order_id) return None # Create inventory request inventory_request_id = str(uuid.uuid4()) inventory_request = { \"request_id\": inventory_request_id, \"order_id\": order_id, \"items\": order.get(\"items\", []), \"timestamp\": int(time.time() * 1000) } # Update order status order_status = order_status_store.get(order_id) order_status[\"status\"] = \"inventory_pending\" order_status[\"inventory_request_id\"] = inventory_request_id order_status_store.put(order_id, order_status) return (inventory_request_id, inventory_request) stores: - order_status_store # Process inventory response process_inventory_response: type: keyValueTransformer code: | inventory_request_id = key inventory_response = value # Extract data from inventory response order_id = inventory_response.get(\"order_id\") status = inventory_response.get(\"status\") if not order_id: log.error(\"Inventory response missing order ID: {}\", inventory_request_id) return None # Get current order status order_status = order_status_store.get(order_id) if not order_status: log.error(\"Order not found for inventory response: {}\", order_id) return None # Update order status based on inventory result if status == \"fulfilled\": order_status[\"status\"] = \"ready_for_shipment\" order_status[\"inventory_timestamp\"] = inventory_response.get(\"timestamp\") order_status[\"warehouse_id\"] = inventory_response.get(\"warehouse_id\") else: order_status[\"status\"] = \"inventory_failed\" order_status[\"inventory_error\"] = inventory_response.get(\"error\") # Store updated status order_status_store.put(order_id, order_status) # Create order status update message status_update = { \"order_id\": order_id, \"status\": order_status[\"status\"], \"timestamp\": int(time.time() * 1000), \"inventory_status\": status, \"warehouse_id\": inventory_response.get(\"warehouse_id\"), \"inventory_error\": inventory_response.get(\"error\") } return (order_id, status_update) stores: - order_status_store # Send shipment notification via REST API send_shipment_notification: type: forEach globalCode: | import requests import json # API configuration NOTIFICATION_API_URL = \"https://notifications.example.com/api/v1/shipment\" API_KEY = \"your-api-key\" def send_notification(order_id, customer_id, status): try: payload = { \"order_id\": order_id, \"customer_id\": customer_id, \"status\": status, \"timestamp\": int(time.time() * 1000) } response = requests.post( NOTIFICATION_API_URL, headers={ \"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {API_KEY}\" }, data=json.dumps(payload), timeout=5.0 ) response.raise_for_status() return True except Exception as e: log.error(\"Failed to send notification for order {}: {}\", order_id, str(e)) return False code: | order_id = key status_update = value # Only send notifications for orders ready for shipment if status_update.get(\"status\") != \"ready_for_shipment\": return # Send notification customer_id = status_update.get(\"customer_id\") success = send_notification(order_id, customer_id, \"ready_for_shipment\") if success: log.info(\"Sent shipment notification for order {}\", order_id) else: log.warn(\"Failed to send shipment notification for order {}\", order_id) pipelines: # Initialize reference data load_reference_data: from: reference_data_trigger mapValues: load_product_data to: reference_data_loaded # Process new orders process_orders: from: order_events transformKeyValue: process_order to: validated_orders # Create payment requests request_payments: from: validated_orders transformKeyValue: create_payment_request filter: is_not_null to: payment_requests # Process payment responses handle_payments: from: payment_responses transformKeyValue: process_payment_response filter: is_not_null to: order_status_updates # Create inventory requests request_inventory: from: order_status_updates transformKeyValue: create_inventory_request filter: is_not_null to: inventory_requests # Process inventory responses handle_inventory: from: inventory_responses transformKeyValue: process_inventory_response filter: is_not_null to: order_status_updates # Send notifications send_notifications: from: order_status_updates forEach: send_shipment_notification This example: Loads product reference data from a database Processes incoming orders and validates them against the reference data Creates payment requests and sends them to a payment service Processes payment responses and updates order status Creates inventory requests for paid orders Processes inventory responses and updates order status Sends shipment notifications via a REST API for orders ready for shipment","title":"Practical Example: Multi-System Integration"},{"location":"tutorials/advanced/external-integration/#integration-with-specific-external-systems","text":"","title":"Integration with Specific External Systems"},{"location":"tutorials/advanced/external-integration/#1-relational-databases","text":"For integrating with relational databases: functions: database_integration: type: valueTransformer globalCode: | import psycopg2 from psycopg2.extras import RealDictCursor import json from contextlib import contextmanager # Database connection parameters DB_PARAMS = { \"host\": \"db.example.com\", \"database\": \"app_db\", \"user\": \"app_user\", \"password\": \"app_password\" } @contextmanager def get_db_connection(): \"\"\"Context manager for database connections\"\"\" conn = None try: conn = psycopg2.connect(**DB_PARAMS) yield conn except Exception as e: log.error(\"Database connection error: {}\", str(e)) raise finally: if conn is not None: conn.close() def query_user_data(user_id): \"\"\"Query user data from the database\"\"\" with get_db_connection() as conn: with conn.cursor(cursor_factory=RealDictCursor) as cursor: cursor.execute(\"\"\" SELECT user_id, name, email, account_type, created_at FROM users WHERE user_id = %s \"\"\", (user_id,)) result = cursor.fetchone() return dict(result) if result else None def insert_user_activity(user_id, activity_type, activity_data): \"\"\"Insert user activity into the database\"\"\" with get_db_connection() as conn: with conn.cursor() as cursor: cursor.execute(\"\"\" INSERT INTO user_activities (user_id, activity_type, activity_data, created_at) VALUES (%s, %s, %s, NOW()) RETURNING id \"\"\", (user_id, activity_type, json.dumps(activity_data))) activity_id = cursor.fetchone()[0] conn.commit() return activity_id code: | user_id = key # Query user data user_data = query_user_data(user_id) if not user_data: log.warn(\"User not found: {}\", user_id) return value # Enrich message with user data value[\"user_name\"] = user_data.get(\"name\") value[\"user_email\"] = user_data.get(\"email\") value[\"user_account_type\"] = user_data.get(\"account_type\") # Record activity in database try: activity_id = insert_user_activity( user_id, value.get(\"activity_type\", \"unknown\"), value ) value[\"activity_id\"] = activity_id except Exception as e: log.error(\"Failed to record activity: {}\", str(e)) return value","title":"1. Relational Databases"},{"location":"tutorials/advanced/external-integration/#2-nosql-databases","text":"For integrating with NoSQL databases like MongoDB: functions: mongodb_integration: type: valueTransformer globalCode: | from pymongo import MongoClient import json # MongoDB connection parameters MONGO_URI = \"mongodb://user:password@mongodb.example.com:27017/app_db\" # Create a MongoDB client client = MongoClient(MONGO_URI) db = client.get_database() def get_document(collection, query): \"\"\"Get a document from MongoDB\"\"\" try: result = db[collection].find_one(query) return result except Exception as e: log.error(\"MongoDB query error: {}\", str(e)) return None def insert_document(collection, document): \"\"\"Insert a document into MongoDB\"\"\" try: result = db[collection].insert_one(document) return str(result.inserted_id) except Exception as e: log.error(\"MongoDB insert error: {}\", str(e)) return None code: | # Get product data from MongoDB product_id = value.get(\"product_id\") if product_id: product = get_document(\"products\", {\"_id\": product_id}) if product: # Enrich message with product data value[\"product_name\"] = product.get(\"name\") value[\"product_category\"] = product.get(\"category\") value[\"product_price\"] = product.get(\"price\") # Store the event in MongoDB event_id = insert_document(\"events\", { \"user_id\": key, \"event_type\": value.get(\"type\"), \"timestamp\": value.get(\"timestamp\"), \"data\": value }) if event_id: value[\"event_id\"] = event_id return value","title":"2. NoSQL Databases"},{"location":"tutorials/advanced/external-integration/#3-rest-apis","text":"For integrating with REST APIs: functions: rest_api_integration: type: valueTransformer globalCode: | import requests import json from cachetools import TTLCache # API configuration API_BASE_URL = \"https://api.example.com/v1\" API_KEY = \"your-api-key\" # Create a cache with TTL of 5 minutes cache = TTLCache(maxsize=1000, ttl=300) def get_api_data(endpoint, params=None): \"\"\"Get data from the API with caching\"\"\" # Create cache key cache_key = f\"{endpoint}:{json.dumps(params or {})}\" # Check cache if cache_key in cache: return cache[cache_key] # Make API request try: response = requests.get( f\"{API_BASE_URL}/{endpoint}\", params=params, headers={ \"Authorization\": f\"Bearer {API_KEY}\", \"Accept\": \"application/json\" }, timeout=5.0 ) response.raise_for_status() result = response.json() # Cache the result cache[cache_key] = result return result except Exception as e: log.error(\"API request failed: {}\", str(e)) return None def post_api_data(endpoint, data): \"\"\"Post data to the API\"\"\" try: response = requests.post( f\"{API_BASE_URL}/{endpoint}\", json=data, headers={ \"Authorization\": f\"Bearer {API_KEY}\", \"Content-Type\": \"application/json\" }, timeout=5.0 ) response.raise_for_status() return response.json() except Exception as e: log.error(\"API post failed: {}\", str(e)) return None code: | # Get weather data for the user's location location = value.get(\"location\") if location: weather_data = get_api_data(\"weather\", { \"lat\": location.get(\"latitude\"), \"lon\": location.get(\"longitude\") }) if weather_data: value[\"weather\"] = { \"temperature\": weather_data.get(\"temperature\"), \"conditions\": weather_data.get(\"conditions\"), \"forecast\": weather_data.get(\"forecast\") } # Send analytics event to API analytics_result = post_api_data(\"analytics/events\", { \"user_id\": key, \"event_type\": value.get(\"type\"), \"timestamp\": value.get(\"timestamp\"), \"properties\": value }) if analytics_result: value[\"analytics_id\"] = analytics_result.get(\"id\") return value","title":"3. REST APIs"},{"location":"tutorials/advanced/external-integration/#4-message-queues","text":"For integrating with message queues like RabbitMQ: functions: rabbitmq_integration: type: forEach globalCode: | import pika import json # RabbitMQ connection parameters RABBITMQ_PARAMS = { \"host\": \"rabbitmq.example.com\", \"port\": 5672, \"virtual_host\": \"/\", \"credentials\": pika.PlainCredentials(\"user\", \"password\") } # Create a connection and channel connection = None channel = None def get_channel(): global connection, channel if connection is None or not connection.is_open: connection = pika.BlockingConnection( pika.ConnectionParameters(**RABBITMQ_PARAMS) ) channel = connection.channel() # Declare queues channel.queue_declare(queue=\"notifications\", durable=True) channel.queue_declare(queue=\"alerts\", durable=True) return channel def send_to_queue(queue, message): \"\"\"Send a message to a RabbitMQ queue\"\"\" try: ch = get_channel() ch.basic_publish( exchange=\"\", routing_key=queue, body=json.dumps(message), properties=pika.BasicProperties( delivery_mode=2, # Make message persistent content_type=\"application/json\" ) ) return True except Exception as e: log.error(\"Failed to send message to RabbitMQ: {}\", str(e)) return False code: | # Determine which queue to use based on message type message_type = value.get(\"type\", \"unknown\") if message_type in [\"alert\", \"error\", \"warning\"]: queue = \"alerts\" else: queue = \"notifications\" # Prepare message message = { \"user_id\": key, \"type\": message_type, \"timestamp\": value.get(\"timestamp\", int(time.time() * 1000)), \"content\": value.get(\"content\"), \"priority\": value.get(\"priority\", \"normal\") } # Send to RabbitMQ success = send_to_queue(queue, message) if success: log.info(\"Sent message to RabbitMQ queue {}: {}\", queue, message_type) else: log.error(\"Failed to send message to RabbitMQ queue {}: {}\", queue, message_type)","title":"4. Message Queues"},{"location":"tutorials/advanced/external-integration/#best-practices-for-external-integration","text":"","title":"Best Practices for External Integration"},{"location":"tutorials/advanced/external-integration/#performance-and-reliability","text":"Connection pooling : Reuse connections to external systems Timeouts : Implement appropriate timeouts for external calls Circuit breakers : Prevent cascading failures when external systems are down Retries with backoff : Implement retries with exponential backoff for transient failures Idempotent operations : Ensure that operations can be safely retried","title":"Performance and Reliability"},{"location":"tutorials/advanced/external-integration/#data-consistency","text":"Transactions : Use transactions when appropriate to ensure data consistency Correlation IDs : Use correlation IDs to track operations across systems Idempotency keys : Use idempotency keys to prevent duplicate processing Compensating transactions : Implement compensating transactions for rollbacks","title":"Data Consistency"},{"location":"tutorials/advanced/external-integration/#security","text":"Secure credentials : Store credentials securely and rotate them regularly TLS/SSL : Use secure connections for all external communication Authentication : Implement proper authentication for all external systems Authorization : Ensure proper authorization for all operations Data encryption : Encrypt sensitive data in transit and at rest","title":"Security"},{"location":"tutorials/advanced/external-integration/#monitoring-and-observability","text":"Logging : Log all external interactions with appropriate context Metrics : Track performance metrics for external calls Tracing : Implement distributed tracing for end-to-end visibility Alerting : Set up alerts for abnormal patterns or failures","title":"Monitoring and Observability"},{"location":"tutorials/advanced/external-integration/#conclusion","text":"Integrating KSML applications with external systems allows you to build comprehensive data processing solutions that connect to your entire technology ecosystem. By using the patterns and techniques covered in this tutorial, you can implement reliable, scalable, and maintainable integrations with databases, APIs, message queues, and other external systems. In the next tutorial, we'll explore Advanced Error Handling to learn sophisticated techniques for handling errors and implementing recovery mechanisms in complex KSML applications.","title":"Conclusion"},{"location":"tutorials/advanced/external-integration/#further-reading","text":"Core Concepts: Operations Core Concepts: Functions Intermediate Tutorial: Error Handling Reference: Configuration Options","title":"Further Reading"},{"location":"tutorials/advanced/performance-optimization/","text":"Performance Optimization in KSML This tutorial explores strategies and techniques for optimizing the performance of your KSML applications, helping you build efficient, scalable, and resource-friendly stream processing solutions. Introduction to Performance Optimization Performance optimization is crucial for stream processing applications that need to handle high volumes of data with low latency. Optimizing your KSML applications can help you: Process more data with the same resources Reduce processing latency Lower infrastructure costs Handle spikes in data volume Ensure consistent performance under load This tutorial covers various aspects of performance optimization, from configuration tuning to code-level optimizations. Prerequisites Before starting this tutorial, you should: Understand intermediate KSML concepts (streams, functions, pipelines) Have completed the Custom State Stores tutorial Be familiar with Kafka Streams concepts Have a basic understanding of JVM performance characteristics Identifying Performance Bottlenecks Before optimizing, it's important to identify where performance bottlenecks exist in your application: 1. Monitoring Metrics KSML exposes various metrics that can help identify bottlenecks: functions: monitor_performance: type: forEach code: | # Record processing time start_time = time.time() # Process message process_message(key, value) # Calculate and record processing time processing_time_ms = (time.time() - start_time) * 1000 metrics.timer(\"message.processing.time\").updateMillis(processing_time_ms) # Record message size if value is not None: message_size = len(str(value)) metrics.meter(\"message.size\").mark(message_size) Key metrics to monitor include: Processing time : How long it takes to process each message Throughput : Messages processed per second State store size : How much data is stored in state stores Memory usage : JVM heap and non-heap memory GC activity : Frequency and duration of garbage collection pauses 2. Profiling For more detailed analysis, use profiling tools to identify hotspots in your code: JVM profilers : Tools like VisualVM, JProfiler, or YourKit Flame graphs : For visualizing call stacks and identifying bottlenecks Distributed tracing : For tracking performance across multiple services Configuration Optimization 1. Kafka Streams Configuration Optimize Kafka Streams configuration for your workload: runner: type: streams config: application.id: optimized-ksml-app bootstrap.servers: kafka:9092 # Performance-related settings num.stream.threads: 8 cache.max.bytes.buffering: 104857600 # 100MB commit.interval.ms: 30000 rocksdb.config.setter: org.example.OptimizedRocksDBConfig # Producer settings producer.linger.ms: 100 producer.batch.size: 16384 producer.buffer.memory: 33554432 # Consumer settings consumer.fetch.max.bytes: 52428800 consumer.max.poll.records: 500 Key configuration parameters to tune: num.stream.threads : Number of threads for parallel processing cache.max.bytes.buffering : Size of the record cache commit.interval.ms : How often to commit offsets rocksdb.config.setter : Custom RocksDB configuration for state stores producer/consumer settings : Tune for throughput or latency as needed 2. JVM Configuration Optimize JVM settings for stream processing: -Xms4g -Xmx4g -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 Key JVM parameters to consider: Heap size : Set initial and maximum heap size appropriately Garbage collector : G1GC is generally recommended for stream processing GC tuning : Minimize pause times for consistent performance Data Serialization Optimization 1. Choose Efficient Serialization Formats Select serialization formats based on your needs: streams: optimized_input: topic: input_data keyType: string valueType: avro:OptimizedRecord # Using Avro for efficient serialization schemaRegistry: http://schema-registry:8081 Comparison of formats: Avro : Good balance of size and processing speed, schema evolution Protobuf : Compact binary format, efficient serialization/deserialization JSON : Human-readable but less efficient Binary : Most compact but lacks schema evolution 2. Minimize Serialization/Deserialization Reduce the number of serialization/deserialization operations: functions: efficient_processing: type: valueTransformer code: | # Process the entire message at once instead of extracting and # reconstructing individual fields if \"status\" in value and value[\"status\"] == \"active\": value[\"processed\"] = True value[\"score\"] = calculate_score(value) return value else: return None 3. Use Schema Evolution Carefully When evolving schemas, consider performance implications: streams: evolving_data: topic: evolving_records keyType: string valueType: avro:Record schemaRegistry: http://schema-registry:8081 schemaRegistryConfig: # Use specific compatibility setting for better performance value.compatibility: FORWARD State Store Optimization 1. Optimize State Store Configuration Configure state stores for your specific workload: stores: optimized_store: type: keyValue keyType: string valueType: avro:CompactRecord persistent: true caching: true cacheSizeBytes: 104857600 # 100MB logConfig: segment.bytes: 104857600 # 100MB cleanup.policy: compact 2. Use Caching Effectively Configure and use caching to reduce disk I/O: stores: hot_data_store: type: keyValue keyType: string valueType: json persistent: true caching: true cacheSizeBytes: 268435456 # 256MB 3. Implement Data Expiration Implement strategies to expire old data: functions: expire_old_data: type: valueTransformer code: | current_time = int(time.time() * 1000) stored_data = data_store.get(key) if stored_data and \"timestamp\" in stored_data: # Keep data for 30 days if current_time - stored_data[\"timestamp\"] > 30 * 24 * 60 * 60 * 1000: data_store.delete(key) return None return value stores: - data_store Python Function Optimization 1. Optimize Python Code Write efficient Python code in your functions: functions: optimized_function: type: valueTransformer code: | # Use efficient data structures result = {} # Pre-compute values when possible multiplier = 10 if value.get(\"priority\") == \"high\" else 1 # Use list comprehensions for better performance filtered_items = [item for item in value.get(\"items\", []) if item[\"quantity\"] > 0] # Avoid unnecessary string operations for item in filtered_items: item_id = item[\"id\"] # Use the ID directly instead of string manipulation result[item_id] = item[\"price\"] * item[\"quantity\"] * multiplier return {\"total\": sum(result.values()), \"items\": result} Key Python optimization techniques: Use appropriate data structures : Choose the right data structure for your use case Minimize object creation : Reuse objects when possible Use built-in functions : They're typically faster than custom implementations Avoid unnecessary computations : Compute values only when needed 2. Minimize External Dependencies Reduce the use of external libraries in Python functions: functions: lightweight_function: type: valueTransformer globalCode: | # Import only what's needed from math import sqrt # Implement simple functions directly instead of importing libraries def calculate_mean(values): return sum(values) / len(values) if values else 0 def calculate_stddev(values): mean = calculate_mean(values) variance = sum((x - mean) ** 2 for x in values) / len(values) if values else 0 return sqrt(variance) code: | # Use the lightweight implementations values = [item[\"value\"] for item in value.get(\"items\", [])] return { \"mean\": calculate_mean(values), \"stddev\": calculate_stddev(values) } 3. Use Batch Processing Process data in batches when possible: functions: batch_processor: type: valueTransformer code: | # Get batch of items items = value.get(\"items\", []) # Process in a single pass results = [] total = 0 count = 0 for item in items: # Process each item processed = process_item(item) results.append(processed) # Update aggregates in the same pass total += processed[\"value\"] count += 1 # Return batch results return { \"results\": results, \"average\": total / count if count > 0 else 0, \"count\": count } Pipeline Design Optimization 1. Optimize Pipeline Structure Design pipelines for optimal performance: pipelines: # Split processing into stages filter_and_enrich: from: input_stream filter: is_valid_record # Filter early to reduce downstream processing mapValues: enrich_with_minimal_data # Add only essential data to: filtered_enriched_stream # Process filtered and enriched data process_data: from: filtered_enriched_stream mapValues: compute_intensive_processing # Heavy processing on reduced dataset to: processed_stream Key pipeline design principles: Filter early : Reduce the volume of data flowing through the pipeline Process in stages : Break complex processing into simpler stages Parallelize when possible : Use multiple pipelines for parallel processing Minimize state size : Keep state stores as small as possible 2. Use Repartitioning Strategically Repartition data only when necessary: pipelines: optimize_partitioning: from: input_stream # Repartition once to optimize downstream joins and aggregations selectKey: extract_optimal_key to: repartitioned_stream process_repartitioned: from: repartitioned_stream groupByKey: # Now working with optimally partitioned data aggregate: initializer: initialize_aggregation aggregator: update_aggregation to: aggregated_results 3. Optimize Joins Implement joins efficiently: streams: small_reference_data: topic: reference_data keyType: string valueType: json materializedAs: globalTable # Use GlobalKTable for small reference data large_stream: topic: transaction_stream keyType: string valueType: json pipelines: efficient_join: from: large_stream join: globalTable: small_reference_data # Join with GlobalKTable for better performance valueJoiner: combine_data to: enriched_stream Practical Example: Optimized Real-time Analytics Let's build a complete example that implements an optimized real-time analytics system: streams: user_events: topic: user_activity keyType: string # User ID valueType: avro:UserEvent # Using Avro for efficient serialization schemaRegistry: http://schema-registry:8081 product_catalog: topic: product_data keyType: string # Product ID valueType: avro:Product materializedAs: globalTable # Use GlobalKTable for reference data schemaRegistry: http://schema-registry:8081 user_metrics: topic: user_analytics keyType: string # User ID valueType: avro:UserMetrics schemaRegistry: http://schema-registry:8081 stores: # Store for recent user activity user_activity_store: type: window keyType: string valueType: avro:ActivitySummary windowSize: 1h retainDuplicates: false caching: true cacheSizeBytes: 104857600 # 100MB # Store for user profiles user_profile_store: type: keyValue keyType: string valueType: avro:UserProfile persistent: true caching: true functions: # Filter and categorize events categorize_events: type: keyValueTransformer code: | # Early filtering - skip events we don't care about event_type = value.get(\"event_type\") if event_type not in [\"view\", \"click\", \"purchase\", \"search\"]: return None # Categorize and extract minimal data for downstream processing category = \"engagement\" if event_type in [\"view\", \"click\"] else \"conversion\" # Create a minimal record with only needed fields minimal_record = { \"event_id\": value.get(\"event_id\"), \"timestamp\": value.get(\"timestamp\"), \"event_type\": event_type, \"category\": category, \"product_id\": value.get(\"product_id\"), \"value\": value.get(\"value\", 0) } return (key, minimal_record) # Update user activity metrics update_user_metrics: type: valueTransformer code: | # Get current window data window_data = user_activity_store.get(key) if window_data is None: window_data = { \"event_count\": 0, \"view_count\": 0, \"click_count\": 0, \"purchase_count\": 0, \"search_count\": 0, \"total_value\": 0, \"product_ids\": set(), # Using set for efficient uniqueness check \"last_updated\": 0 } # Update metrics efficiently event_type = value.get(\"event_type\") window_data[\"event_count\"] += 1 window_data[f\"{event_type}_count\"] = window_data.get(f\"{event_type}_count\", 0) + 1 if \"value\" in value: window_data[\"total_value\"] += value[\"value\"] if \"product_id\" in value and value[\"product_id\"]: window_data[\"product_ids\"].add(value[\"product_id\"]) window_data[\"last_updated\"] = max(window_data.get(\"last_updated\", 0), value.get(\"timestamp\", 0)) # Store updated data user_activity_store.put(key, window_data) # For downstream processing, convert set to list result = dict(window_data) result[\"product_ids\"] = list(window_data[\"product_ids\"]) result[\"unique_products\"] = len(result[\"product_ids\"]) return result stores: - user_activity_store # Enrich with product data enrich_with_products: type: valueTransformer code: | # Skip if no product IDs or no metrics if not value or \"product_ids\" not in value or not value[\"product_ids\"]: return value # Get product categories (efficiently) product_categories = {} for product_id in value[\"product_ids\"][:5]: # Limit to top 5 products for efficiency product = product_catalog.get(product_id) if product: category = product.get(\"category\", \"unknown\") product_categories[category] = product_categories.get(category, 0) + 1 # Add product category distribution value[\"top_categories\"] = sorted( product_categories.items(), key=lambda x: x[1], reverse=True )[:3] # Keep only top 3 categories return value stores: - product_catalog # Generate final user metrics generate_user_metrics: type: valueTransformer code: | # Get user profile for context user_profile = user_profile_store.get(key) # Create optimized metrics record metrics = { \"user_id\": key, \"timestamp\": int(time.time() * 1000), \"window_metrics\": { \"event_count\": value.get(\"event_count\", 0), \"view_count\": value.get(\"view_count\", 0), \"click_count\": value.get(\"click_count\", 0), \"purchase_count\": value.get(\"purchase_count\", 0), \"search_count\": value.get(\"search_count\", 0), \"total_value\": value.get(\"total_value\", 0), \"unique_products\": value.get(\"unique_products\", 0) } } # Add user segment if profile exists if user_profile: metrics[\"user_segment\"] = user_profile.get(\"segment\", \"unknown\") metrics[\"account_age_days\"] = user_profile.get(\"account_age_days\", 0) # Add product category insights if available if \"top_categories\" in value: metrics[\"top_categories\"] = value[\"top_categories\"] # Calculate derived metrics efficiently if metrics[\"window_metrics\"][\"view_count\"] > 0: metrics[\"window_metrics\"][\"click_through_rate\"] = ( metrics[\"window_metrics\"][\"click_count\"] / metrics[\"window_metrics\"][\"view_count\"] ) if metrics[\"window_metrics\"][\"click_count\"] > 0: metrics[\"window_metrics\"][\"conversion_rate\"] = ( metrics[\"window_metrics\"][\"purchase_count\"] / metrics[\"window_metrics\"][\"click_count\"] ) return metrics stores: - user_profile_store pipelines: # Stage 1: Filter and categorize events filter_events: from: user_events transformKeyValue: categorize_events filter: is_not_null to: categorized_events # Stage 2: Update user metrics update_metrics: from: categorized_events mapValues: update_user_metrics to: user_activity_metrics # Stage 3: Enrich with product data enrich_metrics: from: user_activity_metrics mapValues: enrich_with_products to: enriched_metrics # Stage 4: Generate final user metrics finalize_metrics: from: enriched_metrics mapValues: generate_user_metrics to: user_metrics runner: type: streams config: application.id: optimized-analytics bootstrap.servers: kafka:9092 # Performance configuration num.stream.threads: 8 cache.max.bytes.buffering: 104857600 # 100MB commit.interval.ms: 30000 # Producer settings producer.linger.ms: 100 producer.batch.size: 16384 # Consumer settings consumer.fetch.max.bytes: 52428800 consumer.max.poll.records: 500 This example: Uses Avro for efficient serialization Implements early filtering to reduce data volume Processes data in stages for better parallelism Uses optimized state stores with appropriate caching Minimizes data copying and transformation Uses efficient data structures (sets for uniqueness checks) Limits processing of large collections (top 5 products, top 3 categories) Includes optimized Kafka Streams configuration Advanced Optimization Techniques 1. Custom Serializers/Deserializers Implement custom serializers for specialized data types: streams: specialized_data: topic: specialized_events keyType: string valueType: custom serdes: value: org.example.HighPerformanceSerializer 2. Memory-Mapped Files For very large state stores, consider memory-mapped files: stores: large_state_store: type: custom implementation: org.example.MMapStateStore config: file.path: /data/large-state max.size.bytes: 10737418240 # 10GB 3. Off-Heap Memory Use off-heap memory for large state stores: runner: type: streams config: application.id: offheap-optimized-app bootstrap.servers: kafka:9092 # RocksDB configuration for off-heap memory rocksdb.config.setter: org.example.OffHeapRocksDBConfig # JVM settings (would be set in the JVM arguments) # -XX:MaxDirectMemorySize=10G Monitoring and Continuous Optimization 1. Implement Comprehensive Metrics Track detailed performance metrics: functions: performance_metrics: type: forEach code: | # Record message processing metrics start_time = time.time() # Process message process_message(key, value) # Record metrics processing_time_ms = (time.time() - start_time) * 1000 metrics.timer(\"processing.time\").updateMillis(processing_time_ms) # Record message size message_size = len(str(value)) if value else 0 metrics.meter(\"message.size\").mark(message_size) # Record message counts by type event_type = value.get(\"event_type\", \"unknown\") metrics.counter(f\"messages.{event_type}\").increment() 2. Implement Health Checks Add health checks to monitor application health: functions: health_check: type: forEach globalCode: | last_processed_time = int(time.time() * 1000) processed_count = 0 code: | global last_processed_time, processed_count # Update processing metrics current_time = int(time.time() * 1000) last_processed_time = current_time processed_count += 1 # Expose health metrics metrics.gauge(\"health.last_processed_time\").set(last_processed_time) metrics.gauge(\"health.processed_count\").set(processed_count) metrics.gauge(\"health.lag_ms\").set(current_time - value.get(\"timestamp\", current_time)) 3. Implement Adaptive Optimization Implement adaptive optimization based on runtime conditions: functions: adaptive_processing: type: valueTransformer globalCode: | import threading # Monitor system load system_load = 0.0 def update_system_load(): global system_load while True: # Get CPU load (simplified example) system_load = get_cpu_load() time.sleep(5) # Start monitoring thread monitor_thread = threading.Thread(target=update_system_load) monitor_thread.daemon = True monitor_thread.start() code: | global system_load # Adapt processing based on system load if system_load > 0.8: # High load # Use simplified processing return simple_processing(value) else: # Use full processing return full_processing(value) Best Practices for Performance Optimization Measure before optimizing : Establish baselines and identify bottlenecks Optimize incrementally : Make one change at a time and measure the impact Focus on hot spots : Optimize the most frequently executed code paths Consider trade-offs : Balance throughput, latency, and resource usage Test with realistic data : Use production-like data volumes and patterns Monitor continuously : Track performance metrics over time Scale horizontally : Add more instances for linear scaling Optimize data flow : Minimize data movement and transformation Conclusion Performance optimization in KSML involves a combination of configuration tuning, code optimization, and architectural design. By applying the techniques covered in this tutorial, you can build KSML applications that efficiently process high volumes of data with low latency. In the next tutorial, we'll explore Integration with External Systems to learn how to connect your KSML applications with databases, APIs, and other external systems. Further Reading Core Concepts: Operations Core Concepts: Functions Advanced Tutorial: Custom State Stores Reference: Configuration Options","title":"Performance Optimization in KSML"},{"location":"tutorials/advanced/performance-optimization/#performance-optimization-in-ksml","text":"This tutorial explores strategies and techniques for optimizing the performance of your KSML applications, helping you build efficient, scalable, and resource-friendly stream processing solutions.","title":"Performance Optimization in KSML"},{"location":"tutorials/advanced/performance-optimization/#introduction-to-performance-optimization","text":"Performance optimization is crucial for stream processing applications that need to handle high volumes of data with low latency. Optimizing your KSML applications can help you: Process more data with the same resources Reduce processing latency Lower infrastructure costs Handle spikes in data volume Ensure consistent performance under load This tutorial covers various aspects of performance optimization, from configuration tuning to code-level optimizations.","title":"Introduction to Performance Optimization"},{"location":"tutorials/advanced/performance-optimization/#prerequisites","text":"Before starting this tutorial, you should: Understand intermediate KSML concepts (streams, functions, pipelines) Have completed the Custom State Stores tutorial Be familiar with Kafka Streams concepts Have a basic understanding of JVM performance characteristics","title":"Prerequisites"},{"location":"tutorials/advanced/performance-optimization/#identifying-performance-bottlenecks","text":"Before optimizing, it's important to identify where performance bottlenecks exist in your application:","title":"Identifying Performance Bottlenecks"},{"location":"tutorials/advanced/performance-optimization/#1-monitoring-metrics","text":"KSML exposes various metrics that can help identify bottlenecks: functions: monitor_performance: type: forEach code: | # Record processing time start_time = time.time() # Process message process_message(key, value) # Calculate and record processing time processing_time_ms = (time.time() - start_time) * 1000 metrics.timer(\"message.processing.time\").updateMillis(processing_time_ms) # Record message size if value is not None: message_size = len(str(value)) metrics.meter(\"message.size\").mark(message_size) Key metrics to monitor include: Processing time : How long it takes to process each message Throughput : Messages processed per second State store size : How much data is stored in state stores Memory usage : JVM heap and non-heap memory GC activity : Frequency and duration of garbage collection pauses","title":"1. Monitoring Metrics"},{"location":"tutorials/advanced/performance-optimization/#2-profiling","text":"For more detailed analysis, use profiling tools to identify hotspots in your code: JVM profilers : Tools like VisualVM, JProfiler, or YourKit Flame graphs : For visualizing call stacks and identifying bottlenecks Distributed tracing : For tracking performance across multiple services","title":"2. Profiling"},{"location":"tutorials/advanced/performance-optimization/#configuration-optimization","text":"","title":"Configuration Optimization"},{"location":"tutorials/advanced/performance-optimization/#1-kafka-streams-configuration","text":"Optimize Kafka Streams configuration for your workload: runner: type: streams config: application.id: optimized-ksml-app bootstrap.servers: kafka:9092 # Performance-related settings num.stream.threads: 8 cache.max.bytes.buffering: 104857600 # 100MB commit.interval.ms: 30000 rocksdb.config.setter: org.example.OptimizedRocksDBConfig # Producer settings producer.linger.ms: 100 producer.batch.size: 16384 producer.buffer.memory: 33554432 # Consumer settings consumer.fetch.max.bytes: 52428800 consumer.max.poll.records: 500 Key configuration parameters to tune: num.stream.threads : Number of threads for parallel processing cache.max.bytes.buffering : Size of the record cache commit.interval.ms : How often to commit offsets rocksdb.config.setter : Custom RocksDB configuration for state stores producer/consumer settings : Tune for throughput or latency as needed","title":"1. Kafka Streams Configuration"},{"location":"tutorials/advanced/performance-optimization/#2-jvm-configuration","text":"Optimize JVM settings for stream processing: -Xms4g -Xmx4g -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 Key JVM parameters to consider: Heap size : Set initial and maximum heap size appropriately Garbage collector : G1GC is generally recommended for stream processing GC tuning : Minimize pause times for consistent performance","title":"2. JVM Configuration"},{"location":"tutorials/advanced/performance-optimization/#data-serialization-optimization","text":"","title":"Data Serialization Optimization"},{"location":"tutorials/advanced/performance-optimization/#1-choose-efficient-serialization-formats","text":"Select serialization formats based on your needs: streams: optimized_input: topic: input_data keyType: string valueType: avro:OptimizedRecord # Using Avro for efficient serialization schemaRegistry: http://schema-registry:8081 Comparison of formats: Avro : Good balance of size and processing speed, schema evolution Protobuf : Compact binary format, efficient serialization/deserialization JSON : Human-readable but less efficient Binary : Most compact but lacks schema evolution","title":"1. Choose Efficient Serialization Formats"},{"location":"tutorials/advanced/performance-optimization/#2-minimize-serializationdeserialization","text":"Reduce the number of serialization/deserialization operations: functions: efficient_processing: type: valueTransformer code: | # Process the entire message at once instead of extracting and # reconstructing individual fields if \"status\" in value and value[\"status\"] == \"active\": value[\"processed\"] = True value[\"score\"] = calculate_score(value) return value else: return None","title":"2. Minimize Serialization/Deserialization"},{"location":"tutorials/advanced/performance-optimization/#3-use-schema-evolution-carefully","text":"When evolving schemas, consider performance implications: streams: evolving_data: topic: evolving_records keyType: string valueType: avro:Record schemaRegistry: http://schema-registry:8081 schemaRegistryConfig: # Use specific compatibility setting for better performance value.compatibility: FORWARD","title":"3. Use Schema Evolution Carefully"},{"location":"tutorials/advanced/performance-optimization/#state-store-optimization","text":"","title":"State Store Optimization"},{"location":"tutorials/advanced/performance-optimization/#1-optimize-state-store-configuration","text":"Configure state stores for your specific workload: stores: optimized_store: type: keyValue keyType: string valueType: avro:CompactRecord persistent: true caching: true cacheSizeBytes: 104857600 # 100MB logConfig: segment.bytes: 104857600 # 100MB cleanup.policy: compact","title":"1. Optimize State Store Configuration"},{"location":"tutorials/advanced/performance-optimization/#2-use-caching-effectively","text":"Configure and use caching to reduce disk I/O: stores: hot_data_store: type: keyValue keyType: string valueType: json persistent: true caching: true cacheSizeBytes: 268435456 # 256MB","title":"2. Use Caching Effectively"},{"location":"tutorials/advanced/performance-optimization/#3-implement-data-expiration","text":"Implement strategies to expire old data: functions: expire_old_data: type: valueTransformer code: | current_time = int(time.time() * 1000) stored_data = data_store.get(key) if stored_data and \"timestamp\" in stored_data: # Keep data for 30 days if current_time - stored_data[\"timestamp\"] > 30 * 24 * 60 * 60 * 1000: data_store.delete(key) return None return value stores: - data_store","title":"3. Implement Data Expiration"},{"location":"tutorials/advanced/performance-optimization/#python-function-optimization","text":"","title":"Python Function Optimization"},{"location":"tutorials/advanced/performance-optimization/#1-optimize-python-code","text":"Write efficient Python code in your functions: functions: optimized_function: type: valueTransformer code: | # Use efficient data structures result = {} # Pre-compute values when possible multiplier = 10 if value.get(\"priority\") == \"high\" else 1 # Use list comprehensions for better performance filtered_items = [item for item in value.get(\"items\", []) if item[\"quantity\"] > 0] # Avoid unnecessary string operations for item in filtered_items: item_id = item[\"id\"] # Use the ID directly instead of string manipulation result[item_id] = item[\"price\"] * item[\"quantity\"] * multiplier return {\"total\": sum(result.values()), \"items\": result} Key Python optimization techniques: Use appropriate data structures : Choose the right data structure for your use case Minimize object creation : Reuse objects when possible Use built-in functions : They're typically faster than custom implementations Avoid unnecessary computations : Compute values only when needed","title":"1. Optimize Python Code"},{"location":"tutorials/advanced/performance-optimization/#2-minimize-external-dependencies","text":"Reduce the use of external libraries in Python functions: functions: lightweight_function: type: valueTransformer globalCode: | # Import only what's needed from math import sqrt # Implement simple functions directly instead of importing libraries def calculate_mean(values): return sum(values) / len(values) if values else 0 def calculate_stddev(values): mean = calculate_mean(values) variance = sum((x - mean) ** 2 for x in values) / len(values) if values else 0 return sqrt(variance) code: | # Use the lightweight implementations values = [item[\"value\"] for item in value.get(\"items\", [])] return { \"mean\": calculate_mean(values), \"stddev\": calculate_stddev(values) }","title":"2. Minimize External Dependencies"},{"location":"tutorials/advanced/performance-optimization/#3-use-batch-processing","text":"Process data in batches when possible: functions: batch_processor: type: valueTransformer code: | # Get batch of items items = value.get(\"items\", []) # Process in a single pass results = [] total = 0 count = 0 for item in items: # Process each item processed = process_item(item) results.append(processed) # Update aggregates in the same pass total += processed[\"value\"] count += 1 # Return batch results return { \"results\": results, \"average\": total / count if count > 0 else 0, \"count\": count }","title":"3. Use Batch Processing"},{"location":"tutorials/advanced/performance-optimization/#pipeline-design-optimization","text":"","title":"Pipeline Design Optimization"},{"location":"tutorials/advanced/performance-optimization/#1-optimize-pipeline-structure","text":"Design pipelines for optimal performance: pipelines: # Split processing into stages filter_and_enrich: from: input_stream filter: is_valid_record # Filter early to reduce downstream processing mapValues: enrich_with_minimal_data # Add only essential data to: filtered_enriched_stream # Process filtered and enriched data process_data: from: filtered_enriched_stream mapValues: compute_intensive_processing # Heavy processing on reduced dataset to: processed_stream Key pipeline design principles: Filter early : Reduce the volume of data flowing through the pipeline Process in stages : Break complex processing into simpler stages Parallelize when possible : Use multiple pipelines for parallel processing Minimize state size : Keep state stores as small as possible","title":"1. Optimize Pipeline Structure"},{"location":"tutorials/advanced/performance-optimization/#2-use-repartitioning-strategically","text":"Repartition data only when necessary: pipelines: optimize_partitioning: from: input_stream # Repartition once to optimize downstream joins and aggregations selectKey: extract_optimal_key to: repartitioned_stream process_repartitioned: from: repartitioned_stream groupByKey: # Now working with optimally partitioned data aggregate: initializer: initialize_aggregation aggregator: update_aggregation to: aggregated_results","title":"2. Use Repartitioning Strategically"},{"location":"tutorials/advanced/performance-optimization/#3-optimize-joins","text":"Implement joins efficiently: streams: small_reference_data: topic: reference_data keyType: string valueType: json materializedAs: globalTable # Use GlobalKTable for small reference data large_stream: topic: transaction_stream keyType: string valueType: json pipelines: efficient_join: from: large_stream join: globalTable: small_reference_data # Join with GlobalKTable for better performance valueJoiner: combine_data to: enriched_stream","title":"3. Optimize Joins"},{"location":"tutorials/advanced/performance-optimization/#practical-example-optimized-real-time-analytics","text":"Let's build a complete example that implements an optimized real-time analytics system: streams: user_events: topic: user_activity keyType: string # User ID valueType: avro:UserEvent # Using Avro for efficient serialization schemaRegistry: http://schema-registry:8081 product_catalog: topic: product_data keyType: string # Product ID valueType: avro:Product materializedAs: globalTable # Use GlobalKTable for reference data schemaRegistry: http://schema-registry:8081 user_metrics: topic: user_analytics keyType: string # User ID valueType: avro:UserMetrics schemaRegistry: http://schema-registry:8081 stores: # Store for recent user activity user_activity_store: type: window keyType: string valueType: avro:ActivitySummary windowSize: 1h retainDuplicates: false caching: true cacheSizeBytes: 104857600 # 100MB # Store for user profiles user_profile_store: type: keyValue keyType: string valueType: avro:UserProfile persistent: true caching: true functions: # Filter and categorize events categorize_events: type: keyValueTransformer code: | # Early filtering - skip events we don't care about event_type = value.get(\"event_type\") if event_type not in [\"view\", \"click\", \"purchase\", \"search\"]: return None # Categorize and extract minimal data for downstream processing category = \"engagement\" if event_type in [\"view\", \"click\"] else \"conversion\" # Create a minimal record with only needed fields minimal_record = { \"event_id\": value.get(\"event_id\"), \"timestamp\": value.get(\"timestamp\"), \"event_type\": event_type, \"category\": category, \"product_id\": value.get(\"product_id\"), \"value\": value.get(\"value\", 0) } return (key, minimal_record) # Update user activity metrics update_user_metrics: type: valueTransformer code: | # Get current window data window_data = user_activity_store.get(key) if window_data is None: window_data = { \"event_count\": 0, \"view_count\": 0, \"click_count\": 0, \"purchase_count\": 0, \"search_count\": 0, \"total_value\": 0, \"product_ids\": set(), # Using set for efficient uniqueness check \"last_updated\": 0 } # Update metrics efficiently event_type = value.get(\"event_type\") window_data[\"event_count\"] += 1 window_data[f\"{event_type}_count\"] = window_data.get(f\"{event_type}_count\", 0) + 1 if \"value\" in value: window_data[\"total_value\"] += value[\"value\"] if \"product_id\" in value and value[\"product_id\"]: window_data[\"product_ids\"].add(value[\"product_id\"]) window_data[\"last_updated\"] = max(window_data.get(\"last_updated\", 0), value.get(\"timestamp\", 0)) # Store updated data user_activity_store.put(key, window_data) # For downstream processing, convert set to list result = dict(window_data) result[\"product_ids\"] = list(window_data[\"product_ids\"]) result[\"unique_products\"] = len(result[\"product_ids\"]) return result stores: - user_activity_store # Enrich with product data enrich_with_products: type: valueTransformer code: | # Skip if no product IDs or no metrics if not value or \"product_ids\" not in value or not value[\"product_ids\"]: return value # Get product categories (efficiently) product_categories = {} for product_id in value[\"product_ids\"][:5]: # Limit to top 5 products for efficiency product = product_catalog.get(product_id) if product: category = product.get(\"category\", \"unknown\") product_categories[category] = product_categories.get(category, 0) + 1 # Add product category distribution value[\"top_categories\"] = sorted( product_categories.items(), key=lambda x: x[1], reverse=True )[:3] # Keep only top 3 categories return value stores: - product_catalog # Generate final user metrics generate_user_metrics: type: valueTransformer code: | # Get user profile for context user_profile = user_profile_store.get(key) # Create optimized metrics record metrics = { \"user_id\": key, \"timestamp\": int(time.time() * 1000), \"window_metrics\": { \"event_count\": value.get(\"event_count\", 0), \"view_count\": value.get(\"view_count\", 0), \"click_count\": value.get(\"click_count\", 0), \"purchase_count\": value.get(\"purchase_count\", 0), \"search_count\": value.get(\"search_count\", 0), \"total_value\": value.get(\"total_value\", 0), \"unique_products\": value.get(\"unique_products\", 0) } } # Add user segment if profile exists if user_profile: metrics[\"user_segment\"] = user_profile.get(\"segment\", \"unknown\") metrics[\"account_age_days\"] = user_profile.get(\"account_age_days\", 0) # Add product category insights if available if \"top_categories\" in value: metrics[\"top_categories\"] = value[\"top_categories\"] # Calculate derived metrics efficiently if metrics[\"window_metrics\"][\"view_count\"] > 0: metrics[\"window_metrics\"][\"click_through_rate\"] = ( metrics[\"window_metrics\"][\"click_count\"] / metrics[\"window_metrics\"][\"view_count\"] ) if metrics[\"window_metrics\"][\"click_count\"] > 0: metrics[\"window_metrics\"][\"conversion_rate\"] = ( metrics[\"window_metrics\"][\"purchase_count\"] / metrics[\"window_metrics\"][\"click_count\"] ) return metrics stores: - user_profile_store pipelines: # Stage 1: Filter and categorize events filter_events: from: user_events transformKeyValue: categorize_events filter: is_not_null to: categorized_events # Stage 2: Update user metrics update_metrics: from: categorized_events mapValues: update_user_metrics to: user_activity_metrics # Stage 3: Enrich with product data enrich_metrics: from: user_activity_metrics mapValues: enrich_with_products to: enriched_metrics # Stage 4: Generate final user metrics finalize_metrics: from: enriched_metrics mapValues: generate_user_metrics to: user_metrics runner: type: streams config: application.id: optimized-analytics bootstrap.servers: kafka:9092 # Performance configuration num.stream.threads: 8 cache.max.bytes.buffering: 104857600 # 100MB commit.interval.ms: 30000 # Producer settings producer.linger.ms: 100 producer.batch.size: 16384 # Consumer settings consumer.fetch.max.bytes: 52428800 consumer.max.poll.records: 500 This example: Uses Avro for efficient serialization Implements early filtering to reduce data volume Processes data in stages for better parallelism Uses optimized state stores with appropriate caching Minimizes data copying and transformation Uses efficient data structures (sets for uniqueness checks) Limits processing of large collections (top 5 products, top 3 categories) Includes optimized Kafka Streams configuration","title":"Practical Example: Optimized Real-time Analytics"},{"location":"tutorials/advanced/performance-optimization/#advanced-optimization-techniques","text":"","title":"Advanced Optimization Techniques"},{"location":"tutorials/advanced/performance-optimization/#1-custom-serializersdeserializers","text":"Implement custom serializers for specialized data types: streams: specialized_data: topic: specialized_events keyType: string valueType: custom serdes: value: org.example.HighPerformanceSerializer","title":"1. Custom Serializers/Deserializers"},{"location":"tutorials/advanced/performance-optimization/#2-memory-mapped-files","text":"For very large state stores, consider memory-mapped files: stores: large_state_store: type: custom implementation: org.example.MMapStateStore config: file.path: /data/large-state max.size.bytes: 10737418240 # 10GB","title":"2. Memory-Mapped Files"},{"location":"tutorials/advanced/performance-optimization/#3-off-heap-memory","text":"Use off-heap memory for large state stores: runner: type: streams config: application.id: offheap-optimized-app bootstrap.servers: kafka:9092 # RocksDB configuration for off-heap memory rocksdb.config.setter: org.example.OffHeapRocksDBConfig # JVM settings (would be set in the JVM arguments) # -XX:MaxDirectMemorySize=10G","title":"3. Off-Heap Memory"},{"location":"tutorials/advanced/performance-optimization/#monitoring-and-continuous-optimization","text":"","title":"Monitoring and Continuous Optimization"},{"location":"tutorials/advanced/performance-optimization/#1-implement-comprehensive-metrics","text":"Track detailed performance metrics: functions: performance_metrics: type: forEach code: | # Record message processing metrics start_time = time.time() # Process message process_message(key, value) # Record metrics processing_time_ms = (time.time() - start_time) * 1000 metrics.timer(\"processing.time\").updateMillis(processing_time_ms) # Record message size message_size = len(str(value)) if value else 0 metrics.meter(\"message.size\").mark(message_size) # Record message counts by type event_type = value.get(\"event_type\", \"unknown\") metrics.counter(f\"messages.{event_type}\").increment()","title":"1. Implement Comprehensive Metrics"},{"location":"tutorials/advanced/performance-optimization/#2-implement-health-checks","text":"Add health checks to monitor application health: functions: health_check: type: forEach globalCode: | last_processed_time = int(time.time() * 1000) processed_count = 0 code: | global last_processed_time, processed_count # Update processing metrics current_time = int(time.time() * 1000) last_processed_time = current_time processed_count += 1 # Expose health metrics metrics.gauge(\"health.last_processed_time\").set(last_processed_time) metrics.gauge(\"health.processed_count\").set(processed_count) metrics.gauge(\"health.lag_ms\").set(current_time - value.get(\"timestamp\", current_time))","title":"2. Implement Health Checks"},{"location":"tutorials/advanced/performance-optimization/#3-implement-adaptive-optimization","text":"Implement adaptive optimization based on runtime conditions: functions: adaptive_processing: type: valueTransformer globalCode: | import threading # Monitor system load system_load = 0.0 def update_system_load(): global system_load while True: # Get CPU load (simplified example) system_load = get_cpu_load() time.sleep(5) # Start monitoring thread monitor_thread = threading.Thread(target=update_system_load) monitor_thread.daemon = True monitor_thread.start() code: | global system_load # Adapt processing based on system load if system_load > 0.8: # High load # Use simplified processing return simple_processing(value) else: # Use full processing return full_processing(value)","title":"3. Implement Adaptive Optimization"},{"location":"tutorials/advanced/performance-optimization/#best-practices-for-performance-optimization","text":"Measure before optimizing : Establish baselines and identify bottlenecks Optimize incrementally : Make one change at a time and measure the impact Focus on hot spots : Optimize the most frequently executed code paths Consider trade-offs : Balance throughput, latency, and resource usage Test with realistic data : Use production-like data volumes and patterns Monitor continuously : Track performance metrics over time Scale horizontally : Add more instances for linear scaling Optimize data flow : Minimize data movement and transformation","title":"Best Practices for Performance Optimization"},{"location":"tutorials/advanced/performance-optimization/#conclusion","text":"Performance optimization in KSML involves a combination of configuration tuning, code optimization, and architectural design. By applying the techniques covered in this tutorial, you can build KSML applications that efficiently process high volumes of data with low latency. In the next tutorial, we'll explore Integration with External Systems to learn how to connect your KSML applications with databases, APIs, and other external systems.","title":"Conclusion"},{"location":"tutorials/advanced/performance-optimization/#further-reading","text":"Core Concepts: Operations Core Concepts: Functions Advanced Tutorial: Custom State Stores Reference: Configuration Options","title":"Further Reading"},{"location":"tutorials/beginner/","text":"Beginner Tutorials Welcome to the KSML beginner tutorials! These tutorials are designed for users who are new to KSML and want to learn the basics of building data pipelines with KSML. Each tutorial provides step-by-step instructions and explanations to help you understand not just how to use KSML, but why certain approaches are recommended. Available Tutorials Building Your First KSML Data Pipeline This tutorial guides you through building a simple data pipeline that filters and transforms temperature data. You'll learn: How to define streams for input and output How to create and use functions How to build a pipeline with filter, transform, and logging operations How to run and test your KSML application Filtering and Transforming Data This tutorial expands on the basics by exploring more advanced filtering and transformation techniques: Using complex filter conditions Applying multiple transformations Working with nested data structures Error handling in transformations Working with Different Data Formats Learn how to work with various data formats in KSML: JSON data processing Avro schema integration CSV data handling Converting between formats Logging and Monitoring This tutorial focuses on how to add effective logging and monitoring to your KSML pipelines: Using the peek operation for logging Creating custom logging functions Adding metrics to your pipelines Troubleshooting with logs Learning Path We recommend following these tutorials in order, as each one builds on concepts introduced in previous tutorials. After completing these beginner tutorials, you'll be ready to move on to the Intermediate Tutorials , which cover more advanced topics like aggregations, joins, and windowing operations. Additional Resources KSML Core Concepts - Deeper explanations of KSML components Examples Library - Ready-to-use examples for common patterns Operations Reference - Complete reference for all KSML operations","title":"Beginner Tutorials"},{"location":"tutorials/beginner/#beginner-tutorials","text":"Welcome to the KSML beginner tutorials! These tutorials are designed for users who are new to KSML and want to learn the basics of building data pipelines with KSML. Each tutorial provides step-by-step instructions and explanations to help you understand not just how to use KSML, but why certain approaches are recommended.","title":"Beginner Tutorials"},{"location":"tutorials/beginner/#available-tutorials","text":"","title":"Available Tutorials"},{"location":"tutorials/beginner/#building-your-first-ksml-data-pipeline","text":"This tutorial guides you through building a simple data pipeline that filters and transforms temperature data. You'll learn: How to define streams for input and output How to create and use functions How to build a pipeline with filter, transform, and logging operations How to run and test your KSML application","title":"Building Your First KSML Data Pipeline"},{"location":"tutorials/beginner/#filtering-and-transforming-data","text":"This tutorial expands on the basics by exploring more advanced filtering and transformation techniques: Using complex filter conditions Applying multiple transformations Working with nested data structures Error handling in transformations","title":"Filtering and Transforming Data"},{"location":"tutorials/beginner/#working-with-different-data-formats","text":"Learn how to work with various data formats in KSML: JSON data processing Avro schema integration CSV data handling Converting between formats","title":"Working with Different Data Formats"},{"location":"tutorials/beginner/#logging-and-monitoring","text":"This tutorial focuses on how to add effective logging and monitoring to your KSML pipelines: Using the peek operation for logging Creating custom logging functions Adding metrics to your pipelines Troubleshooting with logs","title":"Logging and Monitoring"},{"location":"tutorials/beginner/#learning-path","text":"We recommend following these tutorials in order, as each one builds on concepts introduced in previous tutorials. After completing these beginner tutorials, you'll be ready to move on to the Intermediate Tutorials , which cover more advanced topics like aggregations, joins, and windowing operations.","title":"Learning Path"},{"location":"tutorials/beginner/#additional-resources","text":"KSML Core Concepts - Deeper explanations of KSML components Examples Library - Ready-to-use examples for common patterns Operations Reference - Complete reference for all KSML operations","title":"Additional Resources"},{"location":"tutorials/beginner/data-formats/","text":"Working with Different Data Formats This tutorial guides you through working with various data formats in KSML. You'll learn how to process data in different formats, convert between formats, and leverage schema-based formats for data validation and compatibility. Prerequisites Basic understanding of Kafka concepts (topics, messages) Completed the Building a Simple Data Pipeline tutorial Familiarity with basic KSML concepts (streams, functions, pipelines) What You'll Learn In this tutorial, you'll learn: 1. How to specify data formats for streams 2. Working with JSON data 3. Using Avro schemas for data validation 4. Processing CSV data 5. Converting between different data formats Data Formats in KSML KSML supports a variety of data formats for both keys and values in Kafka messages: String : Plain text data JSON : JavaScript Object Notation for structured data Avro : Schema-based binary format with strong typing CSV : Comma-separated values for tabular data XML : Extensible Markup Language for hierarchical data Protobuf : Google's Protocol Buffers, a compact binary format Binary : Raw binary data Each format has its own strengths and use cases, which we'll explore in this tutorial. Specifying Data Formats When defining streams in KSML, you specify the data format using the keyType and valueType properties: streams: json_stream: topic: example_json_topic keyType: string valueType: json avro_stream: topic: example_avro_topic keyType: string valueType: avro:SensorData For schema-based formats like Avro, XML, and Protobuf, you specify the schema name after the format type (e.g., avro:SensorData ). Working with JSON Data JSON is one of the most common data formats used with Kafka. It's human-readable, flexible, and widely supported. Reading and Writing JSON Data Here's an example of a pipeline that reads and writes JSON data: streams: input_stream: topic: input_topic keyType: string valueType: json output_stream: topic: output_topic keyType: string valueType: json functions: transform_json: type: valueMapper code: | # Access JSON fields directly temperature = value.get('temperature', 0) humidity = value.get('humidity', 0) # Create a new JSON structure return { 'device_id': key, 'readings': { 'temperature': temperature, 'humidity': humidity }, 'status': 'normal' if temperature < 30 else 'warning', 'timestamp': value.get('timestamp') } pipelines: json_pipeline: from: input_stream via: - type: mapValues using: transform_json to: output_stream JSON Data Handling Tips Use Python's dictionary methods like get() to safely access fields that might not exist Remember that numeric values in JSON are preserved as numbers in KSML Nested structures can be accessed using dot notation or nested get() calls Working with Avro Data Avro is a schema-based binary format that provides strong typing, schema evolution, and compact serialization. Defining an Avro Schema Avro schemas are defined in JSON format. Here's a simple example for sensor data: { \"type\": \"record\", \"name\": \"SensorData\", \"namespace\": \"com.example\", \"fields\": [ {\"name\": \"name\", \"type\": [\"null\", \"string\"], \"default\": null}, {\"name\": \"timestamp\", \"type\": [\"null\", \"string\"], \"default\": null}, {\"name\": \"type\", \"type\": [\"null\", \"string\"], \"default\": null}, {\"name\": \"unit\", \"type\": [\"null\", \"string\"], \"default\": null}, {\"name\": \"value\", \"type\": [\"null\", \"string\"], \"default\": null}, {\"name\": \"color\", \"type\": [\"null\", \"string\"], \"default\": null}, {\"name\": \"owner\", \"type\": [\"null\", \"string\"], \"default\": null}, {\"name\": \"city\", \"type\": [\"null\", \"string\"], \"default\": null} ] } Save this schema in a file with a .avsc extension (e.g., SensorData.avsc ). Reading and Writing Avro Data Here's how to work with Avro data in KSML: streams: avro_input: topic: sensor_data_avro keyType: string valueType: avro:SensorData json_output: topic: sensor_data_json keyType: string valueType: json pipelines: avro_to_json: from: avro_input via: - type: peek forEach: code: | log.info(\"Processing Avro record: {}\", value) - type: convertValue into: json to: json_output Benefits of Using Avro Schema Validation : Data is validated against the schema during serialization Compact Format : Binary encoding is more efficient than text-based formats Schema Evolution : Avro supports adding, removing, or changing fields while maintaining compatibility Language Interoperability : Avro schemas can be used with multiple programming languages Working with CSV Data CSV (Comma-Separated Values) is a simple format for representing tabular data. Reading and Writing CSV Data Here's how to work with CSV data in KSML: streams: csv_input: topic: sensor_data_csv keyType: string valueType: csv json_output: topic: sensor_data_json keyType: string valueType: json functions: csv_to_json: type: valueMapper code: | # Assuming CSV columns are: name,timestamp,type,unit,value if len(value) >= 5: return { 'name': value[0], 'timestamp': value[1], 'type': value[2], 'unit': value[3], 'value': value[4] } else: log.warn(\"Invalid CSV record: {}\", value) return {} pipelines: process_csv: from: csv_input via: - type: mapValues using: csv_to_json to: json_output CSV Data Handling Tips CSV data is represented as a list of values in KSML You need to know the column order to correctly interpret the data Consider adding header validation if your CSV data includes headers Converting Between Data Formats KSML makes it easy to convert between different data formats using the convertValue operation. Example: Converting Between Multiple Formats Here's an example that demonstrates converting between several formats: streams: avro_input: topic: sensor_data_avro keyType: string valueType: avro:SensorData xml_output: topic: sensor_data_xml keyType: string valueType: xml:SensorData pipelines: format_conversion: from: avro_input via: # Convert from Avro to JSON - type: convertValue into: json - type: peek forEach: code: | log.info(\"As JSON: {}\", value) # Convert from JSON to XML - type: convertValue into: xml:SensorData - type: peek forEach: code: | log.info(\"As XML object: {}\", value) # Convert to string to see the XML representation - type: convertValue into: string - type: peek forEach: code: | log.info(\"As XML string: {}\", value) # Convert back to XML object for output - type: convertValue into: xml:SensorData to: xml_output Implicit Conversion KSML can also perform implicit conversion when writing to a topic with a different format than the current message format: pipelines: implicit_conversion: from: avro_input # Stream with Avro format to: xml_output # Stream with XML format # The conversion happens automatically Advanced: Working with Multiple Formats in a Single Pipeline In real-world scenarios, you might need to process data from multiple sources with different formats. Here's an example that combines JSON and Avro data: streams: json_stream: topic: device_config keyType: string valueType: json avro_stream: topic: sensor_readings keyType: string valueType: avro:SensorData enriched_output: topic: enriched_sensor_data keyType: string valueType: json tables: config_table: stream: json_stream keyType: string valueType: json functions: enrich_sensor_data: type: valueMapper code: | # Get device configuration from the table device_config = tables.config_table.get(key) # Convert Avro to a dictionary if needed sensor_data = value # Combine the data if device_config is not None: return { 'device_id': key, 'reading': { 'type': sensor_data.get('type'), 'unit': sensor_data.get('unit'), 'value': sensor_data.get('value') }, 'config': { 'threshold': device_config.get('threshold'), 'alert_level': device_config.get('alert_level') }, 'timestamp': sensor_data.get('timestamp') } else: return sensor_data pipelines: enrichment_pipeline: from: avro_stream via: - type: mapValues using: enrich_sensor_data to: enriched_output Conclusion In this tutorial, you've learned how to work with different data formats in KSML, including: Specifying data formats for streams Working with JSON data Using Avro schemas for data validation Processing CSV data Converting between different formats These skills will help you build flexible and interoperable data pipelines that can work with a variety of data sources and destinations. Next Steps Explore the Logging and Monitoring tutorial to learn how to add effective logging to your pipelines Check out the Intermediate Tutorials for more advanced KSML features Review the KSML Examples for more examples of working with different data formats","title":"Working with Different Data Formats"},{"location":"tutorials/beginner/data-formats/#working-with-different-data-formats","text":"This tutorial guides you through working with various data formats in KSML. You'll learn how to process data in different formats, convert between formats, and leverage schema-based formats for data validation and compatibility.","title":"Working with Different Data Formats"},{"location":"tutorials/beginner/data-formats/#prerequisites","text":"Basic understanding of Kafka concepts (topics, messages) Completed the Building a Simple Data Pipeline tutorial Familiarity with basic KSML concepts (streams, functions, pipelines)","title":"Prerequisites"},{"location":"tutorials/beginner/data-formats/#what-youll-learn","text":"In this tutorial, you'll learn: 1. How to specify data formats for streams 2. Working with JSON data 3. Using Avro schemas for data validation 4. Processing CSV data 5. Converting between different data formats","title":"What You'll Learn"},{"location":"tutorials/beginner/data-formats/#data-formats-in-ksml","text":"KSML supports a variety of data formats for both keys and values in Kafka messages: String : Plain text data JSON : JavaScript Object Notation for structured data Avro : Schema-based binary format with strong typing CSV : Comma-separated values for tabular data XML : Extensible Markup Language for hierarchical data Protobuf : Google's Protocol Buffers, a compact binary format Binary : Raw binary data Each format has its own strengths and use cases, which we'll explore in this tutorial.","title":"Data Formats in KSML"},{"location":"tutorials/beginner/data-formats/#specifying-data-formats","text":"When defining streams in KSML, you specify the data format using the keyType and valueType properties: streams: json_stream: topic: example_json_topic keyType: string valueType: json avro_stream: topic: example_avro_topic keyType: string valueType: avro:SensorData For schema-based formats like Avro, XML, and Protobuf, you specify the schema name after the format type (e.g., avro:SensorData ).","title":"Specifying Data Formats"},{"location":"tutorials/beginner/data-formats/#working-with-json-data","text":"JSON is one of the most common data formats used with Kafka. It's human-readable, flexible, and widely supported.","title":"Working with JSON Data"},{"location":"tutorials/beginner/data-formats/#reading-and-writing-json-data","text":"Here's an example of a pipeline that reads and writes JSON data: streams: input_stream: topic: input_topic keyType: string valueType: json output_stream: topic: output_topic keyType: string valueType: json functions: transform_json: type: valueMapper code: | # Access JSON fields directly temperature = value.get('temperature', 0) humidity = value.get('humidity', 0) # Create a new JSON structure return { 'device_id': key, 'readings': { 'temperature': temperature, 'humidity': humidity }, 'status': 'normal' if temperature < 30 else 'warning', 'timestamp': value.get('timestamp') } pipelines: json_pipeline: from: input_stream via: - type: mapValues using: transform_json to: output_stream","title":"Reading and Writing JSON Data"},{"location":"tutorials/beginner/data-formats/#json-data-handling-tips","text":"Use Python's dictionary methods like get() to safely access fields that might not exist Remember that numeric values in JSON are preserved as numbers in KSML Nested structures can be accessed using dot notation or nested get() calls","title":"JSON Data Handling Tips"},{"location":"tutorials/beginner/data-formats/#working-with-avro-data","text":"Avro is a schema-based binary format that provides strong typing, schema evolution, and compact serialization.","title":"Working with Avro Data"},{"location":"tutorials/beginner/data-formats/#defining-an-avro-schema","text":"Avro schemas are defined in JSON format. Here's a simple example for sensor data: { \"type\": \"record\", \"name\": \"SensorData\", \"namespace\": \"com.example\", \"fields\": [ {\"name\": \"name\", \"type\": [\"null\", \"string\"], \"default\": null}, {\"name\": \"timestamp\", \"type\": [\"null\", \"string\"], \"default\": null}, {\"name\": \"type\", \"type\": [\"null\", \"string\"], \"default\": null}, {\"name\": \"unit\", \"type\": [\"null\", \"string\"], \"default\": null}, {\"name\": \"value\", \"type\": [\"null\", \"string\"], \"default\": null}, {\"name\": \"color\", \"type\": [\"null\", \"string\"], \"default\": null}, {\"name\": \"owner\", \"type\": [\"null\", \"string\"], \"default\": null}, {\"name\": \"city\", \"type\": [\"null\", \"string\"], \"default\": null} ] } Save this schema in a file with a .avsc extension (e.g., SensorData.avsc ).","title":"Defining an Avro Schema"},{"location":"tutorials/beginner/data-formats/#reading-and-writing-avro-data","text":"Here's how to work with Avro data in KSML: streams: avro_input: topic: sensor_data_avro keyType: string valueType: avro:SensorData json_output: topic: sensor_data_json keyType: string valueType: json pipelines: avro_to_json: from: avro_input via: - type: peek forEach: code: | log.info(\"Processing Avro record: {}\", value) - type: convertValue into: json to: json_output","title":"Reading and Writing Avro Data"},{"location":"tutorials/beginner/data-formats/#benefits-of-using-avro","text":"Schema Validation : Data is validated against the schema during serialization Compact Format : Binary encoding is more efficient than text-based formats Schema Evolution : Avro supports adding, removing, or changing fields while maintaining compatibility Language Interoperability : Avro schemas can be used with multiple programming languages","title":"Benefits of Using Avro"},{"location":"tutorials/beginner/data-formats/#working-with-csv-data","text":"CSV (Comma-Separated Values) is a simple format for representing tabular data.","title":"Working with CSV Data"},{"location":"tutorials/beginner/data-formats/#reading-and-writing-csv-data","text":"Here's how to work with CSV data in KSML: streams: csv_input: topic: sensor_data_csv keyType: string valueType: csv json_output: topic: sensor_data_json keyType: string valueType: json functions: csv_to_json: type: valueMapper code: | # Assuming CSV columns are: name,timestamp,type,unit,value if len(value) >= 5: return { 'name': value[0], 'timestamp': value[1], 'type': value[2], 'unit': value[3], 'value': value[4] } else: log.warn(\"Invalid CSV record: {}\", value) return {} pipelines: process_csv: from: csv_input via: - type: mapValues using: csv_to_json to: json_output","title":"Reading and Writing CSV Data"},{"location":"tutorials/beginner/data-formats/#csv-data-handling-tips","text":"CSV data is represented as a list of values in KSML You need to know the column order to correctly interpret the data Consider adding header validation if your CSV data includes headers","title":"CSV Data Handling Tips"},{"location":"tutorials/beginner/data-formats/#converting-between-data-formats","text":"KSML makes it easy to convert between different data formats using the convertValue operation.","title":"Converting Between Data Formats"},{"location":"tutorials/beginner/data-formats/#example-converting-between-multiple-formats","text":"Here's an example that demonstrates converting between several formats: streams: avro_input: topic: sensor_data_avro keyType: string valueType: avro:SensorData xml_output: topic: sensor_data_xml keyType: string valueType: xml:SensorData pipelines: format_conversion: from: avro_input via: # Convert from Avro to JSON - type: convertValue into: json - type: peek forEach: code: | log.info(\"As JSON: {}\", value) # Convert from JSON to XML - type: convertValue into: xml:SensorData - type: peek forEach: code: | log.info(\"As XML object: {}\", value) # Convert to string to see the XML representation - type: convertValue into: string - type: peek forEach: code: | log.info(\"As XML string: {}\", value) # Convert back to XML object for output - type: convertValue into: xml:SensorData to: xml_output","title":"Example: Converting Between Multiple Formats"},{"location":"tutorials/beginner/data-formats/#implicit-conversion","text":"KSML can also perform implicit conversion when writing to a topic with a different format than the current message format: pipelines: implicit_conversion: from: avro_input # Stream with Avro format to: xml_output # Stream with XML format # The conversion happens automatically","title":"Implicit Conversion"},{"location":"tutorials/beginner/data-formats/#advanced-working-with-multiple-formats-in-a-single-pipeline","text":"In real-world scenarios, you might need to process data from multiple sources with different formats. Here's an example that combines JSON and Avro data: streams: json_stream: topic: device_config keyType: string valueType: json avro_stream: topic: sensor_readings keyType: string valueType: avro:SensorData enriched_output: topic: enriched_sensor_data keyType: string valueType: json tables: config_table: stream: json_stream keyType: string valueType: json functions: enrich_sensor_data: type: valueMapper code: | # Get device configuration from the table device_config = tables.config_table.get(key) # Convert Avro to a dictionary if needed sensor_data = value # Combine the data if device_config is not None: return { 'device_id': key, 'reading': { 'type': sensor_data.get('type'), 'unit': sensor_data.get('unit'), 'value': sensor_data.get('value') }, 'config': { 'threshold': device_config.get('threshold'), 'alert_level': device_config.get('alert_level') }, 'timestamp': sensor_data.get('timestamp') } else: return sensor_data pipelines: enrichment_pipeline: from: avro_stream via: - type: mapValues using: enrich_sensor_data to: enriched_output","title":"Advanced: Working with Multiple Formats in a Single Pipeline"},{"location":"tutorials/beginner/data-formats/#conclusion","text":"In this tutorial, you've learned how to work with different data formats in KSML, including: Specifying data formats for streams Working with JSON data Using Avro schemas for data validation Processing CSV data Converting between different formats These skills will help you build flexible and interoperable data pipelines that can work with a variety of data sources and destinations.","title":"Conclusion"},{"location":"tutorials/beginner/data-formats/#next-steps","text":"Explore the Logging and Monitoring tutorial to learn how to add effective logging to your pipelines Check out the Intermediate Tutorials for more advanced KSML features Review the KSML Examples for more examples of working with different data formats","title":"Next Steps"},{"location":"tutorials/beginner/filtering-transforming/","text":"Filtering and Transforming Data in KSML This tutorial builds on the KSML Basics Tutorial and explores more advanced filtering and transformation techniques in KSML. You'll learn how to work with complex filter conditions, apply multiple transformations, handle nested data structures, and implement error handling in your transformations. What You'll Build In this tutorial, you'll build a data pipeline that: Reads sensor data from a Kafka topic Applies complex filtering based on multiple conditions Transforms the data using various techniques Handles potential errors in the transformation process Writes the processed data to another Kafka topic Prerequisites Before you begin, make sure you have: Completed the KSML Basics Tutorial A running KSML environment with Kafka Basic understanding of YAML and Python syntax Complex Filtering Techniques Using Multiple Conditions Let's start by creating a filter that combines multiple conditions: streams: input_stream: topic: tutorial_input keyType: string valueType: json output_stream: topic: filtered_data keyType: string valueType: json pipelines: filtering_pipeline: from: input_stream via: - type: filter if: expression: value.get('temperature', 0) > 70 and value.get('humidity', 99) < 50 and value.get('location', '') == 'warehouse' to: output_stream This filter only passes messages where: - The temperature is greater than 70\u00b0F - The humidity is less than 50% - The location is 'warehouse' Using Custom Filter Functions For more complex filtering logic, you can create a custom filter function: streams: input_stream: topic: tutorial_input keyType: string valueType: json alerts_stream: topic: alerts_stream keyType: string valueType: json functions: is_critical_sensor: type: predicate code: | # Check if this is a critical sensor based on multiple criteria if value.get('type') != 'temperature': return False # Check location if value.get('location') not in ['server_room', 'data_center']: return False # Check temperature threshold based on location if value.get('location') == 'server_room' and value.get('temperature') > 80: return True if value.get('location') == 'data_center' and value.get('temperature') > 75: return True return False pipelines: critical_alerts: from: input_stream via: - type: filter if: is_critical_sensor to: alerts_stream This function implements complex business logic to determine if a sensor reading indicates a critical situation that requires an alert. Filtering with Error Handling Sometimes your filter conditions might encounter malformed data. Here's how to handle that: functions: safe_filter: type: predicate code: | try: # Attempt to apply our filter logic if 'temperature' not in value or 'humidity' not in value: log.warn(\"Missing required fields in message: {}\", value) return False return value['temperature'] > 70 and value['humidity'] < 50 except Exception as e: # Log the error and filter out the message log.error(\"Error in filter: {} - Message: {}\", str(e), value) return False pipelines: robust_filtering: from: input_stream via: - type: filter if: safe_filter to: output_stream This approach ensures that malformed messages are logged and filtered out rather than causing the pipeline to fail. Advanced Transformation Techniques Transforming Nested Data Structures Let's look at how to transform data with nested structures: functions: transform_nested_data: type: valueMapper code: | # Create a new structure with flattened and transformed data result = { \"device_id\": key, \"timestamp\": value.get('metadata', {}).get('timestamp'), \"readings\": {} } # Extract and transform sensor readings sensors = value.get('sensors', {}) for sensor_type, reading in sensors.items(): # Convert temperature from F to C if needed if sensor_type == 'temperature' and reading.get('unit') == 'F': celsius = (reading.get('value') - 32) * 5/9 result['readings'][sensor_type] = { 'value': celsius, 'unit': 'C', 'original_value': reading.get('value'), 'original_unit': 'F' } else: result['readings'][sensor_type] = reading return result pipelines: transform_pipeline: from: input_stream via: - type: mapValues mapper: transform_nested_data to: output_stream This transformation function handles a complex nested JSON structure, extracting and transforming specific fields while preserving others. Applying Multiple Transformations You can chain multiple transformations to break down complex logic into manageable steps: pipelines: multi_transform_pipeline: from: input_stream via: # Step 1: Extract relevant fields - type: mapValues mapper: expression: { \"device_id\": key, \"temperature\": value.get('sensors', {}).get('temperature', {}).get('value'), \"humidity\": value.get('sensors', {}).get('humidity', {}).get('value'), \"timestamp\": value.get('metadata', {}).get('timestamp') } # Step 2: Convert temperature from F to C - type: mapValues mapper: expression: { \"device_id\": value.get('device_id'), \"temperature_c\": (value.get('temperature') - 32) * 5/9, \"humidity\": value.get('humidity'), \"timestamp\": value.get('timestamp') } # Step 3: Add calculated fields - type: mapValues mapper: expression: { \"device_id\": value.get('device_id'), \"temperature_c\": value.get('temperature_c'), \"humidity\": value.get('humidity'), \"heat_index\": value.get('temperature_c') * 1.8 + 32 - 0.55 * (1 - value.get('humidity') / 100), \"timestamp\": value.get('timestamp') } to: output_stream Breaking transformations into steps makes your pipeline easier to understand and maintain. Error Handling in Transformations Here's how to implement robust error handling in transformations: functions: safe_transform: type: valueMapper code: | try: # Attempt the transformation if 'temperature' not in value: log.warn(\"Missing temperature field in message: {}\", value) return {\"error\": \"Missing temperature field\", \"original\": value} celsius = (value['temperature'] - 32) * 5/9 return { \"device_id\": key, \"temperature_f\": value['temperature'], \"temperature_c\": celsius, \"status\": \"processed\" } except Exception as e: # Log the error and return a special error message log.error(\"Error in transformation: {} - Message: {}\", str(e), value) return { \"error\": str(e), \"original\": value, \"status\": \"error\" } pipelines: robust_transformation: from: input_stream via: - type: mapValues mapper: safe_transform # You could add a branch here to handle error messages differently to: output_stream This approach ensures that transformation errors are caught, logged, and handled gracefully without crashing the pipeline. Combining Filtering and Transformation Let's put everything together in a complete example: streams: sensor_data: topic: raw_sensor_data keyType: string valueType: json processed_data: topic: processed_sensor_data keyType: string valueType: json error_data: topic: error_sensor_data keyType: string valueType: json functions: validate_sensor_data: type: predicate code: | try: # Check if all required fields are present required_fields = ['temperature', 'humidity', 'location', 'timestamp'] for field in required_fields: if field not in value: log.warn(\"Missing required field '{}' in message: {}\", field, value) return False # Validate data types and ranges if not isinstance(value['temperature'], (int, float)) or value['temperature'] < -100 or value['temperature'] > 200: log.warn(\"Invalid temperature value: {}\", value['temperature']) return False if not isinstance(value['humidity'], (int, float)) or value['humidity'] < 0 or value['humidity'] > 100: log.warn(\"Invalid humidity value: {}\", value['humidity']) return False return True except Exception as e: log.error(\"Error validating sensor data: {} - Message: {}\", str(e), value) return False transform_sensor_data: type: valueMapper code: | try: # Convert temperature from F to C temp_c = (value['temperature'] - 32) * 5/9 # Calculate heat index heat_index = temp_c * 1.8 + 32 - 0.55 * (1 - value['humidity'] / 100) # Format timestamp timestamp = value['timestamp'] if isinstance(timestamp, (int, float)): # Assume it's a Unix timestamp from datetime import datetime formatted_time = datetime.fromtimestamp(timestamp / 1000).isoformat() else: # Pass through as is formatted_time = timestamp return { \"sensor_id\": key, \"location\": value['location'], \"readings\": { \"temperature\": { \"celsius\": round(temp_c, 2), \"fahrenheit\": value['temperature'] }, \"humidity\": value['humidity'], \"heat_index\": round(heat_index, 2) }, \"timestamp\": formatted_time, \"processed_at\": int(time.time() * 1000) } except Exception as e: log.error(\"Error transforming sensor data: {} - Message: {}\", str(e), value) return { \"error\": str(e), \"original\": value, \"sensor_id\": key, \"timestamp\": int(time.time() * 1000) } pipelines: process_sensor_data: from: sensor_data via: - type: filter if: validate_sensor_data - type: mapValues mapper: transform_sensor_data - type: peek forEach: code: | log.info(\"Processed sensor data for {}: temp={}\u00b0C, humidity={}%\", value.get('sensor_id'), value.get('readings', {}).get('temperature', {}).get('celsius'), value.get('readings', {}).get('humidity')) branch: - if: expression: 'error' in value to: error_data - to: processed_data This complete example: Validates incoming sensor data, filtering out invalid messages Transforms valid data, converting temperatures and calculating derived values Logs information about processed messages Routes messages with errors to an error topic and valid messages to a processed data topic Conclusion In this tutorial, you've learned how to: Create complex filters with multiple conditions Implement custom filter functions with business logic Handle errors in filtering and transformation Transform nested data structures Apply multiple transformations in sequence Combine filtering and transformation in a robust pipeline These techniques will help you build more sophisticated and reliable KSML applications that can handle real-world data processing challenges. Next Steps Learn about working with different data formats in KSML Explore logging and monitoring to better understand your pipelines Move on to intermediate tutorials to learn about stateful operations and joins","title":"Filtering and Transforming Data in KSML"},{"location":"tutorials/beginner/filtering-transforming/#filtering-and-transforming-data-in-ksml","text":"This tutorial builds on the KSML Basics Tutorial and explores more advanced filtering and transformation techniques in KSML. You'll learn how to work with complex filter conditions, apply multiple transformations, handle nested data structures, and implement error handling in your transformations.","title":"Filtering and Transforming Data in KSML"},{"location":"tutorials/beginner/filtering-transforming/#what-youll-build","text":"In this tutorial, you'll build a data pipeline that: Reads sensor data from a Kafka topic Applies complex filtering based on multiple conditions Transforms the data using various techniques Handles potential errors in the transformation process Writes the processed data to another Kafka topic","title":"What You'll Build"},{"location":"tutorials/beginner/filtering-transforming/#prerequisites","text":"Before you begin, make sure you have: Completed the KSML Basics Tutorial A running KSML environment with Kafka Basic understanding of YAML and Python syntax","title":"Prerequisites"},{"location":"tutorials/beginner/filtering-transforming/#complex-filtering-techniques","text":"","title":"Complex Filtering Techniques"},{"location":"tutorials/beginner/filtering-transforming/#using-multiple-conditions","text":"Let's start by creating a filter that combines multiple conditions: streams: input_stream: topic: tutorial_input keyType: string valueType: json output_stream: topic: filtered_data keyType: string valueType: json pipelines: filtering_pipeline: from: input_stream via: - type: filter if: expression: value.get('temperature', 0) > 70 and value.get('humidity', 99) < 50 and value.get('location', '') == 'warehouse' to: output_stream This filter only passes messages where: - The temperature is greater than 70\u00b0F - The humidity is less than 50% - The location is 'warehouse'","title":"Using Multiple Conditions"},{"location":"tutorials/beginner/filtering-transforming/#using-custom-filter-functions","text":"For more complex filtering logic, you can create a custom filter function: streams: input_stream: topic: tutorial_input keyType: string valueType: json alerts_stream: topic: alerts_stream keyType: string valueType: json functions: is_critical_sensor: type: predicate code: | # Check if this is a critical sensor based on multiple criteria if value.get('type') != 'temperature': return False # Check location if value.get('location') not in ['server_room', 'data_center']: return False # Check temperature threshold based on location if value.get('location') == 'server_room' and value.get('temperature') > 80: return True if value.get('location') == 'data_center' and value.get('temperature') > 75: return True return False pipelines: critical_alerts: from: input_stream via: - type: filter if: is_critical_sensor to: alerts_stream This function implements complex business logic to determine if a sensor reading indicates a critical situation that requires an alert.","title":"Using Custom Filter Functions"},{"location":"tutorials/beginner/filtering-transforming/#filtering-with-error-handling","text":"Sometimes your filter conditions might encounter malformed data. Here's how to handle that: functions: safe_filter: type: predicate code: | try: # Attempt to apply our filter logic if 'temperature' not in value or 'humidity' not in value: log.warn(\"Missing required fields in message: {}\", value) return False return value['temperature'] > 70 and value['humidity'] < 50 except Exception as e: # Log the error and filter out the message log.error(\"Error in filter: {} - Message: {}\", str(e), value) return False pipelines: robust_filtering: from: input_stream via: - type: filter if: safe_filter to: output_stream This approach ensures that malformed messages are logged and filtered out rather than causing the pipeline to fail.","title":"Filtering with Error Handling"},{"location":"tutorials/beginner/filtering-transforming/#advanced-transformation-techniques","text":"","title":"Advanced Transformation Techniques"},{"location":"tutorials/beginner/filtering-transforming/#transforming-nested-data-structures","text":"Let's look at how to transform data with nested structures: functions: transform_nested_data: type: valueMapper code: | # Create a new structure with flattened and transformed data result = { \"device_id\": key, \"timestamp\": value.get('metadata', {}).get('timestamp'), \"readings\": {} } # Extract and transform sensor readings sensors = value.get('sensors', {}) for sensor_type, reading in sensors.items(): # Convert temperature from F to C if needed if sensor_type == 'temperature' and reading.get('unit') == 'F': celsius = (reading.get('value') - 32) * 5/9 result['readings'][sensor_type] = { 'value': celsius, 'unit': 'C', 'original_value': reading.get('value'), 'original_unit': 'F' } else: result['readings'][sensor_type] = reading return result pipelines: transform_pipeline: from: input_stream via: - type: mapValues mapper: transform_nested_data to: output_stream This transformation function handles a complex nested JSON structure, extracting and transforming specific fields while preserving others.","title":"Transforming Nested Data Structures"},{"location":"tutorials/beginner/filtering-transforming/#applying-multiple-transformations","text":"You can chain multiple transformations to break down complex logic into manageable steps: pipelines: multi_transform_pipeline: from: input_stream via: # Step 1: Extract relevant fields - type: mapValues mapper: expression: { \"device_id\": key, \"temperature\": value.get('sensors', {}).get('temperature', {}).get('value'), \"humidity\": value.get('sensors', {}).get('humidity', {}).get('value'), \"timestamp\": value.get('metadata', {}).get('timestamp') } # Step 2: Convert temperature from F to C - type: mapValues mapper: expression: { \"device_id\": value.get('device_id'), \"temperature_c\": (value.get('temperature') - 32) * 5/9, \"humidity\": value.get('humidity'), \"timestamp\": value.get('timestamp') } # Step 3: Add calculated fields - type: mapValues mapper: expression: { \"device_id\": value.get('device_id'), \"temperature_c\": value.get('temperature_c'), \"humidity\": value.get('humidity'), \"heat_index\": value.get('temperature_c') * 1.8 + 32 - 0.55 * (1 - value.get('humidity') / 100), \"timestamp\": value.get('timestamp') } to: output_stream Breaking transformations into steps makes your pipeline easier to understand and maintain.","title":"Applying Multiple Transformations"},{"location":"tutorials/beginner/filtering-transforming/#error-handling-in-transformations","text":"Here's how to implement robust error handling in transformations: functions: safe_transform: type: valueMapper code: | try: # Attempt the transformation if 'temperature' not in value: log.warn(\"Missing temperature field in message: {}\", value) return {\"error\": \"Missing temperature field\", \"original\": value} celsius = (value['temperature'] - 32) * 5/9 return { \"device_id\": key, \"temperature_f\": value['temperature'], \"temperature_c\": celsius, \"status\": \"processed\" } except Exception as e: # Log the error and return a special error message log.error(\"Error in transformation: {} - Message: {}\", str(e), value) return { \"error\": str(e), \"original\": value, \"status\": \"error\" } pipelines: robust_transformation: from: input_stream via: - type: mapValues mapper: safe_transform # You could add a branch here to handle error messages differently to: output_stream This approach ensures that transformation errors are caught, logged, and handled gracefully without crashing the pipeline.","title":"Error Handling in Transformations"},{"location":"tutorials/beginner/filtering-transforming/#combining-filtering-and-transformation","text":"Let's put everything together in a complete example: streams: sensor_data: topic: raw_sensor_data keyType: string valueType: json processed_data: topic: processed_sensor_data keyType: string valueType: json error_data: topic: error_sensor_data keyType: string valueType: json functions: validate_sensor_data: type: predicate code: | try: # Check if all required fields are present required_fields = ['temperature', 'humidity', 'location', 'timestamp'] for field in required_fields: if field not in value: log.warn(\"Missing required field '{}' in message: {}\", field, value) return False # Validate data types and ranges if not isinstance(value['temperature'], (int, float)) or value['temperature'] < -100 or value['temperature'] > 200: log.warn(\"Invalid temperature value: {}\", value['temperature']) return False if not isinstance(value['humidity'], (int, float)) or value['humidity'] < 0 or value['humidity'] > 100: log.warn(\"Invalid humidity value: {}\", value['humidity']) return False return True except Exception as e: log.error(\"Error validating sensor data: {} - Message: {}\", str(e), value) return False transform_sensor_data: type: valueMapper code: | try: # Convert temperature from F to C temp_c = (value['temperature'] - 32) * 5/9 # Calculate heat index heat_index = temp_c * 1.8 + 32 - 0.55 * (1 - value['humidity'] / 100) # Format timestamp timestamp = value['timestamp'] if isinstance(timestamp, (int, float)): # Assume it's a Unix timestamp from datetime import datetime formatted_time = datetime.fromtimestamp(timestamp / 1000).isoformat() else: # Pass through as is formatted_time = timestamp return { \"sensor_id\": key, \"location\": value['location'], \"readings\": { \"temperature\": { \"celsius\": round(temp_c, 2), \"fahrenheit\": value['temperature'] }, \"humidity\": value['humidity'], \"heat_index\": round(heat_index, 2) }, \"timestamp\": formatted_time, \"processed_at\": int(time.time() * 1000) } except Exception as e: log.error(\"Error transforming sensor data: {} - Message: {}\", str(e), value) return { \"error\": str(e), \"original\": value, \"sensor_id\": key, \"timestamp\": int(time.time() * 1000) } pipelines: process_sensor_data: from: sensor_data via: - type: filter if: validate_sensor_data - type: mapValues mapper: transform_sensor_data - type: peek forEach: code: | log.info(\"Processed sensor data for {}: temp={}\u00b0C, humidity={}%\", value.get('sensor_id'), value.get('readings', {}).get('temperature', {}).get('celsius'), value.get('readings', {}).get('humidity')) branch: - if: expression: 'error' in value to: error_data - to: processed_data This complete example: Validates incoming sensor data, filtering out invalid messages Transforms valid data, converting temperatures and calculating derived values Logs information about processed messages Routes messages with errors to an error topic and valid messages to a processed data topic","title":"Combining Filtering and Transformation"},{"location":"tutorials/beginner/filtering-transforming/#conclusion","text":"In this tutorial, you've learned how to: Create complex filters with multiple conditions Implement custom filter functions with business logic Handle errors in filtering and transformation Transform nested data structures Apply multiple transformations in sequence Combine filtering and transformation in a robust pipeline These techniques will help you build more sophisticated and reliable KSML applications that can handle real-world data processing challenges.","title":"Conclusion"},{"location":"tutorials/beginner/filtering-transforming/#next-steps","text":"Learn about working with different data formats in KSML Explore logging and monitoring to better understand your pipelines Move on to intermediate tutorials to learn about stateful operations and joins","title":"Next Steps"},{"location":"tutorials/beginner/logging-monitoring/","text":"Logging and Monitoring in KSML This tutorial guides you through implementing effective logging and monitoring in your KSML applications. By the end, you'll understand how to track your pipeline's behavior, debug issues, and monitor performance. Prerequisites Basic understanding of KSML concepts Completed the Building a Simple Data Pipeline tutorial A running Kafka environment (as set up in the previous tutorial) What You'll Learn In this tutorial, you'll learn how to: 1. Implement different levels of logging in KSML 2. Use the peek operation for monitoring message flow 3. Create custom monitoring functions 4. Configure logging settings 5. Implement basic error handling with logging Understanding Logging in KSML KSML provides built-in logging capabilities through the log object, which is available in Python functions and expressions. The logging system follows standard logging levels: ERROR : For error events that might still allow the application to continue running WARN : For potentially harmful situations INFO : For informational messages highlighting the progress of the application DEBUG : For detailed information, typically useful only when diagnosing problems TRACE : For even more detailed information than DEBUG Basic Logging Examples Step 1: Create a KSML File with Logging Create a file named logging-example.yaml with the following content: streams: input_stream: topic: logging-input keyType: string valueType: json output_stream: topic: logging-output keyType: string valueType: json functions: log_with_level: type: forEach parameters: - name: level type: string - name: message type: string code: | if level == \"ERROR\": log.error(message) elif level == \"WARN\": log.warn(message) elif level == \"INFO\": log.info(message) elif level == \"DEBUG\": log.debug(message) elif level == \"TRACE\": log.trace(message) else: log.info(message) pipelines: logging_pipeline: from: input_stream via: - type: peek forEach: functionRef: log_with_level args: level: \"INFO\" message: \"Received message with key: {} and value: {}\" - type: filter if: expression: value.get('importance') > 3 - type: peek forEach: functionRef: log_with_level args: level: \"DEBUG\" message: \"Message passed filter: {}\" - type: mapValues mapper: expression: dict(list(value.items()) + [(\"processed_at\", time.time())]) - type: peek forEach: code: | log.info(\"Sending processed message to output: {}\", value) to: output_stream This pipeline: 1. Logs every incoming message at INFO level 2. Filters messages based on an 'importance' field 3. Logs messages that pass the filter at DEBUG level 4. Adds a timestamp to each message 5. Logs the final processed message before sending it to the output topic Step 2: Create Kafka Topics Create the input and output topics: docker-compose exec kafka kafka-topics --create --topic logging-input --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1 docker-compose exec kafka kafka-topics --create --topic logging-output --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1 Step 3: Run the Pipeline with Different Log Levels Run the pipeline with different log levels to see how it affects the output: # Run with INFO level (default) ksml-runner --config logging-example.yaml # Run with DEBUG level to see more detailed logs ksml-runner --config logging-example.yaml --log-level DEBUG # Run with TRACE level to see all logs ksml-runner --config logging-example.yaml --log-level TRACE Step 4: Produce Test Messages In a new terminal, produce some test messages: docker-compose exec kafka kafka-console-producer --topic logging-input --bootstrap-server localhost:9092 --property \"parse.key=true\" --property \"key.separator=:\" Enter messages in the format key:value , for example: message1:{\"importance\": 5, \"content\": \"High importance message\"} message2:{\"importance\": 2, \"content\": \"Low importance message\"} message3:{\"importance\": 4, \"content\": \"Medium-high importance message\"} Advanced Logging Techniques Creating a Detailed Logging Function Add a more detailed logging function to your KSML file: functions: detailed_logger: type: forEach parameters: - name: stage type: string code: | timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\") log.info(\"[{}] Stage: {} | Key: {} | Value Type: {} | Value: {}\", timestamp, stage, key, type(value).__name__, value) Use this function in your pipeline: - type: peek forEach: functionRef: detailed_logger args: stage: \"Pre-processing\" Conditional Logging You can implement conditional logging to only log messages that meet certain criteria: functions: conditional_logger: type: forEach code: | if \"error\" in value or value.get(\"status\") == \"failed\": log.error(\"Error detected in message: {}\", value) elif value.get(\"importance\", 0) > 8: log.warn(\"High importance message detected: {}\", value) Monitoring Message Flow Using Peek for Monitoring The peek operation is a powerful tool for monitoring message flow without modifying the messages: - type: peek forEach: code: | global message_count if 'message_count' not in globals(): message_count = 0 message_count += 1 if message_count % 100 == 0: log.info(\"Processed {} messages\", message_count) Monitoring Processing Time You can monitor how long operations take: functions: time_operation: type: mapper parameters: - name: operation_name type: string code: | start_time = time.time() result = yield end_time = time.time() log.info(\"Operation '{}' took {:.3f} ms\", operation_name, (end_time - start_time) * 1000) return result Use this function to wrap operations: - type: process processor: functionRef: time_operation args: operation_name: \"Complex Transformation\" next: - type: mapValues mapper: expression: # Your complex transformation here Error Handling with Logging Catching and Logging Errors Use try-except blocks in your Python functions to catch and log errors: functions: safe_transform: type: mapper code: | try: # Attempt the transformation result = {\"processed\": value.get(\"data\") * 2, \"status\": \"success\"} return result except Exception as e: # Log the error and return a fallback value log.error(\"Error processing message: {} - Error: {}\", value, str(e)) return {\"processed\": None, \"status\": \"error\", \"error_message\": str(e)} Configuring Logging Log Configuration in KSML Runner The KSML Runner supports various logging configuration options: # Set log level ksml-runner --config your-pipeline.yaml --log-level INFO # Log to a file ksml-runner --config your-pipeline.yaml --log-file pipeline.log # Configure log format ksml-runner --config your-pipeline.yaml --log-format \"%(asctime)s [%(levelname)s] %(message)s\" Best Practices for Logging and Monitoring Use appropriate log levels : ERROR: For actual errors that need attention WARN: For potential issues or unusual conditions INFO: For normal but significant events DEBUG: For detailed troubleshooting information TRACE: For very detailed debugging information Include context in log messages : Message keys and values (or summaries for large values) Operation or stage name Timestamps Relevant metrics or counters Avoid excessive logging : Don't log every message at high volume Use sampling or periodic logging for high-throughput pipelines Consider conditional logging based on message content Structure your logs : Use consistent formats Include key-value pairs for easier parsing Consider using JSON formatting for machine-readable logs Monitor performance metrics : Message throughput Processing time Error rates Resource usage (memory, CPU) Next Steps Now that you've learned about logging and monitoring in KSML, you can: Explore intermediate tutorials to learn about more advanced KSML features Learn about error handling and recovery in more detail Dive into performance optimization techniques Conclusion Effective logging and monitoring are essential for building robust KSML applications. By implementing the techniques covered in this tutorial, you'll be able to: Track the behavior of your pipelines Quickly identify and diagnose issues Monitor performance and resource usage Create more maintainable and reliable applications Remember that good logging practices are a balance between capturing enough information to be useful and avoiding excessive logging that can impact performance.","title":"Logging and Monitoring in KSML"},{"location":"tutorials/beginner/logging-monitoring/#logging-and-monitoring-in-ksml","text":"This tutorial guides you through implementing effective logging and monitoring in your KSML applications. By the end, you'll understand how to track your pipeline's behavior, debug issues, and monitor performance.","title":"Logging and Monitoring in KSML"},{"location":"tutorials/beginner/logging-monitoring/#prerequisites","text":"Basic understanding of KSML concepts Completed the Building a Simple Data Pipeline tutorial A running Kafka environment (as set up in the previous tutorial)","title":"Prerequisites"},{"location":"tutorials/beginner/logging-monitoring/#what-youll-learn","text":"In this tutorial, you'll learn how to: 1. Implement different levels of logging in KSML 2. Use the peek operation for monitoring message flow 3. Create custom monitoring functions 4. Configure logging settings 5. Implement basic error handling with logging","title":"What You'll Learn"},{"location":"tutorials/beginner/logging-monitoring/#understanding-logging-in-ksml","text":"KSML provides built-in logging capabilities through the log object, which is available in Python functions and expressions. The logging system follows standard logging levels: ERROR : For error events that might still allow the application to continue running WARN : For potentially harmful situations INFO : For informational messages highlighting the progress of the application DEBUG : For detailed information, typically useful only when diagnosing problems TRACE : For even more detailed information than DEBUG","title":"Understanding Logging in KSML"},{"location":"tutorials/beginner/logging-monitoring/#basic-logging-examples","text":"","title":"Basic Logging Examples"},{"location":"tutorials/beginner/logging-monitoring/#step-1-create-a-ksml-file-with-logging","text":"Create a file named logging-example.yaml with the following content: streams: input_stream: topic: logging-input keyType: string valueType: json output_stream: topic: logging-output keyType: string valueType: json functions: log_with_level: type: forEach parameters: - name: level type: string - name: message type: string code: | if level == \"ERROR\": log.error(message) elif level == \"WARN\": log.warn(message) elif level == \"INFO\": log.info(message) elif level == \"DEBUG\": log.debug(message) elif level == \"TRACE\": log.trace(message) else: log.info(message) pipelines: logging_pipeline: from: input_stream via: - type: peek forEach: functionRef: log_with_level args: level: \"INFO\" message: \"Received message with key: {} and value: {}\" - type: filter if: expression: value.get('importance') > 3 - type: peek forEach: functionRef: log_with_level args: level: \"DEBUG\" message: \"Message passed filter: {}\" - type: mapValues mapper: expression: dict(list(value.items()) + [(\"processed_at\", time.time())]) - type: peek forEach: code: | log.info(\"Sending processed message to output: {}\", value) to: output_stream This pipeline: 1. Logs every incoming message at INFO level 2. Filters messages based on an 'importance' field 3. Logs messages that pass the filter at DEBUG level 4. Adds a timestamp to each message 5. Logs the final processed message before sending it to the output topic","title":"Step 1: Create a KSML File with Logging"},{"location":"tutorials/beginner/logging-monitoring/#step-2-create-kafka-topics","text":"Create the input and output topics: docker-compose exec kafka kafka-topics --create --topic logging-input --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1 docker-compose exec kafka kafka-topics --create --topic logging-output --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1","title":"Step 2: Create Kafka Topics"},{"location":"tutorials/beginner/logging-monitoring/#step-3-run-the-pipeline-with-different-log-levels","text":"Run the pipeline with different log levels to see how it affects the output: # Run with INFO level (default) ksml-runner --config logging-example.yaml # Run with DEBUG level to see more detailed logs ksml-runner --config logging-example.yaml --log-level DEBUG # Run with TRACE level to see all logs ksml-runner --config logging-example.yaml --log-level TRACE","title":"Step 3: Run the Pipeline with Different Log Levels"},{"location":"tutorials/beginner/logging-monitoring/#step-4-produce-test-messages","text":"In a new terminal, produce some test messages: docker-compose exec kafka kafka-console-producer --topic logging-input --bootstrap-server localhost:9092 --property \"parse.key=true\" --property \"key.separator=:\" Enter messages in the format key:value , for example: message1:{\"importance\": 5, \"content\": \"High importance message\"} message2:{\"importance\": 2, \"content\": \"Low importance message\"} message3:{\"importance\": 4, \"content\": \"Medium-high importance message\"}","title":"Step 4: Produce Test Messages"},{"location":"tutorials/beginner/logging-monitoring/#advanced-logging-techniques","text":"","title":"Advanced Logging Techniques"},{"location":"tutorials/beginner/logging-monitoring/#creating-a-detailed-logging-function","text":"Add a more detailed logging function to your KSML file: functions: detailed_logger: type: forEach parameters: - name: stage type: string code: | timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\") log.info(\"[{}] Stage: {} | Key: {} | Value Type: {} | Value: {}\", timestamp, stage, key, type(value).__name__, value) Use this function in your pipeline: - type: peek forEach: functionRef: detailed_logger args: stage: \"Pre-processing\"","title":"Creating a Detailed Logging Function"},{"location":"tutorials/beginner/logging-monitoring/#conditional-logging","text":"You can implement conditional logging to only log messages that meet certain criteria: functions: conditional_logger: type: forEach code: | if \"error\" in value or value.get(\"status\") == \"failed\": log.error(\"Error detected in message: {}\", value) elif value.get(\"importance\", 0) > 8: log.warn(\"High importance message detected: {}\", value)","title":"Conditional Logging"},{"location":"tutorials/beginner/logging-monitoring/#monitoring-message-flow","text":"","title":"Monitoring Message Flow"},{"location":"tutorials/beginner/logging-monitoring/#using-peek-for-monitoring","text":"The peek operation is a powerful tool for monitoring message flow without modifying the messages: - type: peek forEach: code: | global message_count if 'message_count' not in globals(): message_count = 0 message_count += 1 if message_count % 100 == 0: log.info(\"Processed {} messages\", message_count)","title":"Using Peek for Monitoring"},{"location":"tutorials/beginner/logging-monitoring/#monitoring-processing-time","text":"You can monitor how long operations take: functions: time_operation: type: mapper parameters: - name: operation_name type: string code: | start_time = time.time() result = yield end_time = time.time() log.info(\"Operation '{}' took {:.3f} ms\", operation_name, (end_time - start_time) * 1000) return result Use this function to wrap operations: - type: process processor: functionRef: time_operation args: operation_name: \"Complex Transformation\" next: - type: mapValues mapper: expression: # Your complex transformation here","title":"Monitoring Processing Time"},{"location":"tutorials/beginner/logging-monitoring/#error-handling-with-logging","text":"","title":"Error Handling with Logging"},{"location":"tutorials/beginner/logging-monitoring/#catching-and-logging-errors","text":"Use try-except blocks in your Python functions to catch and log errors: functions: safe_transform: type: mapper code: | try: # Attempt the transformation result = {\"processed\": value.get(\"data\") * 2, \"status\": \"success\"} return result except Exception as e: # Log the error and return a fallback value log.error(\"Error processing message: {} - Error: {}\", value, str(e)) return {\"processed\": None, \"status\": \"error\", \"error_message\": str(e)}","title":"Catching and Logging Errors"},{"location":"tutorials/beginner/logging-monitoring/#configuring-logging","text":"","title":"Configuring Logging"},{"location":"tutorials/beginner/logging-monitoring/#log-configuration-in-ksml-runner","text":"The KSML Runner supports various logging configuration options: # Set log level ksml-runner --config your-pipeline.yaml --log-level INFO # Log to a file ksml-runner --config your-pipeline.yaml --log-file pipeline.log # Configure log format ksml-runner --config your-pipeline.yaml --log-format \"%(asctime)s [%(levelname)s] %(message)s\"","title":"Log Configuration in KSML Runner"},{"location":"tutorials/beginner/logging-monitoring/#best-practices-for-logging-and-monitoring","text":"Use appropriate log levels : ERROR: For actual errors that need attention WARN: For potential issues or unusual conditions INFO: For normal but significant events DEBUG: For detailed troubleshooting information TRACE: For very detailed debugging information Include context in log messages : Message keys and values (or summaries for large values) Operation or stage name Timestamps Relevant metrics or counters Avoid excessive logging : Don't log every message at high volume Use sampling or periodic logging for high-throughput pipelines Consider conditional logging based on message content Structure your logs : Use consistent formats Include key-value pairs for easier parsing Consider using JSON formatting for machine-readable logs Monitor performance metrics : Message throughput Processing time Error rates Resource usage (memory, CPU)","title":"Best Practices for Logging and Monitoring"},{"location":"tutorials/beginner/logging-monitoring/#next-steps","text":"Now that you've learned about logging and monitoring in KSML, you can: Explore intermediate tutorials to learn about more advanced KSML features Learn about error handling and recovery in more detail Dive into performance optimization techniques","title":"Next Steps"},{"location":"tutorials/beginner/logging-monitoring/#conclusion","text":"Effective logging and monitoring are essential for building robust KSML applications. By implementing the techniques covered in this tutorial, you'll be able to: Track the behavior of your pipelines Quickly identify and diagnose issues Monitor performance and resource usage Create more maintainable and reliable applications Remember that good logging practices are a balance between capturing enough information to be useful and avoiding excessive logging that can impact performance.","title":"Conclusion"},{"location":"tutorials/beginner/simple-pipeline/","text":"Building a Simple Data Pipeline This tutorial guides you through building a simple KSML data pipeline. By the end, you'll understand the basic components of KSML and how to create a functional data processing application. Prerequisites Basic understanding of Kafka concepts (topics, messages) Docker installed for running the example environment Completed the KSML Basics Tutorial What You'll Build In this tutorial, you'll build a simple data pipeline that: 1. Reads messages from an input topic 2. Filters messages based on a condition 3. Transforms the remaining messages 4. Writes the results to an output topic Here's a visual representation of what we'll build: Input Topic \u2192 Filter \u2192 Transform \u2192 Output Topic Setting Up Your Environment Start a Local Kafka Cluster Create a new directory for your project Create a docker-compose.yml file with the following content: version: '3' services: zookeeper: image: confluentinc/cp-zookeeper:7.3.0 environment: ZOOKEEPER_CLIENT_PORT: 2181 ports: - \"2181:2181\" kafka: image: confluentinc/cp-kafka:7.3.0 depends_on: - zookeeper ports: - \"9092:9092\" environment: KAFKA_BROKER_ID: 1 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092 KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 Start the services: docker-compose up -d Verify that the services are running: docker-compose ps Create Kafka Topics Create the input and output topics: docker-compose exec kafka kafka-topics --create --topic tutorial-input --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1 docker-compose exec kafka kafka-topics --create --topic tutorial-output --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1 Creating Your KSML Pipeline Step 1: Define Your Streams Create a file named simple-pipeline.yaml with the following content: streams: input_stream: topic: tutorial-input keyType: string valueType: json output_stream: topic: tutorial-output keyType: string valueType: json This defines: - An input stream connected to the tutorial-input topic - An output stream connected to the tutorial-output topic - Both streams use string keys and JSON values Step 2: Create a Simple Function Add a function to log messages as they flow through the pipeline: functions: log_message: type: forEach parameters: - name: message_type type: string code: log.info(\"{} message - key={}, value={}\", message_type, key, value) This function: - Takes a parameter called message_type - Logs the message type, key, and value - Uses the built-in log object to write to the application logs Step 3: Build Your Pipeline Now, add the pipeline definition: pipelines: tutorial_pipeline: from: input_stream via: - type: filter if: expression: value.get('temperature') > 70 - type: mapValues mapper: expression: {\"sensor\": key, \"temp_fahrenheit\": value.get('temperature'), \"temp_celsius\": (value.get('temperature') - 32) * 5/9} - type: peek forEach: functionRef: log_message args: message_type: \"Processed\" to: output_stream This pipeline: 1. Starts from the input_stream 2. Filters messages to keep only those with a temperature greater than 70\u00b0F 3. Transforms each message to include both Fahrenheit and Celsius temperatures 4. Logs each processed message 5. Sends the results to the output_stream Running Your Pipeline Start the KSML Runner Install the KSML Runner (if not already installed) Run your pipeline: ksml-runner --config simple-pipeline.yaml Produce Test Messages In a new terminal, produce some test messages: docker-compose exec kafka kafka-console-producer --topic tutorial-input --bootstrap-server localhost:9092 --property \"parse.key=true\" --property \"key.separator=:\" Enter messages in the format key:value , for example: sensor1:{\"temperature\": 75, \"humidity\": 45} sensor2:{\"temperature\": 65, \"humidity\": 50} sensor3:{\"temperature\": 80, \"humidity\": 40} Observe the Output In another terminal, consume messages from the output topic: docker-compose exec kafka kafka-console-consumer --topic tutorial-output --bootstrap-server localhost:9092 --from-beginning You should see only messages with temperatures above 70\u00b0F, transformed to include Celsius values: {\"sensor\":\"sensor1\",\"temp_fahrenheit\":75,\"temp_celsius\":23.88888888888889} {\"sensor\":\"sensor3\",\"temp_fahrenheit\":80,\"temp_celsius\":26.666666666666668} Understanding the Pipeline The Filter Operation - type: filter if: expression: value.get('temperature') > 70 This operation: - Examines each message and decides whether to keep it or discard it - Evaluates the Python expression value.get('temperature') > 70 - Only keeps messages where the temperature is greater than 70\u00b0F - Discards messages that don't meet this condition The MapValues Operation - type: mapValues mapper: expression: {\"sensor\": key, \"temp_fahrenheit\": value.get('temperature'), \"temp_celsius\": (value.get('temperature') - 32) * 5/9} This operation: - Transforms the value of each message - Creates a new JSON object with: - The sensor ID (from the key) - The original temperature in Fahrenheit - The temperature converted to Celsius - Preserves the original key The Peek Operation - type: peek forEach: functionRef: log_message args: message_type: \"Processed\" This operation: - Calls the log_message function for each message - Passes \"Processed\" as the message_type parameter - Doesn't modify the message - Useful for debugging and monitoring Exploring and Modifying Try these modifications to enhance your pipeline: Add a temperature unit to the output: - type: mapValues mapper: expression: {\"sensor\": key, \"temp_fahrenheit\": str(value.get('temperature')) + \"\u00b0F\", \"temp_celsius\": str(round((value.get('temperature') - 32) * 5/9, 1)) + \"\u00b0C\"} Filter based on multiple conditions: - type: filter if: expression: value.get('temperature') > 70 and value.get('humidity') < 50 Add a timestamp to each message: - type: mapValues mapper: expression: dict(list(value.items()) + [(\"timestamp\", time.time())]) Next Steps Now that you've built a simple pipeline, you can: Learn about filtering and transforming data in more detail Explore logging and monitoring techniques Dive into intermediate tutorials to learn about aggregations and joins Conclusion In this tutorial, you've learned how to: - Set up a local Kafka environment - Define streams in KSML - Create a simple function - Build a pipeline with filter, transform, and logging operations - Run your pipeline and observe the results These fundamental skills form the building blocks for more complex KSML applications.","title":"Building a Simple Data Pipeline"},{"location":"tutorials/beginner/simple-pipeline/#building-a-simple-data-pipeline","text":"This tutorial guides you through building a simple KSML data pipeline. By the end, you'll understand the basic components of KSML and how to create a functional data processing application.","title":"Building a Simple Data Pipeline"},{"location":"tutorials/beginner/simple-pipeline/#prerequisites","text":"Basic understanding of Kafka concepts (topics, messages) Docker installed for running the example environment Completed the KSML Basics Tutorial","title":"Prerequisites"},{"location":"tutorials/beginner/simple-pipeline/#what-youll-build","text":"In this tutorial, you'll build a simple data pipeline that: 1. Reads messages from an input topic 2. Filters messages based on a condition 3. Transforms the remaining messages 4. Writes the results to an output topic Here's a visual representation of what we'll build: Input Topic \u2192 Filter \u2192 Transform \u2192 Output Topic","title":"What You'll Build"},{"location":"tutorials/beginner/simple-pipeline/#setting-up-your-environment","text":"","title":"Setting Up Your Environment"},{"location":"tutorials/beginner/simple-pipeline/#start-a-local-kafka-cluster","text":"Create a new directory for your project Create a docker-compose.yml file with the following content: version: '3' services: zookeeper: image: confluentinc/cp-zookeeper:7.3.0 environment: ZOOKEEPER_CLIENT_PORT: 2181 ports: - \"2181:2181\" kafka: image: confluentinc/cp-kafka:7.3.0 depends_on: - zookeeper ports: - \"9092:9092\" environment: KAFKA_BROKER_ID: 1 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092 KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 Start the services: docker-compose up -d Verify that the services are running: docker-compose ps","title":"Start a Local Kafka Cluster"},{"location":"tutorials/beginner/simple-pipeline/#create-kafka-topics","text":"Create the input and output topics: docker-compose exec kafka kafka-topics --create --topic tutorial-input --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1 docker-compose exec kafka kafka-topics --create --topic tutorial-output --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1","title":"Create Kafka Topics"},{"location":"tutorials/beginner/simple-pipeline/#creating-your-ksml-pipeline","text":"","title":"Creating Your KSML Pipeline"},{"location":"tutorials/beginner/simple-pipeline/#step-1-define-your-streams","text":"Create a file named simple-pipeline.yaml with the following content: streams: input_stream: topic: tutorial-input keyType: string valueType: json output_stream: topic: tutorial-output keyType: string valueType: json This defines: - An input stream connected to the tutorial-input topic - An output stream connected to the tutorial-output topic - Both streams use string keys and JSON values","title":"Step 1: Define Your Streams"},{"location":"tutorials/beginner/simple-pipeline/#step-2-create-a-simple-function","text":"Add a function to log messages as they flow through the pipeline: functions: log_message: type: forEach parameters: - name: message_type type: string code: log.info(\"{} message - key={}, value={}\", message_type, key, value) This function: - Takes a parameter called message_type - Logs the message type, key, and value - Uses the built-in log object to write to the application logs","title":"Step 2: Create a Simple Function"},{"location":"tutorials/beginner/simple-pipeline/#step-3-build-your-pipeline","text":"Now, add the pipeline definition: pipelines: tutorial_pipeline: from: input_stream via: - type: filter if: expression: value.get('temperature') > 70 - type: mapValues mapper: expression: {\"sensor\": key, \"temp_fahrenheit\": value.get('temperature'), \"temp_celsius\": (value.get('temperature') - 32) * 5/9} - type: peek forEach: functionRef: log_message args: message_type: \"Processed\" to: output_stream This pipeline: 1. Starts from the input_stream 2. Filters messages to keep only those with a temperature greater than 70\u00b0F 3. Transforms each message to include both Fahrenheit and Celsius temperatures 4. Logs each processed message 5. Sends the results to the output_stream","title":"Step 3: Build Your Pipeline"},{"location":"tutorials/beginner/simple-pipeline/#running-your-pipeline","text":"","title":"Running Your Pipeline"},{"location":"tutorials/beginner/simple-pipeline/#start-the-ksml-runner","text":"Install the KSML Runner (if not already installed) Run your pipeline: ksml-runner --config simple-pipeline.yaml","title":"Start the KSML Runner"},{"location":"tutorials/beginner/simple-pipeline/#produce-test-messages","text":"In a new terminal, produce some test messages: docker-compose exec kafka kafka-console-producer --topic tutorial-input --bootstrap-server localhost:9092 --property \"parse.key=true\" --property \"key.separator=:\" Enter messages in the format key:value , for example: sensor1:{\"temperature\": 75, \"humidity\": 45} sensor2:{\"temperature\": 65, \"humidity\": 50} sensor3:{\"temperature\": 80, \"humidity\": 40}","title":"Produce Test Messages"},{"location":"tutorials/beginner/simple-pipeline/#observe-the-output","text":"In another terminal, consume messages from the output topic: docker-compose exec kafka kafka-console-consumer --topic tutorial-output --bootstrap-server localhost:9092 --from-beginning You should see only messages with temperatures above 70\u00b0F, transformed to include Celsius values: {\"sensor\":\"sensor1\",\"temp_fahrenheit\":75,\"temp_celsius\":23.88888888888889} {\"sensor\":\"sensor3\",\"temp_fahrenheit\":80,\"temp_celsius\":26.666666666666668}","title":"Observe the Output"},{"location":"tutorials/beginner/simple-pipeline/#understanding-the-pipeline","text":"","title":"Understanding the Pipeline"},{"location":"tutorials/beginner/simple-pipeline/#the-filter-operation","text":"- type: filter if: expression: value.get('temperature') > 70 This operation: - Examines each message and decides whether to keep it or discard it - Evaluates the Python expression value.get('temperature') > 70 - Only keeps messages where the temperature is greater than 70\u00b0F - Discards messages that don't meet this condition","title":"The Filter Operation"},{"location":"tutorials/beginner/simple-pipeline/#the-mapvalues-operation","text":"- type: mapValues mapper: expression: {\"sensor\": key, \"temp_fahrenheit\": value.get('temperature'), \"temp_celsius\": (value.get('temperature') - 32) * 5/9} This operation: - Transforms the value of each message - Creates a new JSON object with: - The sensor ID (from the key) - The original temperature in Fahrenheit - The temperature converted to Celsius - Preserves the original key","title":"The MapValues Operation"},{"location":"tutorials/beginner/simple-pipeline/#the-peek-operation","text":"- type: peek forEach: functionRef: log_message args: message_type: \"Processed\" This operation: - Calls the log_message function for each message - Passes \"Processed\" as the message_type parameter - Doesn't modify the message - Useful for debugging and monitoring","title":"The Peek Operation"},{"location":"tutorials/beginner/simple-pipeline/#exploring-and-modifying","text":"Try these modifications to enhance your pipeline: Add a temperature unit to the output: - type: mapValues mapper: expression: {\"sensor\": key, \"temp_fahrenheit\": str(value.get('temperature')) + \"\u00b0F\", \"temp_celsius\": str(round((value.get('temperature') - 32) * 5/9, 1)) + \"\u00b0C\"} Filter based on multiple conditions: - type: filter if: expression: value.get('temperature') > 70 and value.get('humidity') < 50 Add a timestamp to each message: - type: mapValues mapper: expression: dict(list(value.items()) + [(\"timestamp\", time.time())])","title":"Exploring and Modifying"},{"location":"tutorials/beginner/simple-pipeline/#next-steps","text":"Now that you've built a simple pipeline, you can: Learn about filtering and transforming data in more detail Explore logging and monitoring techniques Dive into intermediate tutorials to learn about aggregations and joins","title":"Next Steps"},{"location":"tutorials/beginner/simple-pipeline/#conclusion","text":"In this tutorial, you've learned how to: - Set up a local Kafka environment - Define streams in KSML - Create a simple function - Build a pipeline with filter, transform, and logging operations - Run your pipeline and observe the results These fundamental skills form the building blocks for more complex KSML applications.","title":"Conclusion"},{"location":"tutorials/getting-started/","text":"Getting Started with KSML Welcome to the Getting Started section of the KSML documentation. This section will help you understand what KSML is, how to set it up, and how to create your first KSML application. In This Section Introduction to KSML Learn what KSML is, why it was created, and how it can help you build powerful stream processing applications without writing Java code. Installation and Setup Step-by-step instructions for installing KSML and setting up your development environment. KSML Basics Tutorial A hands-on tutorial that guides you through creating your first KSML application, explaining the core concepts along the way. Next Steps After completing the getting started guides, you'll be ready to: Explore Core Concepts to deepen your understanding of KSML Try the Beginner Tutorials to build more complex applications Browse the Reference Documentation for detailed information about KSML features","title":"Getting Started"},{"location":"tutorials/getting-started/#getting-started-with-ksml","text":"Welcome to the Getting Started section of the KSML documentation. This section will help you understand what KSML is, how to set it up, and how to create your first KSML application.","title":"Getting Started with KSML"},{"location":"tutorials/getting-started/#in-this-section","text":"","title":"In This Section"},{"location":"tutorials/getting-started/#introduction-to-ksml","text":"Learn what KSML is, why it was created, and how it can help you build powerful stream processing applications without writing Java code.","title":"Introduction to KSML"},{"location":"tutorials/getting-started/#installation-and-setup","text":"Step-by-step instructions for installing KSML and setting up your development environment.","title":"Installation and Setup"},{"location":"tutorials/getting-started/#ksml-basics-tutorial","text":"A hands-on tutorial that guides you through creating your first KSML application, explaining the core concepts along the way.","title":"KSML Basics Tutorial"},{"location":"tutorials/getting-started/#next-steps","text":"After completing the getting started guides, you'll be ready to: Explore Core Concepts to deepen your understanding of KSML Try the Beginner Tutorials to build more complex applications Browse the Reference Documentation for detailed information about KSML features","title":"Next Steps"},{"location":"tutorials/getting-started/basics-tutorial/","text":"KSML Basics Tutorial This tutorial will guide you through creating your first KSML data pipeline. By the end, you'll understand the basic components of KSML and how to create a simple but functional data processing application. What You'll Build In this tutorial, you'll build a simple data pipeline that: Reads temperature sensor data from a Kafka topic Filters out readings below a certain threshold Transforms the data by converting Fahrenheit to Celsius Writes the processed data to another Kafka topic Logs information about the processed messages Here's a visual representation of what we'll build: Input Topic \u2192 Filter \u2192 Transform \u2192 Output Topic \u2193 Logging Prerequisites Before you begin, make sure you have: Completed the Installation and Setup guide A running KSML environment with Kafka Basic understanding of YAML syntax Understanding the KSML File Structure A KSML definition file consists of three main sections: Streams : Define the input and output Kafka topics Functions : Define reusable code snippets Pipelines : Define the data processing flow Let's create each section step by step. Step 1: Define Your Streams First, let's define the input and output streams for our pipeline: streams: input_stream: topic: tutorial_input keyType: string valueType: json output_stream: topic: tutorial_output keyType: string valueType: json This defines: An input stream reading from the tutorial_input topic with string keys and JSON values An output stream writing to the tutorial_output topic with the same data types Understanding Stream Definitions Each stream definition includes: A unique name (e.g., input_stream ) The Kafka topic it connects to The data types for keys and values KSML supports various data types including: string : For text data json : For JSON-formatted data avro : For Avro-formatted data (requires schema) binary : For raw binary data And more Step 2: Create a Simple Function Next, let's create a function to log messages as they flow through our pipeline: functions: log_message: type: forEach parameters: - name: message_type type: string code: | log.info(\"{} message - key={}, value={}\", message_type, key, value) This function: Is named log_message Is of type forEach , which means it always gets two parameters key and value , and does not return a value Takes one additional parameter called message_type Uses the built-in log object to output information about each message Understanding Functions in KSML Functions in KSML: Can be reused across multiple operations in your pipelines Are written in Python Have access to pre-defined parameters based on function type Can take additional parameters for more flexibility Must return a value if so pre-defined by the function type Step 3: Build Your Pipeline Now, let's create the pipeline that processes our data: pipelines: tutorial_pipeline: from: input_stream via: - type: filter if: expression: value.get('temperature') > 70 - type: mapValues mapper: expression: | {\"sensor\": key, \"temp_fahrenheit\": value.get('temperature'), \"temp_celsius\": (value.get('temperature') - 32) * 5/9} - type: peek forEach: code: log_message(key, value, message_type=\"Processed\") to: output_stream This pipeline: Reads from input_stream Filters out messages where the temperature is 70\u00b0F or lower Transforms the values to include both Fahrenheit and Celsius temperatures Logs each processed message Writes the results to output_stream Understanding Pipeline Operations Let's break down each operation: Filter Operation - type: filter if: expression: value.get('temperature') > 70 The filter operation: Evaluates the expression for each message Only passes messages where the expression returns True Discards messages where the expression returns False Map Values Operation - type: mapValues mapper: expression: | {\"sensor\": key, \"temp_fahrenheit\": value.get('temperature'), \"temp_celsius\": (value.get('temperature') - 32) * 5/9} The mapValues operation: Transforms the value of each message Keeps the original key unchanged Creates a new value based on the expression In this case, creates a new JSON object with the original temperature and a calculated Celsius value Note that we put the expression on a new line in this example to force the KSML YAML parser to interpret the expression as a literal string for Python, instead of parsing it as part of the YAML syntax. Another way of achieving the same would be to surround the '{...}' with quotes, but in that case, be aware of consistent single/double quoting to not confuse the KSML parser and/or the Python interpreter. We generally recommend using the above notation for readability purposes. Peek Operation - type: peek forEach: code: log_message(key, value, message_type=\"Processed\") The peek operation: Executes the provided code for each message Doesn't modify the message Allows the message to continue through the pipeline Is useful for logging, metrics, or other side effects Step 4: Put It All Together Let's combine all the sections into a complete KSML definition file: streams: input_stream: topic: tutorial_input keyType: string valueType: json output_stream: topic: tutorial_output keyType: string valueType: json functions: log_message: type: forEach parameters: - name: message_type type: string code: | log.info(\"{} message - key={}, value={}\", message_type, key, value) pipelines: tutorial_pipeline: from: input_stream via: - type: filter if: expression: value.get('temperature') > 70 - type: mapValues mapper: expression: | {\"sensor\": key, \"temp_fahrenheit\": value.get('temperature'), \"temp_celsius\": (value.get('temperature') - 32) * 5/9} - type: peek forEach: code: log_message(key, value, message_type=\"Processed\") to: output_stream Save this file as tutorial.yaml . Step 5: Run Your Pipeline Now let's run the pipeline and see it in action: Make sure your Kafka environment is running Create the input and output topics: bash docker exec -it kafka kafka-topics --bootstrap-server localhost:9092 --create --topic tutorial_input --partitions 1 --replication-factor 1 docker exec -it kafka kafka-topics --bootstrap-server localhost:9092 --create --topic tutorial_output --partitions 1 --replication-factor 1 Run the KSML runner with your definition file: bash docker run -v $(pwd):/app -e KAFKA_BOOTSTRAP_SERVERS=kafka:9092 axual/ksml-runner:latest --definitions /app/tutorial.yaml In another terminal, produce some test messages to the input topic: bash docker exec -it kafka kafka-console-producer --bootstrap-server localhost:9092 --topic tutorial_input --property \"parse.key=true\" --property \"key.separator=:\" Then enter messages in the format key:value , for example: sensor1:{\"temperature\": 75} sensor2:{\"temperature\": 65} sensor3:{\"temperature\": 80} In a third terminal, consume messages from the output topic to see the results: bash docker exec -it kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic tutorial_output --from-beginning You should see messages like: {\"sensor\":\"sensor1\",\"temp_fahrenheit\":75,\"temp_celsius\":23.88888888888889} {\"sensor\":\"sensor3\",\"temp_fahrenheit\":80,\"temp_celsius\":26.666666666666668} Notice that the message with temperature 65\u00b0F was filtered out, and the remaining messages have been transformed to include the Celsius temperature. Understanding What's Happening When you run your KSML definition: The KSML runner parses your YAML file It creates a Kafka Streams topology based on your pipeline definition The topology starts consuming from the input topic Each message flows through the operations you defined: The filter operation drops messages with temperatures \u2264 70\u00b0F The mapValues operation transforms the remaining messages The peek operation logs each message The messages are written to the output topic Using KSML to produce messages While you can manually produce the above messages, KSML can also generate messages for you. See below for a KSML definition that would randomly generate test messages every three seconds. functions: generate_temperature_message: type: generator globalCode: | import random sensorCounter = 0 code: | global sensorCounter key = \"sensor\"+str(sensorCounter) # Set the key to return (\"sensor0\" to \"sensor9\") sensorCounter = (sensorCounter+1) % 10 # Increase the counter for next iteration value = {\"temperature\": random.randrange(150)} expression: (key, value) # Return a message tuple with the key and value resultType: (string, json) # Indicate the type of key and value producers: # Produce a temperature message every 3 seconds tutorial_producer: generator: generate_temperature_message interval: 3s to: topic: tutorial_input keyType: string valueType: json Next Steps Congratulations! You've built your first KSML data pipeline. Here are some ways to expand on what you've learned: Try These Modifications: Add another filter condition (e.g., filter by sensor name) Add more fields to the transformed output Create a second pipeline that processes the data differently Explore More Advanced Concepts: Learn about stateful operations like aggregations and joins Explore windowing operations for time-based processing Try working with different data formats Continue Your Learning Journey: Check out the beginner tutorials for more guided examples Explore the examples library for inspiration Dive into the reference documentation to learn about all available operations","title":"KSML Basics Tutorial"},{"location":"tutorials/getting-started/basics-tutorial/#ksml-basics-tutorial","text":"This tutorial will guide you through creating your first KSML data pipeline. By the end, you'll understand the basic components of KSML and how to create a simple but functional data processing application.","title":"KSML Basics Tutorial"},{"location":"tutorials/getting-started/basics-tutorial/#what-youll-build","text":"In this tutorial, you'll build a simple data pipeline that: Reads temperature sensor data from a Kafka topic Filters out readings below a certain threshold Transforms the data by converting Fahrenheit to Celsius Writes the processed data to another Kafka topic Logs information about the processed messages Here's a visual representation of what we'll build: Input Topic \u2192 Filter \u2192 Transform \u2192 Output Topic \u2193 Logging","title":"What You'll Build"},{"location":"tutorials/getting-started/basics-tutorial/#prerequisites","text":"Before you begin, make sure you have: Completed the Installation and Setup guide A running KSML environment with Kafka Basic understanding of YAML syntax","title":"Prerequisites"},{"location":"tutorials/getting-started/basics-tutorial/#understanding-the-ksml-file-structure","text":"A KSML definition file consists of three main sections: Streams : Define the input and output Kafka topics Functions : Define reusable code snippets Pipelines : Define the data processing flow Let's create each section step by step.","title":"Understanding the KSML File Structure"},{"location":"tutorials/getting-started/basics-tutorial/#step-1-define-your-streams","text":"First, let's define the input and output streams for our pipeline: streams: input_stream: topic: tutorial_input keyType: string valueType: json output_stream: topic: tutorial_output keyType: string valueType: json This defines: An input stream reading from the tutorial_input topic with string keys and JSON values An output stream writing to the tutorial_output topic with the same data types","title":"Step 1: Define Your Streams"},{"location":"tutorials/getting-started/basics-tutorial/#understanding-stream-definitions","text":"Each stream definition includes: A unique name (e.g., input_stream ) The Kafka topic it connects to The data types for keys and values KSML supports various data types including: string : For text data json : For JSON-formatted data avro : For Avro-formatted data (requires schema) binary : For raw binary data And more","title":"Understanding Stream Definitions"},{"location":"tutorials/getting-started/basics-tutorial/#step-2-create-a-simple-function","text":"Next, let's create a function to log messages as they flow through our pipeline: functions: log_message: type: forEach parameters: - name: message_type type: string code: | log.info(\"{} message - key={}, value={}\", message_type, key, value) This function: Is named log_message Is of type forEach , which means it always gets two parameters key and value , and does not return a value Takes one additional parameter called message_type Uses the built-in log object to output information about each message","title":"Step 2: Create a Simple Function"},{"location":"tutorials/getting-started/basics-tutorial/#understanding-functions-in-ksml","text":"Functions in KSML: Can be reused across multiple operations in your pipelines Are written in Python Have access to pre-defined parameters based on function type Can take additional parameters for more flexibility Must return a value if so pre-defined by the function type","title":"Understanding Functions in KSML"},{"location":"tutorials/getting-started/basics-tutorial/#step-3-build-your-pipeline","text":"Now, let's create the pipeline that processes our data: pipelines: tutorial_pipeline: from: input_stream via: - type: filter if: expression: value.get('temperature') > 70 - type: mapValues mapper: expression: | {\"sensor\": key, \"temp_fahrenheit\": value.get('temperature'), \"temp_celsius\": (value.get('temperature') - 32) * 5/9} - type: peek forEach: code: log_message(key, value, message_type=\"Processed\") to: output_stream This pipeline: Reads from input_stream Filters out messages where the temperature is 70\u00b0F or lower Transforms the values to include both Fahrenheit and Celsius temperatures Logs each processed message Writes the results to output_stream","title":"Step 3: Build Your Pipeline"},{"location":"tutorials/getting-started/basics-tutorial/#understanding-pipeline-operations","text":"Let's break down each operation:","title":"Understanding Pipeline Operations"},{"location":"tutorials/getting-started/basics-tutorial/#filter-operation","text":"- type: filter if: expression: value.get('temperature') > 70 The filter operation: Evaluates the expression for each message Only passes messages where the expression returns True Discards messages where the expression returns False","title":"Filter Operation"},{"location":"tutorials/getting-started/basics-tutorial/#map-values-operation","text":"- type: mapValues mapper: expression: | {\"sensor\": key, \"temp_fahrenheit\": value.get('temperature'), \"temp_celsius\": (value.get('temperature') - 32) * 5/9} The mapValues operation: Transforms the value of each message Keeps the original key unchanged Creates a new value based on the expression In this case, creates a new JSON object with the original temperature and a calculated Celsius value Note that we put the expression on a new line in this example to force the KSML YAML parser to interpret the expression as a literal string for Python, instead of parsing it as part of the YAML syntax. Another way of achieving the same would be to surround the '{...}' with quotes, but in that case, be aware of consistent single/double quoting to not confuse the KSML parser and/or the Python interpreter. We generally recommend using the above notation for readability purposes.","title":"Map Values Operation"},{"location":"tutorials/getting-started/basics-tutorial/#peek-operation","text":"- type: peek forEach: code: log_message(key, value, message_type=\"Processed\") The peek operation: Executes the provided code for each message Doesn't modify the message Allows the message to continue through the pipeline Is useful for logging, metrics, or other side effects","title":"Peek Operation"},{"location":"tutorials/getting-started/basics-tutorial/#step-4-put-it-all-together","text":"Let's combine all the sections into a complete KSML definition file: streams: input_stream: topic: tutorial_input keyType: string valueType: json output_stream: topic: tutorial_output keyType: string valueType: json functions: log_message: type: forEach parameters: - name: message_type type: string code: | log.info(\"{} message - key={}, value={}\", message_type, key, value) pipelines: tutorial_pipeline: from: input_stream via: - type: filter if: expression: value.get('temperature') > 70 - type: mapValues mapper: expression: | {\"sensor\": key, \"temp_fahrenheit\": value.get('temperature'), \"temp_celsius\": (value.get('temperature') - 32) * 5/9} - type: peek forEach: code: log_message(key, value, message_type=\"Processed\") to: output_stream Save this file as tutorial.yaml .","title":"Step 4: Put It All Together"},{"location":"tutorials/getting-started/basics-tutorial/#step-5-run-your-pipeline","text":"Now let's run the pipeline and see it in action: Make sure your Kafka environment is running Create the input and output topics: bash docker exec -it kafka kafka-topics --bootstrap-server localhost:9092 --create --topic tutorial_input --partitions 1 --replication-factor 1 docker exec -it kafka kafka-topics --bootstrap-server localhost:9092 --create --topic tutorial_output --partitions 1 --replication-factor 1 Run the KSML runner with your definition file: bash docker run -v $(pwd):/app -e KAFKA_BOOTSTRAP_SERVERS=kafka:9092 axual/ksml-runner:latest --definitions /app/tutorial.yaml In another terminal, produce some test messages to the input topic: bash docker exec -it kafka kafka-console-producer --bootstrap-server localhost:9092 --topic tutorial_input --property \"parse.key=true\" --property \"key.separator=:\" Then enter messages in the format key:value , for example: sensor1:{\"temperature\": 75} sensor2:{\"temperature\": 65} sensor3:{\"temperature\": 80} In a third terminal, consume messages from the output topic to see the results: bash docker exec -it kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic tutorial_output --from-beginning You should see messages like: {\"sensor\":\"sensor1\",\"temp_fahrenheit\":75,\"temp_celsius\":23.88888888888889} {\"sensor\":\"sensor3\",\"temp_fahrenheit\":80,\"temp_celsius\":26.666666666666668} Notice that the message with temperature 65\u00b0F was filtered out, and the remaining messages have been transformed to include the Celsius temperature.","title":"Step 5: Run Your Pipeline"},{"location":"tutorials/getting-started/basics-tutorial/#understanding-whats-happening","text":"When you run your KSML definition: The KSML runner parses your YAML file It creates a Kafka Streams topology based on your pipeline definition The topology starts consuming from the input topic Each message flows through the operations you defined: The filter operation drops messages with temperatures \u2264 70\u00b0F The mapValues operation transforms the remaining messages The peek operation logs each message The messages are written to the output topic","title":"Understanding What's Happening"},{"location":"tutorials/getting-started/basics-tutorial/#using-ksml-to-produce-messages","text":"While you can manually produce the above messages, KSML can also generate messages for you. See below for a KSML definition that would randomly generate test messages every three seconds. functions: generate_temperature_message: type: generator globalCode: | import random sensorCounter = 0 code: | global sensorCounter key = \"sensor\"+str(sensorCounter) # Set the key to return (\"sensor0\" to \"sensor9\") sensorCounter = (sensorCounter+1) % 10 # Increase the counter for next iteration value = {\"temperature\": random.randrange(150)} expression: (key, value) # Return a message tuple with the key and value resultType: (string, json) # Indicate the type of key and value producers: # Produce a temperature message every 3 seconds tutorial_producer: generator: generate_temperature_message interval: 3s to: topic: tutorial_input keyType: string valueType: json","title":"Using KSML to produce messages"},{"location":"tutorials/getting-started/basics-tutorial/#next-steps","text":"Congratulations! You've built your first KSML data pipeline. Here are some ways to expand on what you've learned:","title":"Next Steps"},{"location":"tutorials/getting-started/basics-tutorial/#try-these-modifications","text":"Add another filter condition (e.g., filter by sensor name) Add more fields to the transformed output Create a second pipeline that processes the data differently","title":"Try These Modifications:"},{"location":"tutorials/getting-started/basics-tutorial/#explore-more-advanced-concepts","text":"Learn about stateful operations like aggregations and joins Explore windowing operations for time-based processing Try working with different data formats","title":"Explore More Advanced Concepts:"},{"location":"tutorials/getting-started/basics-tutorial/#continue-your-learning-journey","text":"Check out the beginner tutorials for more guided examples Explore the examples library for inspiration Dive into the reference documentation to learn about all available operations","title":"Continue Your Learning Journey:"},{"location":"tutorials/getting-started/installation/","text":"Installation and Setup This guide will walk you through setting up a development environment for KSML and running your first KSML application. Prerequisites Before you begin, make sure you have the following installed: Docker : KSML examples and development environment run in Docker containers Git : To clone the KSML repository (if you want to run the examples) Basic understanding of Kafka : Familiarity with Kafka concepts like topics and messages Setting Up a Development Environment There are two main ways to set up KSML: Using the provided Docker Compose setup (recommended for beginners) Configuring KSML to connect to an existing Kafka cluster Option 1: Using Docker Compose The easiest way to get started with KSML is to use the provided Docker Compose setup, which includes: Zookeeper Kafka broker Schema Registry Example data producer KSML runner Step 1: Clone the Repository If you haven't already, clone the KSML repository: git clone https://github.com/axual/ksml.git cd ksml Step 2: Start the Environment Start the Docker Compose environment: docker compose up -d This command starts all the necessary services in the background. You can check the logs to verify everything is running correctly: docker compose logs -f You should see log messages indicating that the services are running, including messages from the example producer that's generating random sensor data: example-producer-1 | 2024-03-06T20:24:49,480Z INFO i.a.k.r.backend.KafkaProducerRunner Calling generate_sensordata_message example-producer-1 | 2024-03-06T20:24:49,480Z INFO i.a.k.r.backend.ExecutableProducer Message: key=sensor2, value=SensorData: {\"city\":\"Utrecht\", \"color\":\"white\", \"name\":\"sensor2\", \"owner\":\"Alice\", \"timestamp\":1709756689480, \"type\":\"HUMIDITY\", \"unit\":\"%\", \"value\":\"66\"} Press CTRL-C to stop following the logs when you're satisfied that the environment is running correctly. Option 2: Connecting to an Existing Kafka Cluster If you already have a Kafka cluster and want to use KSML with it, you'll need to configure the KSML runner to connect to your cluster. Step 1: Create a Configuration File Create a file named ksml-runner.yaml with the following content, adjusting the values to match your Kafka cluster configuration: kafka: bootstrap.servers: your-kafka-broker:9092 # Add any other Kafka client properties here schemaRegistry: url: http://your-schema-registry:8081 # Add any other Schema Registry properties here # Path to your KSML definition files definitions: - path/to/your/ksml/definition.yaml Step 2: Run the KSML Runner Run the KSML runner with your configuration: docker run -v $(pwd):/app axual/ksml-runner:latest --config /app/ksml-runner.yaml Running Your First KSML Application Now that you have a running environment, let's run a simple KSML application that processes the example sensor data. Step 1: Run the Example KSML Runner If you're using the Docker Compose setup, after building a local KSML image, you can run the example KSML runner with: ./examples/run-local.sh This will start a KSML runner container that processes the example KSML definitions. You should see output similar to: 2024-03-06T20:24:51,921Z INFO io.axual.ksml.runner.KSMLRunner Starting KSML Runner 1.76.0.0 ... 2024-03-06T20:24:57,196Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor9, value={'city': 'Alkmaar', 'color': 'yellow', 'name': 'sensor9', 'owner': 'Bob', 'timestamp': 1709749917190, 'type': 'LENGTH', 'unit': 'm', 'value': '562', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} Step 2: Explore the Example KSML Definitions The example KSML definitions are located in the examples directory. Take a look at some of the simpler examples to understand how KSML works: 01-example-inspect.yaml : Shows how to read and log messages from a topic 02-example-copy.yaml : Demonstrates copying messages from one topic to another 03-example-filter.yaml : Shows how to filter messages based on their content Step 3: Modify an Example Try modifying one of the examples to see how changes affect the behavior. For instance, you could change the filter condition in 03-example-filter.yaml to filter based on a different field or value. Edit the file using your favorite text editor Restart the KSML runner to apply your changes: bash docker compose restart ksml-runner Check the logs to see the effect of your changes: bash docker compose logs -f ksml-runner Troubleshooting Common Issues Connection refused errors : Make sure all the Docker containers are running with docker compose ps Schema not found errors : Ensure the Schema Registry is running and accessible KSML syntax errors : Check your YAML syntax for proper indentation and formatting Getting Help If you encounter issues not covered here: Check the Troubleshooting Guide for more detailed solutions Visit the Community and Support page for information on how to get help from the KSML community Next Steps Now that you have KSML up and running, you can: Follow the KSML Basics Tutorial to learn how to build your first KSML pipeline from scratch Explore the Core Concepts to deepen your understanding of KSML Browse the Examples Library for inspiration and ready-to-use patterns","title":"Installation and Setup"},{"location":"tutorials/getting-started/installation/#installation-and-setup","text":"This guide will walk you through setting up a development environment for KSML and running your first KSML application.","title":"Installation and Setup"},{"location":"tutorials/getting-started/installation/#prerequisites","text":"Before you begin, make sure you have the following installed: Docker : KSML examples and development environment run in Docker containers Git : To clone the KSML repository (if you want to run the examples) Basic understanding of Kafka : Familiarity with Kafka concepts like topics and messages","title":"Prerequisites"},{"location":"tutorials/getting-started/installation/#setting-up-a-development-environment","text":"There are two main ways to set up KSML: Using the provided Docker Compose setup (recommended for beginners) Configuring KSML to connect to an existing Kafka cluster","title":"Setting Up a Development Environment"},{"location":"tutorials/getting-started/installation/#option-1-using-docker-compose","text":"The easiest way to get started with KSML is to use the provided Docker Compose setup, which includes: Zookeeper Kafka broker Schema Registry Example data producer KSML runner","title":"Option 1: Using Docker Compose"},{"location":"tutorials/getting-started/installation/#step-1-clone-the-repository","text":"If you haven't already, clone the KSML repository: git clone https://github.com/axual/ksml.git cd ksml","title":"Step 1: Clone the Repository"},{"location":"tutorials/getting-started/installation/#step-2-start-the-environment","text":"Start the Docker Compose environment: docker compose up -d This command starts all the necessary services in the background. You can check the logs to verify everything is running correctly: docker compose logs -f You should see log messages indicating that the services are running, including messages from the example producer that's generating random sensor data: example-producer-1 | 2024-03-06T20:24:49,480Z INFO i.a.k.r.backend.KafkaProducerRunner Calling generate_sensordata_message example-producer-1 | 2024-03-06T20:24:49,480Z INFO i.a.k.r.backend.ExecutableProducer Message: key=sensor2, value=SensorData: {\"city\":\"Utrecht\", \"color\":\"white\", \"name\":\"sensor2\", \"owner\":\"Alice\", \"timestamp\":1709756689480, \"type\":\"HUMIDITY\", \"unit\":\"%\", \"value\":\"66\"} Press CTRL-C to stop following the logs when you're satisfied that the environment is running correctly.","title":"Step 2: Start the Environment"},{"location":"tutorials/getting-started/installation/#option-2-connecting-to-an-existing-kafka-cluster","text":"If you already have a Kafka cluster and want to use KSML with it, you'll need to configure the KSML runner to connect to your cluster.","title":"Option 2: Connecting to an Existing Kafka Cluster"},{"location":"tutorials/getting-started/installation/#step-1-create-a-configuration-file","text":"Create a file named ksml-runner.yaml with the following content, adjusting the values to match your Kafka cluster configuration: kafka: bootstrap.servers: your-kafka-broker:9092 # Add any other Kafka client properties here schemaRegistry: url: http://your-schema-registry:8081 # Add any other Schema Registry properties here # Path to your KSML definition files definitions: - path/to/your/ksml/definition.yaml","title":"Step 1: Create a Configuration File"},{"location":"tutorials/getting-started/installation/#step-2-run-the-ksml-runner","text":"Run the KSML runner with your configuration: docker run -v $(pwd):/app axual/ksml-runner:latest --config /app/ksml-runner.yaml","title":"Step 2: Run the KSML Runner"},{"location":"tutorials/getting-started/installation/#running-your-first-ksml-application","text":"Now that you have a running environment, let's run a simple KSML application that processes the example sensor data.","title":"Running Your First KSML Application"},{"location":"tutorials/getting-started/installation/#step-1-run-the-example-ksml-runner","text":"If you're using the Docker Compose setup, after building a local KSML image, you can run the example KSML runner with: ./examples/run-local.sh This will start a KSML runner container that processes the example KSML definitions. You should see output similar to: 2024-03-06T20:24:51,921Z INFO io.axual.ksml.runner.KSMLRunner Starting KSML Runner 1.76.0.0 ... 2024-03-06T20:24:57,196Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor9, value={'city': 'Alkmaar', 'color': 'yellow', 'name': 'sensor9', 'owner': 'Bob', 'timestamp': 1709749917190, 'type': 'LENGTH', 'unit': 'm', 'value': '562', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}}","title":"Step 1: Run the Example KSML Runner"},{"location":"tutorials/getting-started/installation/#step-2-explore-the-example-ksml-definitions","text":"The example KSML definitions are located in the examples directory. Take a look at some of the simpler examples to understand how KSML works: 01-example-inspect.yaml : Shows how to read and log messages from a topic 02-example-copy.yaml : Demonstrates copying messages from one topic to another 03-example-filter.yaml : Shows how to filter messages based on their content","title":"Step 2: Explore the Example KSML Definitions"},{"location":"tutorials/getting-started/installation/#step-3-modify-an-example","text":"Try modifying one of the examples to see how changes affect the behavior. For instance, you could change the filter condition in 03-example-filter.yaml to filter based on a different field or value. Edit the file using your favorite text editor Restart the KSML runner to apply your changes: bash docker compose restart ksml-runner Check the logs to see the effect of your changes: bash docker compose logs -f ksml-runner","title":"Step 3: Modify an Example"},{"location":"tutorials/getting-started/installation/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"tutorials/getting-started/installation/#common-issues","text":"Connection refused errors : Make sure all the Docker containers are running with docker compose ps Schema not found errors : Ensure the Schema Registry is running and accessible KSML syntax errors : Check your YAML syntax for proper indentation and formatting","title":"Common Issues"},{"location":"tutorials/getting-started/installation/#getting-help","text":"If you encounter issues not covered here: Check the Troubleshooting Guide for more detailed solutions Visit the Community and Support page for information on how to get help from the KSML community","title":"Getting Help"},{"location":"tutorials/getting-started/installation/#next-steps","text":"Now that you have KSML up and running, you can: Follow the KSML Basics Tutorial to learn how to build your first KSML pipeline from scratch Explore the Core Concepts to deepen your understanding of KSML Browse the Examples Library for inspiration and ready-to-use patterns","title":"Next Steps"},{"location":"tutorials/getting-started/introduction/","text":"Introduction to KSML What is KSML? KSML is a declarative language that allows you to build powerful Kafka Streams applications using YAML and Python, without writing Java code. KSML makes stream processing accessible to a wider audience by removing the need for Java development skills and complex build pipelines. With KSML, you can: - Define streaming data pipelines using simple YAML syntax - Process data using Python functions embedded directly in your KSML definitions - Deploy applications without compiling Java code - Rapidly prototype and iterate on streaming applications Why Use KSML? Simplified Development KSML dramatically reduces the complexity of building Kafka Streams applications. Instead of writing hundreds of lines of Java code, you can define your streaming logic in a concise YAML file with embedded Python functions. No Java Required While Kafka Streams is a powerful Java library, KSML eliminates the need to write Java code. This opens up Kafka Streams to data engineers, analysts, and other professionals who may not have Java expertise. Rapid Prototyping KSML allows you to quickly prototype and test streaming applications. Changes can be made to your KSML definition and deployed immediately, without a compile-package-deploy cycle. Full Access to Kafka Streams Capabilities KSML provides access to the full power of Kafka Streams, including: - Stateless operations (map, filter, etc.) - Stateful operations (aggregate, count, etc.) - Windowing operations - Stream and table joins - And more Key Concepts and Terminology Streams In KSML, a stream represents a flow of data from or to a Kafka topic. Streams are defined with a name, a topic, and data types for keys and values. streams: my_input_stream: topic: input-topic keyType: string valueType: json Functions Functions in KSML are reusable pieces of Python code that can be called from your pipelines. They can transform data, filter messages, or perform side effects like logging. functions: log_message: type: forEach code: | log.info(\"Processing message with key: {}\", key) Pipelines Pipelines define the flow and processing of data. A pipeline starts with a source stream, applies a series of operations, and typically ends with a sink operation that writes to another stream or performs a terminal action. pipelines: my_pipeline: from: my_input_stream via: - type: filter if: expression: value.get('temperature') > 70 - type: mapValues mapper: expression: {\"temp_celsius\": (value.get('temperature') - 32) * 5/9} to: my_output_stream Operations Operations are the building blocks of pipelines. They define how data is processed as it flows through a pipeline. KSML supports a wide range of operations, including: Stateless operations : filter, map, flatMap, peek Stateful operations : aggregate, count, reduce Joining operations : join, leftJoin, outerJoin Windowing operations : windowedBy Sink operations : to, forEach, branch How KSML Relates to Kafka Streams KSML is built on top of Kafka Streams and translates your YAML definitions into Kafka Streams topologies. This means: You get all the benefits of Kafka Streams (exactly-once processing, fault tolerance, scalability) Your KSML applications can integrate with existing Kafka Streams applications Performance characteristics are similar to native Kafka Streams applications When you run a KSML definition, the KSML runner: 1. Parses your YAML definition 2. Translates it into a Kafka Streams topology 3. Executes the topology using the Kafka Streams runtime This translation happens automatically, allowing you to focus on your business logic rather than the details of a normal Kafka Streams implementation. Next Steps Now that you understand what KSML is and its key concepts, you can: Learn how to install and set up KSML Follow the KSML Basics Tutorial to build your first application Explore the Core Concepts in more detail","title":"Introduction to KSML"},{"location":"tutorials/getting-started/introduction/#introduction-to-ksml","text":"","title":"Introduction to KSML"},{"location":"tutorials/getting-started/introduction/#what-is-ksml","text":"KSML is a declarative language that allows you to build powerful Kafka Streams applications using YAML and Python, without writing Java code. KSML makes stream processing accessible to a wider audience by removing the need for Java development skills and complex build pipelines. With KSML, you can: - Define streaming data pipelines using simple YAML syntax - Process data using Python functions embedded directly in your KSML definitions - Deploy applications without compiling Java code - Rapidly prototype and iterate on streaming applications","title":"What is KSML?"},{"location":"tutorials/getting-started/introduction/#why-use-ksml","text":"","title":"Why Use KSML?"},{"location":"tutorials/getting-started/introduction/#simplified-development","text":"KSML dramatically reduces the complexity of building Kafka Streams applications. Instead of writing hundreds of lines of Java code, you can define your streaming logic in a concise YAML file with embedded Python functions.","title":"Simplified Development"},{"location":"tutorials/getting-started/introduction/#no-java-required","text":"While Kafka Streams is a powerful Java library, KSML eliminates the need to write Java code. This opens up Kafka Streams to data engineers, analysts, and other professionals who may not have Java expertise.","title":"No Java Required"},{"location":"tutorials/getting-started/introduction/#rapid-prototyping","text":"KSML allows you to quickly prototype and test streaming applications. Changes can be made to your KSML definition and deployed immediately, without a compile-package-deploy cycle.","title":"Rapid Prototyping"},{"location":"tutorials/getting-started/introduction/#full-access-to-kafka-streams-capabilities","text":"KSML provides access to the full power of Kafka Streams, including: - Stateless operations (map, filter, etc.) - Stateful operations (aggregate, count, etc.) - Windowing operations - Stream and table joins - And more","title":"Full Access to Kafka Streams Capabilities"},{"location":"tutorials/getting-started/introduction/#key-concepts-and-terminology","text":"","title":"Key Concepts and Terminology"},{"location":"tutorials/getting-started/introduction/#streams","text":"In KSML, a stream represents a flow of data from or to a Kafka topic. Streams are defined with a name, a topic, and data types for keys and values. streams: my_input_stream: topic: input-topic keyType: string valueType: json","title":"Streams"},{"location":"tutorials/getting-started/introduction/#functions","text":"Functions in KSML are reusable pieces of Python code that can be called from your pipelines. They can transform data, filter messages, or perform side effects like logging. functions: log_message: type: forEach code: | log.info(\"Processing message with key: {}\", key)","title":"Functions"},{"location":"tutorials/getting-started/introduction/#pipelines","text":"Pipelines define the flow and processing of data. A pipeline starts with a source stream, applies a series of operations, and typically ends with a sink operation that writes to another stream or performs a terminal action. pipelines: my_pipeline: from: my_input_stream via: - type: filter if: expression: value.get('temperature') > 70 - type: mapValues mapper: expression: {\"temp_celsius\": (value.get('temperature') - 32) * 5/9} to: my_output_stream","title":"Pipelines"},{"location":"tutorials/getting-started/introduction/#operations","text":"Operations are the building blocks of pipelines. They define how data is processed as it flows through a pipeline. KSML supports a wide range of operations, including: Stateless operations : filter, map, flatMap, peek Stateful operations : aggregate, count, reduce Joining operations : join, leftJoin, outerJoin Windowing operations : windowedBy Sink operations : to, forEach, branch","title":"Operations"},{"location":"tutorials/getting-started/introduction/#how-ksml-relates-to-kafka-streams","text":"KSML is built on top of Kafka Streams and translates your YAML definitions into Kafka Streams topologies. This means: You get all the benefits of Kafka Streams (exactly-once processing, fault tolerance, scalability) Your KSML applications can integrate with existing Kafka Streams applications Performance characteristics are similar to native Kafka Streams applications When you run a KSML definition, the KSML runner: 1. Parses your YAML definition 2. Translates it into a Kafka Streams topology 3. Executes the topology using the Kafka Streams runtime This translation happens automatically, allowing you to focus on your business logic rather than the details of a normal Kafka Streams implementation.","title":"How KSML Relates to Kafka Streams"},{"location":"tutorials/getting-started/introduction/#next-steps","text":"Now that you understand what KSML is and its key concepts, you can: Learn how to install and set up KSML Follow the KSML Basics Tutorial to build your first application Explore the Core Concepts in more detail","title":"Next Steps"},{"location":"tutorials/intermediate/","text":"Intermediate Tutorials Welcome to the KSML intermediate tutorials! These tutorials are designed for users who have completed the beginner tutorials and are ready to explore more advanced KSML features and patterns. These tutorials will help you build more sophisticated data processing applications with KSML, introducing stateful operations, joins, and other advanced concepts. Available Tutorials Working with Aggregations Learn how to use KSML's stateful operations to aggregate data: Counting events by key Calculating running averages Using custom aggregation functions Understanding state stores Implementing Joins This tutorial covers how to join data from multiple streams: Stream-to-stream joins Stream-to-table joins Global table joins Handling join windows and grace periods Using Windowed Operations Learn how to process data within time windows: Tumbling windows Hopping windows Session windows Sliding windows Time-based aggregations Error Handling and Recovery This tutorial focuses on building robust KSML applications: Handling malformed data Implementing dead letter queues Recovering from errors Monitoring and alerting Working with State Stores Learn how to use and manage state in your KSML applications: Creating and configuring state stores Reading from and writing to state stores Handling state store failures State store backup and recovery Learning Path We recommend following these tutorials in order, as they build on concepts introduced in previous tutorials. After completing these intermediate tutorials, you'll be ready to move on to the Advanced Tutorials , which cover more complex topics like custom processors, performance optimization, and integration with external systems. Additional Resources Core Concepts: Operations - Detailed explanations of KSML operations Reference: Stateful Operations - Complete reference for stateful operations Examples Library: Intermediate Examples - Ready-to-use examples for intermediate patterns","title":"Intermediate Tutorials"},{"location":"tutorials/intermediate/#intermediate-tutorials","text":"Welcome to the KSML intermediate tutorials! These tutorials are designed for users who have completed the beginner tutorials and are ready to explore more advanced KSML features and patterns. These tutorials will help you build more sophisticated data processing applications with KSML, introducing stateful operations, joins, and other advanced concepts.","title":"Intermediate Tutorials"},{"location":"tutorials/intermediate/#available-tutorials","text":"","title":"Available Tutorials"},{"location":"tutorials/intermediate/#working-with-aggregations","text":"Learn how to use KSML's stateful operations to aggregate data: Counting events by key Calculating running averages Using custom aggregation functions Understanding state stores","title":"Working with Aggregations"},{"location":"tutorials/intermediate/#implementing-joins","text":"This tutorial covers how to join data from multiple streams: Stream-to-stream joins Stream-to-table joins Global table joins Handling join windows and grace periods","title":"Implementing Joins"},{"location":"tutorials/intermediate/#using-windowed-operations","text":"Learn how to process data within time windows: Tumbling windows Hopping windows Session windows Sliding windows Time-based aggregations","title":"Using Windowed Operations"},{"location":"tutorials/intermediate/#error-handling-and-recovery","text":"This tutorial focuses on building robust KSML applications: Handling malformed data Implementing dead letter queues Recovering from errors Monitoring and alerting","title":"Error Handling and Recovery"},{"location":"tutorials/intermediate/#working-with-state-stores","text":"Learn how to use and manage state in your KSML applications: Creating and configuring state stores Reading from and writing to state stores Handling state store failures State store backup and recovery","title":"Working with State Stores"},{"location":"tutorials/intermediate/#learning-path","text":"We recommend following these tutorials in order, as they build on concepts introduced in previous tutorials. After completing these intermediate tutorials, you'll be ready to move on to the Advanced Tutorials , which cover more complex topics like custom processors, performance optimization, and integration with external systems.","title":"Learning Path"},{"location":"tutorials/intermediate/#additional-resources","text":"Core Concepts: Operations - Detailed explanations of KSML operations Reference: Stateful Operations - Complete reference for stateful operations Examples Library: Intermediate Examples - Ready-to-use examples for intermediate patterns","title":"Additional Resources"},{"location":"tutorials/intermediate/aggregations/","text":"Working with Aggregations in KSML This tutorial explores how to implement aggregation operations in KSML, allowing you to combine multiple messages into meaningful summaries and statistics. Introduction to Aggregations Aggregations are stateful operations that combine multiple messages to produce a single result. They're essential for: Computing statistics (sums, averages, counts) Building summaries of streaming data Reducing data volume by consolidating related messages Creating time-based analytics In KSML, aggregations are implemented using the Kafka Streams stateful operations, but with the added flexibility of Python functions. Prerequisites Before starting this tutorial, you should: Understand basic KSML concepts (streams, functions, pipelines) Have completed the KSML Basics Tutorial Be familiar with Stateful Operations Types of Aggregations in KSML KSML supports several types of aggregation operations: Count The simplest aggregation is count , which counts the number of messages with the same key: pipelines: count_by_user: from: user_actions groupByKey: count: to: user_action_counts Reduce The reduce operation combines messages with the same key using a reducer function: functions: sum_amounts: type: reducer expression: value1 + value2 pipelines: sum_transactions: from: financial_transactions groupByKey: reduce: sum_amounts to: transaction_sums Aggregate The most flexible aggregation is aggregate , which uses an initializer function to create the initial aggregation value and an aggregator function to update it: functions: init_stats: type: initializer expression: {\"count\": 0, \"sum\": 0, \"min\": None, \"max\": None} update_stats: type: aggregator code: | if aggregatedValue is None: aggregatedValue = {\"count\": 0, \"sum\": 0, \"min\": None, \"max\": None} amount = value.get(\"amount\", 0) # Update count and sum aggregatedValue[\"count\"] += 1 aggregatedValue[\"sum\"] += amount # Update min and max if aggregatedValue[\"min\"] is None or amount < aggregatedValue[\"min\"]: aggregatedValue[\"min\"] = amount if aggregatedValue[\"max\"] is None or amount > aggregatedValue[\"max\"]: aggregatedValue[\"max\"] = amount return aggregatedValue pipelines: calculate_statistics: from: payment_stream groupByKey: aggregate: initializer: init_stats aggregator: update_stats to: payment_statistics Working with Windows Aggregations become even more powerful when combined with windowing operations, which group messages into time-based windows: pipelines: hourly_statistics: from: sensor_readings groupByKey: windowByTime: size: 1h advanceBy: 1h grace: 10m aggregate: initializer: init_stats aggregator: update_stats to: hourly_sensor_statistics This creates hourly statistics for each sensor (assuming the sensor ID is the key). Practical Example: Sales Analytics Let's build a complete example that calculates sales analytics by region and product: streams: sales_events: topic: retail_sales keyType: string # Product ID valueType: json # Sale details including region, amount, quantity sales_by_region: topic: sales_by_region keyType: string # Region valueType: json # Aggregated sales statistics functions: extract_region: type: keyTransformer code: | # Extract region from the sale event and use it as the new key return value.get(\"region\", \"unknown\") initialize_sales_stats: type: initializer expression: {\"total_sales\": 0, \"total_quantity\": 0, \"transaction_count\": 0, \"products\": {}} aggregate_sales: type: aggregator code: | # Initialize if needed if aggregatedValue is None: aggregatedValue = {\"total_sales\": 0, \"total_quantity\": 0, \"transaction_count\": 0, \"products\": {}} # Extract data from the sale product_id = key amount = value.get(\"amount\", 0) quantity = value.get(\"quantity\", 0) # Update aggregated values aggregatedValue[\"total_sales\"] += amount aggregatedValue[\"total_quantity\"] += quantity aggregatedValue[\"transaction_count\"] += 1 # Track per-product statistics if product_id not in aggregatedValue[\"products\"]: aggregatedValue[\"products\"][product_id] = {\"sales\": 0, \"quantity\": 0} aggregatedValue[\"products\"][product_id][\"sales\"] += amount aggregatedValue[\"products\"][product_id][\"quantity\"] += quantity return aggregatedValue pipelines: regional_sales_analytics: from: sales_events # Group by region instead of product ID groupBy: extract_region # Use tumbling window of 1 day windowByTime: size: 1d advanceBy: 1d # Aggregate sales data aggregate: initializer: initialize_sales_stats aggregator: aggregate_sales # Output to region-specific topic to: sales_by_region This pipeline: 1. Takes sales events with product IDs as keys 2. Regroups them by region 3. Creates daily windows 4. Aggregates sales statistics including per-product breakdowns 5. Outputs the results to a new topic Best Practices for Aggregations Performance Considerations Memory Usage : Aggregations store state, which consumes memory. Be mindful of the volume of unique keys. Window Size : Larger windows require more state storage. Choose appropriate window sizes. Serialization : Complex aggregated objects can be expensive to serialize/deserialize. Design Patterns Two-Phase Aggregation : For high-cardinality data, consider aggregating in two phases (local then global). Pre-Filtering : Filter unnecessary data before aggregation to reduce state size. Downsampling : For time series data, consider downsampling before aggregation. Error Handling Always handle potential errors in your aggregator functions: functions: safe_aggregator: type: aggregator code: | try: # Your aggregation logic here return result except Exception as e: log.error(\"Error in aggregation: {}\", str(e)) # Return previous state to avoid losing data return aggregatedValue Conclusion Aggregations are powerful tools for deriving insights from streaming data. By combining KSML's aggregation operations with Python functions, you can implement sophisticated analytics that would be complex to build with traditional Kafka Streams applications. In the next tutorial, we'll explore how to implement joins to combine data from multiple streams. Further Reading Core Concepts: Operations Core Concepts: Functions Reference: Aggregation Operations","title":"Working with Aggregations in KSML"},{"location":"tutorials/intermediate/aggregations/#working-with-aggregations-in-ksml","text":"This tutorial explores how to implement aggregation operations in KSML, allowing you to combine multiple messages into meaningful summaries and statistics.","title":"Working with Aggregations in KSML"},{"location":"tutorials/intermediate/aggregations/#introduction-to-aggregations","text":"Aggregations are stateful operations that combine multiple messages to produce a single result. They're essential for: Computing statistics (sums, averages, counts) Building summaries of streaming data Reducing data volume by consolidating related messages Creating time-based analytics In KSML, aggregations are implemented using the Kafka Streams stateful operations, but with the added flexibility of Python functions.","title":"Introduction to Aggregations"},{"location":"tutorials/intermediate/aggregations/#prerequisites","text":"Before starting this tutorial, you should: Understand basic KSML concepts (streams, functions, pipelines) Have completed the KSML Basics Tutorial Be familiar with Stateful Operations","title":"Prerequisites"},{"location":"tutorials/intermediate/aggregations/#types-of-aggregations-in-ksml","text":"KSML supports several types of aggregation operations:","title":"Types of Aggregations in KSML"},{"location":"tutorials/intermediate/aggregations/#count","text":"The simplest aggregation is count , which counts the number of messages with the same key: pipelines: count_by_user: from: user_actions groupByKey: count: to: user_action_counts","title":"Count"},{"location":"tutorials/intermediate/aggregations/#reduce","text":"The reduce operation combines messages with the same key using a reducer function: functions: sum_amounts: type: reducer expression: value1 + value2 pipelines: sum_transactions: from: financial_transactions groupByKey: reduce: sum_amounts to: transaction_sums","title":"Reduce"},{"location":"tutorials/intermediate/aggregations/#aggregate","text":"The most flexible aggregation is aggregate , which uses an initializer function to create the initial aggregation value and an aggregator function to update it: functions: init_stats: type: initializer expression: {\"count\": 0, \"sum\": 0, \"min\": None, \"max\": None} update_stats: type: aggregator code: | if aggregatedValue is None: aggregatedValue = {\"count\": 0, \"sum\": 0, \"min\": None, \"max\": None} amount = value.get(\"amount\", 0) # Update count and sum aggregatedValue[\"count\"] += 1 aggregatedValue[\"sum\"] += amount # Update min and max if aggregatedValue[\"min\"] is None or amount < aggregatedValue[\"min\"]: aggregatedValue[\"min\"] = amount if aggregatedValue[\"max\"] is None or amount > aggregatedValue[\"max\"]: aggregatedValue[\"max\"] = amount return aggregatedValue pipelines: calculate_statistics: from: payment_stream groupByKey: aggregate: initializer: init_stats aggregator: update_stats to: payment_statistics","title":"Aggregate"},{"location":"tutorials/intermediate/aggregations/#working-with-windows","text":"Aggregations become even more powerful when combined with windowing operations, which group messages into time-based windows: pipelines: hourly_statistics: from: sensor_readings groupByKey: windowByTime: size: 1h advanceBy: 1h grace: 10m aggregate: initializer: init_stats aggregator: update_stats to: hourly_sensor_statistics This creates hourly statistics for each sensor (assuming the sensor ID is the key).","title":"Working with Windows"},{"location":"tutorials/intermediate/aggregations/#practical-example-sales-analytics","text":"Let's build a complete example that calculates sales analytics by region and product: streams: sales_events: topic: retail_sales keyType: string # Product ID valueType: json # Sale details including region, amount, quantity sales_by_region: topic: sales_by_region keyType: string # Region valueType: json # Aggregated sales statistics functions: extract_region: type: keyTransformer code: | # Extract region from the sale event and use it as the new key return value.get(\"region\", \"unknown\") initialize_sales_stats: type: initializer expression: {\"total_sales\": 0, \"total_quantity\": 0, \"transaction_count\": 0, \"products\": {}} aggregate_sales: type: aggregator code: | # Initialize if needed if aggregatedValue is None: aggregatedValue = {\"total_sales\": 0, \"total_quantity\": 0, \"transaction_count\": 0, \"products\": {}} # Extract data from the sale product_id = key amount = value.get(\"amount\", 0) quantity = value.get(\"quantity\", 0) # Update aggregated values aggregatedValue[\"total_sales\"] += amount aggregatedValue[\"total_quantity\"] += quantity aggregatedValue[\"transaction_count\"] += 1 # Track per-product statistics if product_id not in aggregatedValue[\"products\"]: aggregatedValue[\"products\"][product_id] = {\"sales\": 0, \"quantity\": 0} aggregatedValue[\"products\"][product_id][\"sales\"] += amount aggregatedValue[\"products\"][product_id][\"quantity\"] += quantity return aggregatedValue pipelines: regional_sales_analytics: from: sales_events # Group by region instead of product ID groupBy: extract_region # Use tumbling window of 1 day windowByTime: size: 1d advanceBy: 1d # Aggregate sales data aggregate: initializer: initialize_sales_stats aggregator: aggregate_sales # Output to region-specific topic to: sales_by_region This pipeline: 1. Takes sales events with product IDs as keys 2. Regroups them by region 3. Creates daily windows 4. Aggregates sales statistics including per-product breakdowns 5. Outputs the results to a new topic","title":"Practical Example: Sales Analytics"},{"location":"tutorials/intermediate/aggregations/#best-practices-for-aggregations","text":"","title":"Best Practices for Aggregations"},{"location":"tutorials/intermediate/aggregations/#performance-considerations","text":"Memory Usage : Aggregations store state, which consumes memory. Be mindful of the volume of unique keys. Window Size : Larger windows require more state storage. Choose appropriate window sizes. Serialization : Complex aggregated objects can be expensive to serialize/deserialize.","title":"Performance Considerations"},{"location":"tutorials/intermediate/aggregations/#design-patterns","text":"Two-Phase Aggregation : For high-cardinality data, consider aggregating in two phases (local then global). Pre-Filtering : Filter unnecessary data before aggregation to reduce state size. Downsampling : For time series data, consider downsampling before aggregation.","title":"Design Patterns"},{"location":"tutorials/intermediate/aggregations/#error-handling","text":"Always handle potential errors in your aggregator functions: functions: safe_aggregator: type: aggregator code: | try: # Your aggregation logic here return result except Exception as e: log.error(\"Error in aggregation: {}\", str(e)) # Return previous state to avoid losing data return aggregatedValue","title":"Error Handling"},{"location":"tutorials/intermediate/aggregations/#conclusion","text":"Aggregations are powerful tools for deriving insights from streaming data. By combining KSML's aggregation operations with Python functions, you can implement sophisticated analytics that would be complex to build with traditional Kafka Streams applications. In the next tutorial, we'll explore how to implement joins to combine data from multiple streams.","title":"Conclusion"},{"location":"tutorials/intermediate/aggregations/#further-reading","text":"Core Concepts: Operations Core Concepts: Functions Reference: Aggregation Operations","title":"Further Reading"},{"location":"tutorials/intermediate/error-handling/","text":"Error Handling and Recovery in KSML This tutorial explores strategies for handling errors and implementing recovery mechanisms in KSML applications, helping you build more robust and resilient stream processing pipelines. Introduction to Error Handling Error handling is a critical aspect of any production-grade stream processing application. In streaming contexts, errors can occur for various reasons: Invalid or malformed input data External service failures Resource constraints Business rule violations Unexpected edge cases Without proper error handling, these issues can cause your application to: Crash and stop processing Skip or lose important messages Produce incorrect results Create inconsistent state KSML provides several mechanisms to handle errors gracefully and implement recovery strategies, allowing your applications to continue processing even when problems occur. Prerequisites Before starting this tutorial, you should: Understand basic KSML concepts (streams, functions, pipelines) Have completed the KSML Basics Tutorial Be familiar with Python exception handling Error Handling Strategies in KSML 1. Try-Except Blocks in Python Functions The most basic form of error handling in KSML is using try-except blocks in your Python functions: functions: safe_transform: type: valueTransformer code: | try: # Potentially risky operation result = process_data(value) return result except Exception as e: # Log the error log.error(\"Error processing data: {}\", str(e)) # Return a default or fallback value return {\"error\": str(e), \"original_data\": value} This approach: - Prevents the function from failing completely - Logs the error for troubleshooting - Returns a fallback value that allows processing to continue 2. Validation and Filtering Proactively validate data and filter out problematic messages before they cause errors: functions: validate_data: type: predicate code: | # Check if the message has all required fields if value is None: log.warn(\"Received null value for key: {}\", key) return False required_fields = [\"user_id\", \"timestamp\", \"action\"] for field in required_fields: if field not in value: log.warn(\"Missing required field '{}' in message with key: {}\", field, key) return False return True pipelines: process_valid_data: from: input_stream filter: validate_data # Only process messages that pass validation # Continue processing with valid data... to: processed_stream 3. Dead Letter Queues Implement a \"dead letter queue\" pattern to capture and store messages that couldn't be processed: functions: process_with_dlq: type: keyValueTransformer code: | try: # Attempt to process the message processed_value = process_data(value) # Return the processed message to the main output return (key, processed_value) except Exception as e: # Create an error record error_record = { \"original_key\": key, \"original_value\": value, \"error\": str(e), \"timestamp\": int(time.time() * 1000) } # Send to the dead letter queue dlq_topic.send(key, error_record) # Return None to filter this message from the main output return None pipelines: main_processing: from: input_stream transformKeyValue: process_with_dlq to: processed_stream 4. Error Classification and Routing Classify errors and route messages to different handling paths based on the error type: functions: classify_errors: type: keyValueTransformer code: | try: # Attempt normal processing result = process_data(value) # Tag with success status result[\"status\"] = \"success\" return (key, result) except ValidationError as e: # Handle validation errors log.warn(\"Validation error: {}\", str(e)) value[\"status\"] = \"validation_error\" value[\"error_details\"] = str(e) return (key, value) except TemporaryError as e: # Handle temporary errors that can be retried log.warn(\"Temporary error, will retry: {}\", str(e)) value[\"status\"] = \"retry\" value[\"error_details\"] = str(e) return (key, value) except Exception as e: # Handle unexpected errors log.error(\"Unexpected error: {}\", str(e)) value[\"status\"] = \"fatal_error\" value[\"error_details\"] = str(e) return (key, value) pipelines: error_routing: from: input_stream transformKeyValue: classify_errors branch: - predicate: is_success to: success_stream - predicate: is_validation_error to: validation_errors - predicate: is_retry to: retry_queue - to: fatal_errors Implementing Recovery Mechanisms 1. Retry Logic Implement retry logic for transient failures, such as temporary network issues: functions: retry_external_call: type: valueTransformer code: | max_retries = 3 retry_count = 0 while retry_count < max_retries: try: # Attempt to call external service result = call_external_service(value) return result except TemporaryError as e: # Log the retry attempt retry_count += 1 log.warn(\"Retry {}/{} after error: {}\", retry_count, max_retries, str(e)) # Add exponential backoff time.sleep(0.1 * (2 ** retry_count)) except Exception as e: # Non-retryable error log.error(\"Non-retryable error: {}\", str(e)) return {\"error\": str(e), \"original_data\": value} # If we've exhausted retries log.error(\"Failed after {} retries\", max_retries) return {\"error\": \"Max retries exceeded\", \"original_data\": value} 2. Circuit Breakers Implement circuit breaker patterns to prevent cascading failures when external systems are down: functions: circuit_breaker: type: valueTransformer globalCode: | # Circuit breaker state circuit_state = \"CLOSED\" # CLOSED, OPEN, HALF_OPEN failure_count = 0 last_failure_time = 0 failure_threshold = 5 reset_timeout = 30 # seconds code: | global circuit_state, failure_count, last_failure_time current_time = time.time() # Check if circuit is OPEN if circuit_state == \"OPEN\": # Check if it's time to try again if current_time - last_failure_time > reset_timeout: log.info(\"Circuit transitioning from OPEN to HALF_OPEN\") circuit_state = \"HALF_OPEN\" else: # Circuit is still OPEN, fail fast log.warn(\"Circuit OPEN, skipping call\") return {\"error\": \"Circuit breaker open\", \"original_data\": value} try: # Attempt the call result = call_external_service(value) # If successful and in HALF_OPEN, reset the circuit if circuit_state == \"HALF_OPEN\": log.info(\"Circuit transitioning from HALF_OPEN to CLOSED\") circuit_state = \"CLOSED\" failure_count = 0 return result except Exception as e: # Record the failure failure_count += 1 last_failure_time = current_time # Check if we need to open the circuit if circuit_state == \"CLOSED\" and failure_count >= failure_threshold: log.warn(\"Circuit transitioning from CLOSED to OPEN after {} failures\", failure_count) circuit_state = \"OPEN\" # Log and return error log.error(\"Service call failed: {}\", str(e)) return {\"error\": str(e), \"original_data\": value} 3. Compensating Transactions For operations that need to maintain consistency across multiple systems, implement compensating transactions: functions: process_with_compensation: type: valueTransformer code: | # Track operations that need to be compensated if there's a failure completed_operations = [] try: # First operation result1 = operation1(value) completed_operations.append(\"operation1\") # Second operation result2 = operation2(result1) completed_operations.append(\"operation2\") # Third operation result3 = operation3(result2) completed_operations.append(\"operation3\") return result3 except Exception as e: log.error(\"Error during processing: {}\", str(e)) # Perform compensating actions in reverse order for operation in reversed(completed_operations): try: if operation == \"operation1\": compensate_operation1(value) elif operation == \"operation2\": compensate_operation2(result1) elif operation == \"operation3\": compensate_operation3(result2) except Exception as comp_error: log.error(\"Error during compensation for {}: {}\", operation, str(comp_error)) # Return error information return {\"error\": str(e), \"compensated\": completed_operations, \"original_data\": value} Practical Example: Robust Order Processing Let's build a complete example that implements robust error handling in an order processing system: streams: incoming_orders: topic: new_orders keyType: string # Order ID valueType: json # Order details validated_orders: topic: validated_orders keyType: string # Order ID valueType: json # Validated order details processed_orders: topic: processed_orders keyType: string # Order ID valueType: json # Processed order details invalid_orders: topic: invalid_orders keyType: string # Order ID valueType: json # Order with validation errors processing_errors: topic: order_processing_errors keyType: string # Order ID valueType: json # Order with processing errors retry_orders: topic: orders_to_retry keyType: string # Order ID valueType: json # Orders to retry later functions: validate_order: type: keyValueTransformer code: | try: # Check if order has all required fields required_fields = [\"customer_id\", \"items\", \"shipping_address\", \"payment_method\"] missing_fields = [field for field in required_fields if field not in value or not value[field]] if missing_fields: # Return invalid order with details value[\"status\"] = \"invalid\" value[\"validation_errors\"] = {\"missing_fields\": missing_fields} value[\"timestamp\"] = int(time.time() * 1000) return (key, value) # Check if items array is not empty if not value.get(\"items\") or len(value[\"items\"]) == 0: value[\"status\"] = \"invalid\" value[\"validation_errors\"] = {\"reason\": \"Order contains no items\"} value[\"timestamp\"] = int(time.time() * 1000) return (key, value) # Order is valid value[\"status\"] = \"valid\" value[\"validation_timestamp\"] = int(time.time() * 1000) return (key, value) except Exception as e: # Handle unexpected errors during validation log.error(\"Error validating order {}: {}\", key, str(e)) value[\"status\"] = \"error\" value[\"error_details\"] = str(e) value[\"error_timestamp\"] = int(time.time() * 1000) return (key, value) is_valid_order: type: predicate expression: value.get(\"status\") == \"valid\" is_invalid_order: type: predicate expression: value.get(\"status\") == \"invalid\" is_error_order: type: predicate expression: value.get(\"status\") == \"error\" process_order: type: valueTransformer code: | if value is None or value.get(\"status\") != \"valid\": log.warn(\"Received invalid order for processing: {}\", key) return value try: # Simulate inventory check for item in value.get(\"items\", []): # Simulate temporary failure for some items if \"retry\" in item.get(\"product_id\", \"\"): raise TemporaryError(f\"Temporary inventory issue with {item['product_id']}\") # Simulate permanent failure for some items if \"fail\" in item.get(\"product_id\", \"\"): raise PermanentError(f\"Product {item['product_id']} is discontinued\") # Simulate payment processing if value.get(\"payment_method\") == \"credit_card\": # Add payment processing logic here value[\"payment_status\"] = \"processed\" # Order successfully processed value[\"status\"] = \"processed\" value[\"processing_timestamp\"] = int(time.time() * 1000) return value except TemporaryError as e: # Handle temporary errors (can be retried) log.warn(\"Temporary error processing order {}: {}\", key, str(e)) value[\"status\"] = \"retry\" value[\"retry_reason\"] = str(e) value[\"retry_timestamp\"] = int(time.time() * 1000) value[\"retry_count\"] = value.get(\"retry_count\", 0) + 1 return value except Exception as e: # Handle permanent errors log.error(\"Error processing order {}: {}\", key, str(e)) value[\"status\"] = \"failed\" value[\"error_details\"] = str(e) value[\"error_timestamp\"] = int(time.time() * 1000) return value is_processed_order: type: predicate expression: value.get(\"status\") == \"processed\" is_retry_order: type: predicate expression: value.get(\"status\") == \"retry\" is_failed_order: type: predicate expression: value.get(\"status\") == \"failed\" pipelines: # Validate incoming orders validate_orders: from: incoming_orders transformKeyValue: validate_order branch: - predicate: is_valid_order to: validated_orders - predicate: is_invalid_order to: invalid_orders - to: processing_errors # Catch-all for unexpected errors # Process validated orders process_orders: from: validated_orders mapValues: process_order branch: - predicate: is_processed_order to: processed_orders - predicate: is_retry_order to: retry_orders - to: processing_errors # Failed orders and unexpected errors This example: 1. Validates incoming orders and routes them based on validation results 2. Processes valid orders with error handling for temporary and permanent failures 3. Routes processed orders to different destinations based on processing results 4. Implements a retry mechanism for orders with temporary issues Monitoring and Debugging To effectively manage errors in your KSML applications: 1. Implement Comprehensive Logging Use the KSML logging capabilities to track errors and their context: functions: log_with_context: type: forEach code: | # Log with different levels based on message status status = value.get(\"status\", \"unknown\") if status == \"error\" or status == \"failed\": log.error(\"Order {} failed: {}\", key, value.get(\"error_details\", \"Unknown error\")) elif status == \"retry\": log.warn(\"Order {} needs retry: {}\", key, value.get(\"retry_reason\", \"Unknown reason\")) elif status == \"invalid\": log.warn(\"Order {} is invalid: {}\", key, value.get(\"validation_errors\", \"Unknown validation error\")) else: log.info(\"Processing order {}: status={}\", key, status) 2. Use Metrics to Track Error Rates Track error metrics to monitor the health of your application: functions: track_error_metrics: type: forEach code: | # Get status status = value.get(\"status\", \"unknown\") # Update appropriate counter based on status if status == \"valid\": metrics.counter(\"orders.valid\").increment() elif status == \"invalid\": metrics.counter(\"orders.invalid\").increment() elif status == \"processed\": metrics.counter(\"orders.processed\").increment() elif status == \"failed\": metrics.counter(\"orders.failed\").increment() elif status == \"retry\": metrics.counter(\"orders.retry\").increment() else: metrics.counter(\"orders.unknown\").increment() 3. Implement Health Checks Create health check streams that monitor error rates and alert when they exceed thresholds. Best Practices for Error Handling Fail Fast : Validate input data early to catch issues before expensive processing Be Specific : Catch specific exceptions rather than using broad exception handlers Provide Context : Include relevant information in error messages and logs Design for Failure : Assume that errors will occur and design your pipelines accordingly Isolate Failures : Use circuit breakers to prevent cascading failures Monitor and Alert : Set up monitoring and alerting for error conditions Test Error Scenarios : Explicitly test error handling code with simulated failures Conclusion Robust error handling is essential for building reliable stream processing applications. KSML provides flexible mechanisms for handling errors at various levels, from simple try-except blocks to sophisticated patterns like dead letter queues and circuit breakers. By implementing proper error handling and recovery mechanisms, you can build KSML applications that gracefully handle failures, maintain data integrity, and continue processing even when problems occur. Further Reading Core Concepts: Operations Core Concepts: Functions Reference: Error Handling","title":"Error Handling and Recovery in KSML"},{"location":"tutorials/intermediate/error-handling/#error-handling-and-recovery-in-ksml","text":"This tutorial explores strategies for handling errors and implementing recovery mechanisms in KSML applications, helping you build more robust and resilient stream processing pipelines.","title":"Error Handling and Recovery in KSML"},{"location":"tutorials/intermediate/error-handling/#introduction-to-error-handling","text":"Error handling is a critical aspect of any production-grade stream processing application. In streaming contexts, errors can occur for various reasons: Invalid or malformed input data External service failures Resource constraints Business rule violations Unexpected edge cases Without proper error handling, these issues can cause your application to: Crash and stop processing Skip or lose important messages Produce incorrect results Create inconsistent state KSML provides several mechanisms to handle errors gracefully and implement recovery strategies, allowing your applications to continue processing even when problems occur.","title":"Introduction to Error Handling"},{"location":"tutorials/intermediate/error-handling/#prerequisites","text":"Before starting this tutorial, you should: Understand basic KSML concepts (streams, functions, pipelines) Have completed the KSML Basics Tutorial Be familiar with Python exception handling","title":"Prerequisites"},{"location":"tutorials/intermediate/error-handling/#error-handling-strategies-in-ksml","text":"","title":"Error Handling Strategies in KSML"},{"location":"tutorials/intermediate/error-handling/#1-try-except-blocks-in-python-functions","text":"The most basic form of error handling in KSML is using try-except blocks in your Python functions: functions: safe_transform: type: valueTransformer code: | try: # Potentially risky operation result = process_data(value) return result except Exception as e: # Log the error log.error(\"Error processing data: {}\", str(e)) # Return a default or fallback value return {\"error\": str(e), \"original_data\": value} This approach: - Prevents the function from failing completely - Logs the error for troubleshooting - Returns a fallback value that allows processing to continue","title":"1. Try-Except Blocks in Python Functions"},{"location":"tutorials/intermediate/error-handling/#2-validation-and-filtering","text":"Proactively validate data and filter out problematic messages before they cause errors: functions: validate_data: type: predicate code: | # Check if the message has all required fields if value is None: log.warn(\"Received null value for key: {}\", key) return False required_fields = [\"user_id\", \"timestamp\", \"action\"] for field in required_fields: if field not in value: log.warn(\"Missing required field '{}' in message with key: {}\", field, key) return False return True pipelines: process_valid_data: from: input_stream filter: validate_data # Only process messages that pass validation # Continue processing with valid data... to: processed_stream","title":"2. Validation and Filtering"},{"location":"tutorials/intermediate/error-handling/#3-dead-letter-queues","text":"Implement a \"dead letter queue\" pattern to capture and store messages that couldn't be processed: functions: process_with_dlq: type: keyValueTransformer code: | try: # Attempt to process the message processed_value = process_data(value) # Return the processed message to the main output return (key, processed_value) except Exception as e: # Create an error record error_record = { \"original_key\": key, \"original_value\": value, \"error\": str(e), \"timestamp\": int(time.time() * 1000) } # Send to the dead letter queue dlq_topic.send(key, error_record) # Return None to filter this message from the main output return None pipelines: main_processing: from: input_stream transformKeyValue: process_with_dlq to: processed_stream","title":"3. Dead Letter Queues"},{"location":"tutorials/intermediate/error-handling/#4-error-classification-and-routing","text":"Classify errors and route messages to different handling paths based on the error type: functions: classify_errors: type: keyValueTransformer code: | try: # Attempt normal processing result = process_data(value) # Tag with success status result[\"status\"] = \"success\" return (key, result) except ValidationError as e: # Handle validation errors log.warn(\"Validation error: {}\", str(e)) value[\"status\"] = \"validation_error\" value[\"error_details\"] = str(e) return (key, value) except TemporaryError as e: # Handle temporary errors that can be retried log.warn(\"Temporary error, will retry: {}\", str(e)) value[\"status\"] = \"retry\" value[\"error_details\"] = str(e) return (key, value) except Exception as e: # Handle unexpected errors log.error(\"Unexpected error: {}\", str(e)) value[\"status\"] = \"fatal_error\" value[\"error_details\"] = str(e) return (key, value) pipelines: error_routing: from: input_stream transformKeyValue: classify_errors branch: - predicate: is_success to: success_stream - predicate: is_validation_error to: validation_errors - predicate: is_retry to: retry_queue - to: fatal_errors","title":"4. Error Classification and Routing"},{"location":"tutorials/intermediate/error-handling/#implementing-recovery-mechanisms","text":"","title":"Implementing Recovery Mechanisms"},{"location":"tutorials/intermediate/error-handling/#1-retry-logic","text":"Implement retry logic for transient failures, such as temporary network issues: functions: retry_external_call: type: valueTransformer code: | max_retries = 3 retry_count = 0 while retry_count < max_retries: try: # Attempt to call external service result = call_external_service(value) return result except TemporaryError as e: # Log the retry attempt retry_count += 1 log.warn(\"Retry {}/{} after error: {}\", retry_count, max_retries, str(e)) # Add exponential backoff time.sleep(0.1 * (2 ** retry_count)) except Exception as e: # Non-retryable error log.error(\"Non-retryable error: {}\", str(e)) return {\"error\": str(e), \"original_data\": value} # If we've exhausted retries log.error(\"Failed after {} retries\", max_retries) return {\"error\": \"Max retries exceeded\", \"original_data\": value}","title":"1. Retry Logic"},{"location":"tutorials/intermediate/error-handling/#2-circuit-breakers","text":"Implement circuit breaker patterns to prevent cascading failures when external systems are down: functions: circuit_breaker: type: valueTransformer globalCode: | # Circuit breaker state circuit_state = \"CLOSED\" # CLOSED, OPEN, HALF_OPEN failure_count = 0 last_failure_time = 0 failure_threshold = 5 reset_timeout = 30 # seconds code: | global circuit_state, failure_count, last_failure_time current_time = time.time() # Check if circuit is OPEN if circuit_state == \"OPEN\": # Check if it's time to try again if current_time - last_failure_time > reset_timeout: log.info(\"Circuit transitioning from OPEN to HALF_OPEN\") circuit_state = \"HALF_OPEN\" else: # Circuit is still OPEN, fail fast log.warn(\"Circuit OPEN, skipping call\") return {\"error\": \"Circuit breaker open\", \"original_data\": value} try: # Attempt the call result = call_external_service(value) # If successful and in HALF_OPEN, reset the circuit if circuit_state == \"HALF_OPEN\": log.info(\"Circuit transitioning from HALF_OPEN to CLOSED\") circuit_state = \"CLOSED\" failure_count = 0 return result except Exception as e: # Record the failure failure_count += 1 last_failure_time = current_time # Check if we need to open the circuit if circuit_state == \"CLOSED\" and failure_count >= failure_threshold: log.warn(\"Circuit transitioning from CLOSED to OPEN after {} failures\", failure_count) circuit_state = \"OPEN\" # Log and return error log.error(\"Service call failed: {}\", str(e)) return {\"error\": str(e), \"original_data\": value}","title":"2. Circuit Breakers"},{"location":"tutorials/intermediate/error-handling/#3-compensating-transactions","text":"For operations that need to maintain consistency across multiple systems, implement compensating transactions: functions: process_with_compensation: type: valueTransformer code: | # Track operations that need to be compensated if there's a failure completed_operations = [] try: # First operation result1 = operation1(value) completed_operations.append(\"operation1\") # Second operation result2 = operation2(result1) completed_operations.append(\"operation2\") # Third operation result3 = operation3(result2) completed_operations.append(\"operation3\") return result3 except Exception as e: log.error(\"Error during processing: {}\", str(e)) # Perform compensating actions in reverse order for operation in reversed(completed_operations): try: if operation == \"operation1\": compensate_operation1(value) elif operation == \"operation2\": compensate_operation2(result1) elif operation == \"operation3\": compensate_operation3(result2) except Exception as comp_error: log.error(\"Error during compensation for {}: {}\", operation, str(comp_error)) # Return error information return {\"error\": str(e), \"compensated\": completed_operations, \"original_data\": value}","title":"3. Compensating Transactions"},{"location":"tutorials/intermediate/error-handling/#practical-example-robust-order-processing","text":"Let's build a complete example that implements robust error handling in an order processing system: streams: incoming_orders: topic: new_orders keyType: string # Order ID valueType: json # Order details validated_orders: topic: validated_orders keyType: string # Order ID valueType: json # Validated order details processed_orders: topic: processed_orders keyType: string # Order ID valueType: json # Processed order details invalid_orders: topic: invalid_orders keyType: string # Order ID valueType: json # Order with validation errors processing_errors: topic: order_processing_errors keyType: string # Order ID valueType: json # Order with processing errors retry_orders: topic: orders_to_retry keyType: string # Order ID valueType: json # Orders to retry later functions: validate_order: type: keyValueTransformer code: | try: # Check if order has all required fields required_fields = [\"customer_id\", \"items\", \"shipping_address\", \"payment_method\"] missing_fields = [field for field in required_fields if field not in value or not value[field]] if missing_fields: # Return invalid order with details value[\"status\"] = \"invalid\" value[\"validation_errors\"] = {\"missing_fields\": missing_fields} value[\"timestamp\"] = int(time.time() * 1000) return (key, value) # Check if items array is not empty if not value.get(\"items\") or len(value[\"items\"]) == 0: value[\"status\"] = \"invalid\" value[\"validation_errors\"] = {\"reason\": \"Order contains no items\"} value[\"timestamp\"] = int(time.time() * 1000) return (key, value) # Order is valid value[\"status\"] = \"valid\" value[\"validation_timestamp\"] = int(time.time() * 1000) return (key, value) except Exception as e: # Handle unexpected errors during validation log.error(\"Error validating order {}: {}\", key, str(e)) value[\"status\"] = \"error\" value[\"error_details\"] = str(e) value[\"error_timestamp\"] = int(time.time() * 1000) return (key, value) is_valid_order: type: predicate expression: value.get(\"status\") == \"valid\" is_invalid_order: type: predicate expression: value.get(\"status\") == \"invalid\" is_error_order: type: predicate expression: value.get(\"status\") == \"error\" process_order: type: valueTransformer code: | if value is None or value.get(\"status\") != \"valid\": log.warn(\"Received invalid order for processing: {}\", key) return value try: # Simulate inventory check for item in value.get(\"items\", []): # Simulate temporary failure for some items if \"retry\" in item.get(\"product_id\", \"\"): raise TemporaryError(f\"Temporary inventory issue with {item['product_id']}\") # Simulate permanent failure for some items if \"fail\" in item.get(\"product_id\", \"\"): raise PermanentError(f\"Product {item['product_id']} is discontinued\") # Simulate payment processing if value.get(\"payment_method\") == \"credit_card\": # Add payment processing logic here value[\"payment_status\"] = \"processed\" # Order successfully processed value[\"status\"] = \"processed\" value[\"processing_timestamp\"] = int(time.time() * 1000) return value except TemporaryError as e: # Handle temporary errors (can be retried) log.warn(\"Temporary error processing order {}: {}\", key, str(e)) value[\"status\"] = \"retry\" value[\"retry_reason\"] = str(e) value[\"retry_timestamp\"] = int(time.time() * 1000) value[\"retry_count\"] = value.get(\"retry_count\", 0) + 1 return value except Exception as e: # Handle permanent errors log.error(\"Error processing order {}: {}\", key, str(e)) value[\"status\"] = \"failed\" value[\"error_details\"] = str(e) value[\"error_timestamp\"] = int(time.time() * 1000) return value is_processed_order: type: predicate expression: value.get(\"status\") == \"processed\" is_retry_order: type: predicate expression: value.get(\"status\") == \"retry\" is_failed_order: type: predicate expression: value.get(\"status\") == \"failed\" pipelines: # Validate incoming orders validate_orders: from: incoming_orders transformKeyValue: validate_order branch: - predicate: is_valid_order to: validated_orders - predicate: is_invalid_order to: invalid_orders - to: processing_errors # Catch-all for unexpected errors # Process validated orders process_orders: from: validated_orders mapValues: process_order branch: - predicate: is_processed_order to: processed_orders - predicate: is_retry_order to: retry_orders - to: processing_errors # Failed orders and unexpected errors This example: 1. Validates incoming orders and routes them based on validation results 2. Processes valid orders with error handling for temporary and permanent failures 3. Routes processed orders to different destinations based on processing results 4. Implements a retry mechanism for orders with temporary issues","title":"Practical Example: Robust Order Processing"},{"location":"tutorials/intermediate/error-handling/#monitoring-and-debugging","text":"To effectively manage errors in your KSML applications:","title":"Monitoring and Debugging"},{"location":"tutorials/intermediate/error-handling/#1-implement-comprehensive-logging","text":"Use the KSML logging capabilities to track errors and their context: functions: log_with_context: type: forEach code: | # Log with different levels based on message status status = value.get(\"status\", \"unknown\") if status == \"error\" or status == \"failed\": log.error(\"Order {} failed: {}\", key, value.get(\"error_details\", \"Unknown error\")) elif status == \"retry\": log.warn(\"Order {} needs retry: {}\", key, value.get(\"retry_reason\", \"Unknown reason\")) elif status == \"invalid\": log.warn(\"Order {} is invalid: {}\", key, value.get(\"validation_errors\", \"Unknown validation error\")) else: log.info(\"Processing order {}: status={}\", key, status)","title":"1. Implement Comprehensive Logging"},{"location":"tutorials/intermediate/error-handling/#2-use-metrics-to-track-error-rates","text":"Track error metrics to monitor the health of your application: functions: track_error_metrics: type: forEach code: | # Get status status = value.get(\"status\", \"unknown\") # Update appropriate counter based on status if status == \"valid\": metrics.counter(\"orders.valid\").increment() elif status == \"invalid\": metrics.counter(\"orders.invalid\").increment() elif status == \"processed\": metrics.counter(\"orders.processed\").increment() elif status == \"failed\": metrics.counter(\"orders.failed\").increment() elif status == \"retry\": metrics.counter(\"orders.retry\").increment() else: metrics.counter(\"orders.unknown\").increment()","title":"2. Use Metrics to Track Error Rates"},{"location":"tutorials/intermediate/error-handling/#3-implement-health-checks","text":"Create health check streams that monitor error rates and alert when they exceed thresholds.","title":"3. Implement Health Checks"},{"location":"tutorials/intermediate/error-handling/#best-practices-for-error-handling","text":"Fail Fast : Validate input data early to catch issues before expensive processing Be Specific : Catch specific exceptions rather than using broad exception handlers Provide Context : Include relevant information in error messages and logs Design for Failure : Assume that errors will occur and design your pipelines accordingly Isolate Failures : Use circuit breakers to prevent cascading failures Monitor and Alert : Set up monitoring and alerting for error conditions Test Error Scenarios : Explicitly test error handling code with simulated failures","title":"Best Practices for Error Handling"},{"location":"tutorials/intermediate/error-handling/#conclusion","text":"Robust error handling is essential for building reliable stream processing applications. KSML provides flexible mechanisms for handling errors at various levels, from simple try-except blocks to sophisticated patterns like dead letter queues and circuit breakers. By implementing proper error handling and recovery mechanisms, you can build KSML applications that gracefully handle failures, maintain data integrity, and continue processing even when problems occur.","title":"Conclusion"},{"location":"tutorials/intermediate/error-handling/#further-reading","text":"Core Concepts: Operations Core Concepts: Functions Reference: Error Handling","title":"Further Reading"},{"location":"tutorials/intermediate/joins/","text":"Implementing Joins in KSML This tutorial explores how to implement join operations in KSML, allowing you to combine data from multiple streams or tables to create enriched datasets. Introduction to Joins Joins are powerful operations that allow you to combine data from different sources based on a common key. In stream processing, joins enable you to: Enrich streaming data with reference information Correlate events from different systems Build comprehensive views of entities from fragmented data Implement complex business logic that depends on multiple data sources KSML supports various types of joins, each with different semantics and use cases. Prerequisites Before starting this tutorial, you should: Understand basic KSML concepts (streams, functions, pipelines) Have completed the KSML Basics Tutorial Be familiar with Stateful Operations Understand the difference between streams and tables Types of Joins in KSML KSML supports several types of joins, each with different semantics: Stream-Stream Joins Join two streams based on a common key within a specified time window: Inner Join ( join ) : Outputs a result only when both streams have matching keys within the time window Left Join ( leftJoin ) : Always outputs a result for messages from the left stream, joining with the right stream if available Outer Join ( outerJoin ) : Outputs a result whenever either stream has a message, joining them when both are available Stream-Table Joins Join a stream with a table (materialized view) based on the key: Inner Join ( join ) : Outputs a result only when the stream key exists in the table Left Join ( leftJoin ) : Always outputs a result for stream messages, joining with the table value if available Stream-GlobalTable Joins Join a stream with a global table, with the ability to use a foreign key: Inner Join ( join ) : Outputs a result only when the stream's foreign key exists in the global table Left Join ( leftJoin ) : Always outputs a result for stream messages, joining with the global table value if available Basic Join Example Let's start with a simple example that joins a stream of orders with a table of customer information: streams: orders: topic: new_orders keyType: string # Customer ID valueType: json # Order details enriched_orders: topic: orders_with_customer_data keyType: string # Customer ID valueType: json # Combined order and customer data tables: customers: topic: customer_data keyType: string # Customer ID valueType: json # Customer details functions: join_order_with_customer: type: valueJoiner code: | # Combine order and customer information result = {} # Add order details if value1 is not None: result.update(value1) # Add customer details if value2 is not None: result[\"customer\"] = value2 return result pipelines: enrich_orders: from: orders join: stream: customers valueJoiner: join_order_with_customer to: enriched_orders This pipeline: 1. Takes a stream of orders with customer IDs as keys 2. Joins it with a table of customer data using the customer ID 3. Combines the order and customer information using a valueJoiner function 4. Outputs the enriched orders to a new topic Working with Time Windows in Joins Stream-stream joins require a time window to define how long to wait for matching records: pipelines: match_clicks_with_purchases: from: product_clicks join: stream: product_purchases valueJoiner: correlate_click_and_purchase window: type: time size: 30m # Look for purchases within 30 minutes of a click to: correlated_user_actions Foreign Key Joins When joining with a GlobalKTable, you can use a foreign key extractor to join on fields other than the primary key: streams: orders: topic: new_orders keyType: string # Order ID valueType: json # Order details including product_id globalTables: products: topic: product_catalog keyType: string # Product ID valueType: json # Product details functions: extract_product_id: type: foreignKeyExtractor expression: value.get(\"product_id\") join_order_with_product: type: valueJoiner code: | # Combine order and product information result = {} # Add order details if value1 is not None: result.update(value1) # Add product details if value2 is not None: result[\"product\"] = value2 return result pipelines: enrich_orders_with_products: from: orders join: globalTable: products foreignKeyExtractor: extract_product_id valueJoiner: join_order_with_product to: orders_with_product_details Practical Example: Order Processing System Let's build a more complex example that implements an order processing system with multiple joins: streams: orders: topic: incoming_orders keyType: string # Order ID valueType: json # Order details including customer_id and items array processed_orders: topic: orders_ready_for_fulfillment keyType: string # Order ID valueType: json # Fully processed order globalTables: customers: topic: customer_data keyType: string # Customer ID valueType: json # Customer details inventory: topic: inventory_levels keyType: string # Product ID valueType: json # Inventory information shipping_rates: topic: shipping_rate_data keyType: string # Region code valueType: json # Shipping rates functions: extract_customer_id: type: foreignKeyExtractor expression: value.get(\"customer_id\") join_order_with_customer: type: valueJoiner code: | result = value1.copy() if value1 else {} if value2: result[\"customer_details\"] = value2 return result extract_region_code: type: foreignKeyExtractor code: | if value and \"customer_details\" in value and \"region\" in value[\"customer_details\"]: return value[\"customer_details\"][\"region\"] return \"UNKNOWN\" join_with_shipping_rates: type: valueJoiner code: | result = value1.copy() if value1 else {} if value2: result[\"shipping_rates\"] = value2 return result check_inventory_and_calculate_total: type: valueTransformer code: | if value is None: return None # Initialize fields value[\"items_with_inventory\"] = [] value[\"out_of_stock_items\"] = [] value[\"subtotal\"] = 0 value[\"shipping_cost\"] = 0 value[\"total\"] = 0 # Process each item for item in value.get(\"items\", []): product_id = item.get(\"product_id\") quantity = item.get(\"quantity\", 0) price = item.get(\"price\", 0) # Check inventory (would be another join in a real system) # For simplicity, we're just checking if the product_id is even or odd in_stock = int(product_id) % 2 == 0 if in_stock: value[\"items_with_inventory\"].append(item) value[\"subtotal\"] += price * quantity else: value[\"out_of_stock_items\"].append(item) # Calculate shipping if \"shipping_rates\" in value: base_rate = value[\"shipping_rates\"].get(\"base_rate\", 5.0) value[\"shipping_cost\"] = base_rate # Calculate total value[\"total\"] = value[\"subtotal\"] + value[\"shipping_cost\"] return value pipelines: # First pipeline: Join orders with customer data join_orders_with_customers: from: orders join: globalTable: customers foreignKeyExtractor: extract_customer_id valueJoiner: join_order_with_customer to: orders_with_customers # Second pipeline: Join with shipping rates join_with_shipping: from: orders_with_customers join: globalTable: shipping_rates foreignKeyExtractor: extract_region_code valueJoiner: join_with_shipping_rates to: orders_with_shipping # Final pipeline: Process inventory and calculate totals finalize_orders: from: orders_with_shipping mapValues: check_inventory_and_calculate_total to: processed_orders This pipeline: 1. Takes incoming orders 2. Enriches them with customer data using a foreign key join 3. Adds shipping rates based on the customer's region 4. Checks inventory, calculates totals, and marks items as in-stock or out-of-stock 5. Outputs the fully processed order Best Practices for Joins Performance Considerations State Size : Joins maintain state, which consumes memory. Monitor state store sizes. Window Size : For windowed joins, smaller windows use less state but may miss matches. Join Order : Join with smaller datasets first when possible to reduce intermediate result sizes. Design Patterns Pre-filtering : Filter unnecessary data before joins to reduce state size. Denormalization : Consider denormalizing data at write time for frequently joined data. Caching : Use GlobalKTables for reference data that needs to be joined with many streams. Error Handling Handle missing or invalid data in your joiner functions: functions: robust_joiner: type: valueJoiner code: | try: # Validate inputs if value1 is None: log.warn(\"Left side of join is null for key: {}\", key) return None if value2 is None: log.warn(\"Right side of join is null for key: {}\", key) # Still proceed with just the left data return value1 # Perform the join result = value1.copy() result[\"joined_data\"] = value2 return result except Exception as e: log.error(\"Error in join operation: {}\", str(e)) # Return left side data to avoid losing the record return value1 Conclusion Joins are essential operations for building sophisticated stream processing applications. KSML makes it easy to implement various types of joins while leveraging Python for the joining logic. By understanding the different join types and their appropriate use cases, you can build powerful data pipelines that combine and enrich data from multiple sources. In the next tutorial, we'll explore Using Windowed Operations to process data within time boundaries. Further Reading Core Concepts: Operations Core Concepts: Streams and Data Types Reference: Join Operations","title":"Implementing Joins in KSML"},{"location":"tutorials/intermediate/joins/#implementing-joins-in-ksml","text":"This tutorial explores how to implement join operations in KSML, allowing you to combine data from multiple streams or tables to create enriched datasets.","title":"Implementing Joins in KSML"},{"location":"tutorials/intermediate/joins/#introduction-to-joins","text":"Joins are powerful operations that allow you to combine data from different sources based on a common key. In stream processing, joins enable you to: Enrich streaming data with reference information Correlate events from different systems Build comprehensive views of entities from fragmented data Implement complex business logic that depends on multiple data sources KSML supports various types of joins, each with different semantics and use cases.","title":"Introduction to Joins"},{"location":"tutorials/intermediate/joins/#prerequisites","text":"Before starting this tutorial, you should: Understand basic KSML concepts (streams, functions, pipelines) Have completed the KSML Basics Tutorial Be familiar with Stateful Operations Understand the difference between streams and tables","title":"Prerequisites"},{"location":"tutorials/intermediate/joins/#types-of-joins-in-ksml","text":"KSML supports several types of joins, each with different semantics:","title":"Types of Joins in KSML"},{"location":"tutorials/intermediate/joins/#stream-stream-joins","text":"Join two streams based on a common key within a specified time window: Inner Join ( join ) : Outputs a result only when both streams have matching keys within the time window Left Join ( leftJoin ) : Always outputs a result for messages from the left stream, joining with the right stream if available Outer Join ( outerJoin ) : Outputs a result whenever either stream has a message, joining them when both are available","title":"Stream-Stream Joins"},{"location":"tutorials/intermediate/joins/#stream-table-joins","text":"Join a stream with a table (materialized view) based on the key: Inner Join ( join ) : Outputs a result only when the stream key exists in the table Left Join ( leftJoin ) : Always outputs a result for stream messages, joining with the table value if available","title":"Stream-Table Joins"},{"location":"tutorials/intermediate/joins/#stream-globaltable-joins","text":"Join a stream with a global table, with the ability to use a foreign key: Inner Join ( join ) : Outputs a result only when the stream's foreign key exists in the global table Left Join ( leftJoin ) : Always outputs a result for stream messages, joining with the global table value if available","title":"Stream-GlobalTable Joins"},{"location":"tutorials/intermediate/joins/#basic-join-example","text":"Let's start with a simple example that joins a stream of orders with a table of customer information: streams: orders: topic: new_orders keyType: string # Customer ID valueType: json # Order details enriched_orders: topic: orders_with_customer_data keyType: string # Customer ID valueType: json # Combined order and customer data tables: customers: topic: customer_data keyType: string # Customer ID valueType: json # Customer details functions: join_order_with_customer: type: valueJoiner code: | # Combine order and customer information result = {} # Add order details if value1 is not None: result.update(value1) # Add customer details if value2 is not None: result[\"customer\"] = value2 return result pipelines: enrich_orders: from: orders join: stream: customers valueJoiner: join_order_with_customer to: enriched_orders This pipeline: 1. Takes a stream of orders with customer IDs as keys 2. Joins it with a table of customer data using the customer ID 3. Combines the order and customer information using a valueJoiner function 4. Outputs the enriched orders to a new topic","title":"Basic Join Example"},{"location":"tutorials/intermediate/joins/#working-with-time-windows-in-joins","text":"Stream-stream joins require a time window to define how long to wait for matching records: pipelines: match_clicks_with_purchases: from: product_clicks join: stream: product_purchases valueJoiner: correlate_click_and_purchase window: type: time size: 30m # Look for purchases within 30 minutes of a click to: correlated_user_actions","title":"Working with Time Windows in Joins"},{"location":"tutorials/intermediate/joins/#foreign-key-joins","text":"When joining with a GlobalKTable, you can use a foreign key extractor to join on fields other than the primary key: streams: orders: topic: new_orders keyType: string # Order ID valueType: json # Order details including product_id globalTables: products: topic: product_catalog keyType: string # Product ID valueType: json # Product details functions: extract_product_id: type: foreignKeyExtractor expression: value.get(\"product_id\") join_order_with_product: type: valueJoiner code: | # Combine order and product information result = {} # Add order details if value1 is not None: result.update(value1) # Add product details if value2 is not None: result[\"product\"] = value2 return result pipelines: enrich_orders_with_products: from: orders join: globalTable: products foreignKeyExtractor: extract_product_id valueJoiner: join_order_with_product to: orders_with_product_details","title":"Foreign Key Joins"},{"location":"tutorials/intermediate/joins/#practical-example-order-processing-system","text":"Let's build a more complex example that implements an order processing system with multiple joins: streams: orders: topic: incoming_orders keyType: string # Order ID valueType: json # Order details including customer_id and items array processed_orders: topic: orders_ready_for_fulfillment keyType: string # Order ID valueType: json # Fully processed order globalTables: customers: topic: customer_data keyType: string # Customer ID valueType: json # Customer details inventory: topic: inventory_levels keyType: string # Product ID valueType: json # Inventory information shipping_rates: topic: shipping_rate_data keyType: string # Region code valueType: json # Shipping rates functions: extract_customer_id: type: foreignKeyExtractor expression: value.get(\"customer_id\") join_order_with_customer: type: valueJoiner code: | result = value1.copy() if value1 else {} if value2: result[\"customer_details\"] = value2 return result extract_region_code: type: foreignKeyExtractor code: | if value and \"customer_details\" in value and \"region\" in value[\"customer_details\"]: return value[\"customer_details\"][\"region\"] return \"UNKNOWN\" join_with_shipping_rates: type: valueJoiner code: | result = value1.copy() if value1 else {} if value2: result[\"shipping_rates\"] = value2 return result check_inventory_and_calculate_total: type: valueTransformer code: | if value is None: return None # Initialize fields value[\"items_with_inventory\"] = [] value[\"out_of_stock_items\"] = [] value[\"subtotal\"] = 0 value[\"shipping_cost\"] = 0 value[\"total\"] = 0 # Process each item for item in value.get(\"items\", []): product_id = item.get(\"product_id\") quantity = item.get(\"quantity\", 0) price = item.get(\"price\", 0) # Check inventory (would be another join in a real system) # For simplicity, we're just checking if the product_id is even or odd in_stock = int(product_id) % 2 == 0 if in_stock: value[\"items_with_inventory\"].append(item) value[\"subtotal\"] += price * quantity else: value[\"out_of_stock_items\"].append(item) # Calculate shipping if \"shipping_rates\" in value: base_rate = value[\"shipping_rates\"].get(\"base_rate\", 5.0) value[\"shipping_cost\"] = base_rate # Calculate total value[\"total\"] = value[\"subtotal\"] + value[\"shipping_cost\"] return value pipelines: # First pipeline: Join orders with customer data join_orders_with_customers: from: orders join: globalTable: customers foreignKeyExtractor: extract_customer_id valueJoiner: join_order_with_customer to: orders_with_customers # Second pipeline: Join with shipping rates join_with_shipping: from: orders_with_customers join: globalTable: shipping_rates foreignKeyExtractor: extract_region_code valueJoiner: join_with_shipping_rates to: orders_with_shipping # Final pipeline: Process inventory and calculate totals finalize_orders: from: orders_with_shipping mapValues: check_inventory_and_calculate_total to: processed_orders This pipeline: 1. Takes incoming orders 2. Enriches them with customer data using a foreign key join 3. Adds shipping rates based on the customer's region 4. Checks inventory, calculates totals, and marks items as in-stock or out-of-stock 5. Outputs the fully processed order","title":"Practical Example: Order Processing System"},{"location":"tutorials/intermediate/joins/#best-practices-for-joins","text":"","title":"Best Practices for Joins"},{"location":"tutorials/intermediate/joins/#performance-considerations","text":"State Size : Joins maintain state, which consumes memory. Monitor state store sizes. Window Size : For windowed joins, smaller windows use less state but may miss matches. Join Order : Join with smaller datasets first when possible to reduce intermediate result sizes.","title":"Performance Considerations"},{"location":"tutorials/intermediate/joins/#design-patterns","text":"Pre-filtering : Filter unnecessary data before joins to reduce state size. Denormalization : Consider denormalizing data at write time for frequently joined data. Caching : Use GlobalKTables for reference data that needs to be joined with many streams.","title":"Design Patterns"},{"location":"tutorials/intermediate/joins/#error-handling","text":"Handle missing or invalid data in your joiner functions: functions: robust_joiner: type: valueJoiner code: | try: # Validate inputs if value1 is None: log.warn(\"Left side of join is null for key: {}\", key) return None if value2 is None: log.warn(\"Right side of join is null for key: {}\", key) # Still proceed with just the left data return value1 # Perform the join result = value1.copy() result[\"joined_data\"] = value2 return result except Exception as e: log.error(\"Error in join operation: {}\", str(e)) # Return left side data to avoid losing the record return value1","title":"Error Handling"},{"location":"tutorials/intermediate/joins/#conclusion","text":"Joins are essential operations for building sophisticated stream processing applications. KSML makes it easy to implement various types of joins while leveraging Python for the joining logic. By understanding the different join types and their appropriate use cases, you can build powerful data pipelines that combine and enrich data from multiple sources. In the next tutorial, we'll explore Using Windowed Operations to process data within time boundaries.","title":"Conclusion"},{"location":"tutorials/intermediate/joins/#further-reading","text":"Core Concepts: Operations Core Concepts: Streams and Data Types Reference: Join Operations","title":"Further Reading"},{"location":"tutorials/intermediate/windowed-operations/","text":"Using Windowed Operations in KSML This tutorial explores how to implement time-based windowing operations in KSML, allowing you to process data within specific time boundaries. Introduction to Windowed Operations Windowed operations are a powerful feature in stream processing that allow you to group and process data within specific time intervals. They're essential for: Time-based aggregations (hourly counts, daily averages, etc.) Detecting patterns over time Handling late-arriving data Limiting the scope of stateful operations In KSML, windowing operations are implemented using Kafka Streams' windowing capabilities, with the added flexibility of Python functions for processing the windowed data. Prerequisites Before starting this tutorial, you should: Understand basic KSML concepts (streams, functions, pipelines) Have completed the KSML Basics Tutorial Be familiar with Stateful Operations Have a basic understanding of Aggregations Types of Windows in KSML KSML supports several types of windows, each with different semantics: Tumbling Windows Tumbling windows divide the stream into fixed-size, non-overlapping time intervals. Each event belongs to exactly one window. windowByTime: size: 1h # 1-hour windows advanceBy: 1h # Move forward by the full window size Use tumbling windows when you need: - Clear boundaries between time periods - Non-overlapping aggregations (e.g., hourly statistics) - Minimal state storage requirements Hopping Windows Hopping windows are fixed-size windows that can overlap because they \"hop\" forward by a specified amount that's smaller than the window size. windowByTime: size: 1h # 1-hour windows advanceBy: 15m # Move forward by 15 minutes Use hopping windows when you need: - Overlapping time periods - Smoother transitions between windows - Moving averages or sliding metrics Sliding Windows Sliding windows in KSML are implemented using session windows with a very small inactivity gap: windowBySession: inactivityGap: 1ms # Minimal gap creates a sliding effect Use sliding windows when you need: - Event-driven windows that adapt to data arrival patterns - Windows that close after a period of inactivity Session Windows Session windows group events that occur close to each other in time, with windows closing after a specified period of inactivity. windowBySession: inactivityGap: 30m # Close the session after 30 minutes of inactivity Use session windows when you need: - Activity-based grouping (e.g., user sessions) - Dynamic window sizes based on event patterns - Windows that adapt to natural breaks in the data Working with Windowed Operations Basic Windowed Aggregation Here's a simple example that counts events in 5-minute tumbling windows: streams: user_clicks: topic: website_clicks keyType: string # User ID valueType: json # Click details click_counts: topic: user_click_counts keyType: string # User ID valueType: json # Count information with window details functions: initialize_count: type: initializer expression: 0 increment_count: type: aggregator expression: aggregatedValue + 1 pipelines: count_clicks: from: user_clicks groupByKey: windowByTime: size: 5m advanceBy: 5m aggregate: initializer: initialize_count aggregator: increment_count to: click_counts Handling Late Data with Grace Periods In real-world scenarios, data often arrives late. KSML allows you to specify a grace period during which late-arriving data will still be included in the window: windowByTime: size: 1h advanceBy: 1h grace: 10m # Accept data up to 10 minutes late Accessing Window Information When working with windowed operations, you might need to access information about the window itself. KSML provides this information through the window metadata: functions: aggregate_with_window_info: type: aggregator code: | # Initialize if needed if aggregatedValue is None: aggregatedValue = {\"count\": 0, \"window_start\": None, \"window_end\": None} # Update count aggregatedValue[\"count\"] += 1 # Access window information from metadata if \"window\" in metadata: aggregatedValue[\"window_start\"] = metadata[\"window\"][\"start\"] aggregatedValue[\"window_end\"] = metadata[\"window\"][\"end\"] return aggregatedValue Practical Example: Time-Based Analytics Let's build a complete example that implements a real-time analytics system using windowed operations: streams: page_views: topic: website_page_views keyType: string # Page URL valueType: json # View details including user_id, duration, etc. hourly_stats: topic: page_view_hourly_stats keyType: string # Page URL valueType: json # Hourly statistics daily_stats: topic: page_view_daily_stats keyType: string # Page URL valueType: json # Daily statistics functions: initialize_stats: type: initializer expression: {\"views\": 0, \"unique_users\": set(), \"total_duration\": 0, \"window_start\": None, \"window_end\": None} update_stats: type: aggregator code: | # Initialize if needed if aggregatedValue is None: aggregatedValue = {\"views\": 0, \"unique_users\": set(), \"total_duration\": 0, \"window_start\": None, \"window_end\": None} # Extract data user_id = value.get(\"user_id\", \"unknown\") duration = value.get(\"duration\", 0) # Update statistics aggregatedValue[\"views\"] += 1 aggregatedValue[\"unique_users\"].add(user_id) aggregatedValue[\"total_duration\"] += duration # Access window information if \"window\" in metadata: aggregatedValue[\"window_start\"] = metadata[\"window\"][\"start\"] aggregatedValue[\"window_end\"] = metadata[\"window\"][\"end\"] return aggregatedValue finalize_stats: type: valueTransformer code: | if value is None: return None # Convert set to count for serialization if \"unique_users\" in value and isinstance(value[\"unique_users\"], set): value[\"unique_visitors\"] = len(value[\"unique_users\"]) del value[\"unique_users\"] # Remove the set which can't be serialized # Calculate averages if value[\"views\"] > 0: value[\"avg_duration\"] = value[\"total_duration\"] / value[\"views\"] else: value[\"avg_duration\"] = 0 # Add timestamp for easier querying if \"window_start\" in value: value[\"timestamp\"] = value[\"window_start\"] return value pipelines: # Hourly statistics pipeline hourly_analytics: from: page_views groupByKey: windowByTime: size: 1h advanceBy: 1h grace: 10m aggregate: initializer: initialize_stats aggregator: update_stats mapValues: finalize_stats to: hourly_stats # Daily statistics pipeline daily_analytics: from: page_views groupByKey: windowByTime: size: 1d advanceBy: 1d grace: 2h aggregate: initializer: initialize_stats aggregator: update_stats mapValues: finalize_stats to: daily_stats This example: 1. Processes page view events 2. Creates both hourly and daily windows 3. Calculates statistics including view counts, unique visitors, and average duration 4. Handles the conversion of non-serializable data (sets) before output 5. Includes window timestamp information for easier querying Advanced Windowing Patterns Multi-Level Windowing For complex analytics, you might want to implement multi-level windowing, where data is aggregated at different time granularities: pipelines: # Minute-level aggregation minute_aggregation: from: raw_events groupByKey: windowByTime: size: 1m advanceBy: 1m aggregate: initializer: initialize_minute_stats aggregator: update_minute_stats to: minute_stats # Hour-level aggregation from minute stats hour_aggregation: from: minute_stats groupByKey: windowByTime: size: 1h advanceBy: 1h aggregate: initializer: initialize_hour_stats aggregator: update_hour_stats to: hour_stats Combining Windows with Joins Windowed operations can be combined with joins to correlate events from different streams within time boundaries: pipelines: correlate_events: from: stream_a join: stream: stream_b valueJoiner: combine_events windows: type: time size: 5m grace: 1m to: correlated_events Best Practices for Windowed Operations Performance Considerations Window Size : Larger windows require more state storage. Choose appropriate window sizes. Grace Period : Longer grace periods increase state storage requirements but improve handling of late data. Serialization : Be careful with complex objects in windowed aggregations, as they need to be serialized. Design Patterns Pre-Aggregation : For high-volume streams, consider pre-aggregating at a finer granularity before wider windows. Downsampling : For time series data, consider downsampling before windowing to reduce state size. Window Alignment : For easier analysis, align windows to natural time boundaries (start of hour, day, etc.). Error Handling Always handle potential errors in your windowed aggregation functions: functions: safe_windowed_aggregator: type: aggregator code: | try: # Your windowed aggregation logic here return result except Exception as e: log.error(\"Error in windowed aggregation: {}\", str(e)) # Return previous state to avoid losing data return aggregatedValue Conclusion Windowed operations are essential for time-based analytics and processing in streaming applications. KSML makes it easy to implement various types of windows while leveraging Python for the processing logic. By understanding the different window types and their appropriate use cases, you can build powerful data pipelines that process data within meaningful time boundaries. In the next tutorial, we'll explore Error Handling and Recovery to build more robust KSML applications. Further Reading Core Concepts: Operations Core Concepts: Functions Reference: Windowing Operations","title":"Using Windowed Operations in KSML"},{"location":"tutorials/intermediate/windowed-operations/#using-windowed-operations-in-ksml","text":"This tutorial explores how to implement time-based windowing operations in KSML, allowing you to process data within specific time boundaries.","title":"Using Windowed Operations in KSML"},{"location":"tutorials/intermediate/windowed-operations/#introduction-to-windowed-operations","text":"Windowed operations are a powerful feature in stream processing that allow you to group and process data within specific time intervals. They're essential for: Time-based aggregations (hourly counts, daily averages, etc.) Detecting patterns over time Handling late-arriving data Limiting the scope of stateful operations In KSML, windowing operations are implemented using Kafka Streams' windowing capabilities, with the added flexibility of Python functions for processing the windowed data.","title":"Introduction to Windowed Operations"},{"location":"tutorials/intermediate/windowed-operations/#prerequisites","text":"Before starting this tutorial, you should: Understand basic KSML concepts (streams, functions, pipelines) Have completed the KSML Basics Tutorial Be familiar with Stateful Operations Have a basic understanding of Aggregations","title":"Prerequisites"},{"location":"tutorials/intermediate/windowed-operations/#types-of-windows-in-ksml","text":"KSML supports several types of windows, each with different semantics:","title":"Types of Windows in KSML"},{"location":"tutorials/intermediate/windowed-operations/#tumbling-windows","text":"Tumbling windows divide the stream into fixed-size, non-overlapping time intervals. Each event belongs to exactly one window. windowByTime: size: 1h # 1-hour windows advanceBy: 1h # Move forward by the full window size Use tumbling windows when you need: - Clear boundaries between time periods - Non-overlapping aggregations (e.g., hourly statistics) - Minimal state storage requirements","title":"Tumbling Windows"},{"location":"tutorials/intermediate/windowed-operations/#hopping-windows","text":"Hopping windows are fixed-size windows that can overlap because they \"hop\" forward by a specified amount that's smaller than the window size. windowByTime: size: 1h # 1-hour windows advanceBy: 15m # Move forward by 15 minutes Use hopping windows when you need: - Overlapping time periods - Smoother transitions between windows - Moving averages or sliding metrics","title":"Hopping Windows"},{"location":"tutorials/intermediate/windowed-operations/#sliding-windows","text":"Sliding windows in KSML are implemented using session windows with a very small inactivity gap: windowBySession: inactivityGap: 1ms # Minimal gap creates a sliding effect Use sliding windows when you need: - Event-driven windows that adapt to data arrival patterns - Windows that close after a period of inactivity","title":"Sliding Windows"},{"location":"tutorials/intermediate/windowed-operations/#session-windows","text":"Session windows group events that occur close to each other in time, with windows closing after a specified period of inactivity. windowBySession: inactivityGap: 30m # Close the session after 30 minutes of inactivity Use session windows when you need: - Activity-based grouping (e.g., user sessions) - Dynamic window sizes based on event patterns - Windows that adapt to natural breaks in the data","title":"Session Windows"},{"location":"tutorials/intermediate/windowed-operations/#working-with-windowed-operations","text":"","title":"Working with Windowed Operations"},{"location":"tutorials/intermediate/windowed-operations/#basic-windowed-aggregation","text":"Here's a simple example that counts events in 5-minute tumbling windows: streams: user_clicks: topic: website_clicks keyType: string # User ID valueType: json # Click details click_counts: topic: user_click_counts keyType: string # User ID valueType: json # Count information with window details functions: initialize_count: type: initializer expression: 0 increment_count: type: aggregator expression: aggregatedValue + 1 pipelines: count_clicks: from: user_clicks groupByKey: windowByTime: size: 5m advanceBy: 5m aggregate: initializer: initialize_count aggregator: increment_count to: click_counts","title":"Basic Windowed Aggregation"},{"location":"tutorials/intermediate/windowed-operations/#handling-late-data-with-grace-periods","text":"In real-world scenarios, data often arrives late. KSML allows you to specify a grace period during which late-arriving data will still be included in the window: windowByTime: size: 1h advanceBy: 1h grace: 10m # Accept data up to 10 minutes late","title":"Handling Late Data with Grace Periods"},{"location":"tutorials/intermediate/windowed-operations/#accessing-window-information","text":"When working with windowed operations, you might need to access information about the window itself. KSML provides this information through the window metadata: functions: aggregate_with_window_info: type: aggregator code: | # Initialize if needed if aggregatedValue is None: aggregatedValue = {\"count\": 0, \"window_start\": None, \"window_end\": None} # Update count aggregatedValue[\"count\"] += 1 # Access window information from metadata if \"window\" in metadata: aggregatedValue[\"window_start\"] = metadata[\"window\"][\"start\"] aggregatedValue[\"window_end\"] = metadata[\"window\"][\"end\"] return aggregatedValue","title":"Accessing Window Information"},{"location":"tutorials/intermediate/windowed-operations/#practical-example-time-based-analytics","text":"Let's build a complete example that implements a real-time analytics system using windowed operations: streams: page_views: topic: website_page_views keyType: string # Page URL valueType: json # View details including user_id, duration, etc. hourly_stats: topic: page_view_hourly_stats keyType: string # Page URL valueType: json # Hourly statistics daily_stats: topic: page_view_daily_stats keyType: string # Page URL valueType: json # Daily statistics functions: initialize_stats: type: initializer expression: {\"views\": 0, \"unique_users\": set(), \"total_duration\": 0, \"window_start\": None, \"window_end\": None} update_stats: type: aggregator code: | # Initialize if needed if aggregatedValue is None: aggregatedValue = {\"views\": 0, \"unique_users\": set(), \"total_duration\": 0, \"window_start\": None, \"window_end\": None} # Extract data user_id = value.get(\"user_id\", \"unknown\") duration = value.get(\"duration\", 0) # Update statistics aggregatedValue[\"views\"] += 1 aggregatedValue[\"unique_users\"].add(user_id) aggregatedValue[\"total_duration\"] += duration # Access window information if \"window\" in metadata: aggregatedValue[\"window_start\"] = metadata[\"window\"][\"start\"] aggregatedValue[\"window_end\"] = metadata[\"window\"][\"end\"] return aggregatedValue finalize_stats: type: valueTransformer code: | if value is None: return None # Convert set to count for serialization if \"unique_users\" in value and isinstance(value[\"unique_users\"], set): value[\"unique_visitors\"] = len(value[\"unique_users\"]) del value[\"unique_users\"] # Remove the set which can't be serialized # Calculate averages if value[\"views\"] > 0: value[\"avg_duration\"] = value[\"total_duration\"] / value[\"views\"] else: value[\"avg_duration\"] = 0 # Add timestamp for easier querying if \"window_start\" in value: value[\"timestamp\"] = value[\"window_start\"] return value pipelines: # Hourly statistics pipeline hourly_analytics: from: page_views groupByKey: windowByTime: size: 1h advanceBy: 1h grace: 10m aggregate: initializer: initialize_stats aggregator: update_stats mapValues: finalize_stats to: hourly_stats # Daily statistics pipeline daily_analytics: from: page_views groupByKey: windowByTime: size: 1d advanceBy: 1d grace: 2h aggregate: initializer: initialize_stats aggregator: update_stats mapValues: finalize_stats to: daily_stats This example: 1. Processes page view events 2. Creates both hourly and daily windows 3. Calculates statistics including view counts, unique visitors, and average duration 4. Handles the conversion of non-serializable data (sets) before output 5. Includes window timestamp information for easier querying","title":"Practical Example: Time-Based Analytics"},{"location":"tutorials/intermediate/windowed-operations/#advanced-windowing-patterns","text":"","title":"Advanced Windowing Patterns"},{"location":"tutorials/intermediate/windowed-operations/#multi-level-windowing","text":"For complex analytics, you might want to implement multi-level windowing, where data is aggregated at different time granularities: pipelines: # Minute-level aggregation minute_aggregation: from: raw_events groupByKey: windowByTime: size: 1m advanceBy: 1m aggregate: initializer: initialize_minute_stats aggregator: update_minute_stats to: minute_stats # Hour-level aggregation from minute stats hour_aggregation: from: minute_stats groupByKey: windowByTime: size: 1h advanceBy: 1h aggregate: initializer: initialize_hour_stats aggregator: update_hour_stats to: hour_stats","title":"Multi-Level Windowing"},{"location":"tutorials/intermediate/windowed-operations/#combining-windows-with-joins","text":"Windowed operations can be combined with joins to correlate events from different streams within time boundaries: pipelines: correlate_events: from: stream_a join: stream: stream_b valueJoiner: combine_events windows: type: time size: 5m grace: 1m to: correlated_events","title":"Combining Windows with Joins"},{"location":"tutorials/intermediate/windowed-operations/#best-practices-for-windowed-operations","text":"","title":"Best Practices for Windowed Operations"},{"location":"tutorials/intermediate/windowed-operations/#performance-considerations","text":"Window Size : Larger windows require more state storage. Choose appropriate window sizes. Grace Period : Longer grace periods increase state storage requirements but improve handling of late data. Serialization : Be careful with complex objects in windowed aggregations, as they need to be serialized.","title":"Performance Considerations"},{"location":"tutorials/intermediate/windowed-operations/#design-patterns","text":"Pre-Aggregation : For high-volume streams, consider pre-aggregating at a finer granularity before wider windows. Downsampling : For time series data, consider downsampling before windowing to reduce state size. Window Alignment : For easier analysis, align windows to natural time boundaries (start of hour, day, etc.).","title":"Design Patterns"},{"location":"tutorials/intermediate/windowed-operations/#error-handling","text":"Always handle potential errors in your windowed aggregation functions: functions: safe_windowed_aggregator: type: aggregator code: | try: # Your windowed aggregation logic here return result except Exception as e: log.error(\"Error in windowed aggregation: {}\", str(e)) # Return previous state to avoid losing data return aggregatedValue","title":"Error Handling"},{"location":"tutorials/intermediate/windowed-operations/#conclusion","text":"Windowed operations are essential for time-based analytics and processing in streaming applications. KSML makes it easy to implement various types of windows while leveraging Python for the processing logic. By understanding the different window types and their appropriate use cases, you can build powerful data pipelines that process data within meaningful time boundaries. In the next tutorial, we'll explore Error Handling and Recovery to build more robust KSML applications.","title":"Conclusion"},{"location":"tutorials/intermediate/windowed-operations/#further-reading","text":"Core Concepts: Operations Core Concepts: Functions Reference: Windowing Operations","title":"Further Reading"},{"location":"use-cases/","text":"Use Case Guides Welcome to the KSML Use Case Guides! These guides demonstrate how to apply KSML to solve real-world business problems and implement common stream processing patterns. Unlike the tutorials that focus on specific KSML features, these guides take a problem-first approach, showing you how to combine various KSML capabilities to address practical use cases. Available Use Case Guides Real-time Analytics Learn how to implement real-time analytics solutions with KSML: Building real-time dashboards Calculating key performance indicators Implementing sliding window analytics Detecting anomalies in streaming data Data Transformation This guide covers common data transformation patterns: Format conversion (JSON to Avro, CSV to JSON, etc.) Data normalization and cleansing Schema evolution handling Complex data transformations Event-Driven Applications Learn how to build event-driven applications with KSML: Implementing the event sourcing pattern Building event-driven microservices Command-query responsibility segregation (CQRS) Event notification systems Microservices Integration This guide focuses on using KSML to integrate microservices: Service-to-service communication Event-based integration patterns Implementing saga patterns Building resilient microservice architectures IoT Data Processing Learn how to process IoT data streams with KSML: Handling high-volume sensor data Device state tracking Geospatial data processing Edge-to-cloud data pipelines Fraud Detection This guide demonstrates how to implement fraud detection systems: Pattern recognition in transaction streams Real-time risk scoring Multi-factor anomaly detection Alert generation and notification How to Use These Guides Each guide includes: Problem Statement : A clear description of the business problem or use case Solution Architecture : An overview of the KSML solution design Implementation : Step-by-step instructions with KSML code examples Testing and Validation : How to test and validate the solution Production Considerations : Tips for deploying to production environments You can follow these guides end-to-end to implement the complete solution, or adapt specific patterns to your own use cases. Additional Resources Examples Library - Ready-to-use examples for common patterns Core Concepts - Detailed explanations of KSML components Reference Documentation - Complete reference for all KSML operations Community and Support - Connect with other KSML users and get help","title":"Use Case Guides"},{"location":"use-cases/#use-case-guides","text":"Welcome to the KSML Use Case Guides! These guides demonstrate how to apply KSML to solve real-world business problems and implement common stream processing patterns. Unlike the tutorials that focus on specific KSML features, these guides take a problem-first approach, showing you how to combine various KSML capabilities to address practical use cases.","title":"Use Case Guides"},{"location":"use-cases/#available-use-case-guides","text":"","title":"Available Use Case Guides"},{"location":"use-cases/#real-time-analytics","text":"Learn how to implement real-time analytics solutions with KSML: Building real-time dashboards Calculating key performance indicators Implementing sliding window analytics Detecting anomalies in streaming data","title":"Real-time Analytics"},{"location":"use-cases/#data-transformation","text":"This guide covers common data transformation patterns: Format conversion (JSON to Avro, CSV to JSON, etc.) Data normalization and cleansing Schema evolution handling Complex data transformations","title":"Data Transformation"},{"location":"use-cases/#event-driven-applications","text":"Learn how to build event-driven applications with KSML: Implementing the event sourcing pattern Building event-driven microservices Command-query responsibility segregation (CQRS) Event notification systems","title":"Event-Driven Applications"},{"location":"use-cases/#microservices-integration","text":"This guide focuses on using KSML to integrate microservices: Service-to-service communication Event-based integration patterns Implementing saga patterns Building resilient microservice architectures","title":"Microservices Integration"},{"location":"use-cases/#iot-data-processing","text":"Learn how to process IoT data streams with KSML: Handling high-volume sensor data Device state tracking Geospatial data processing Edge-to-cloud data pipelines","title":"IoT Data Processing"},{"location":"use-cases/#fraud-detection","text":"This guide demonstrates how to implement fraud detection systems: Pattern recognition in transaction streams Real-time risk scoring Multi-factor anomaly detection Alert generation and notification","title":"Fraud Detection"},{"location":"use-cases/#how-to-use-these-guides","text":"Each guide includes: Problem Statement : A clear description of the business problem or use case Solution Architecture : An overview of the KSML solution design Implementation : Step-by-step instructions with KSML code examples Testing and Validation : How to test and validate the solution Production Considerations : Tips for deploying to production environments You can follow these guides end-to-end to implement the complete solution, or adapt specific patterns to your own use cases.","title":"How to Use These Guides"},{"location":"use-cases/#additional-resources","text":"Examples Library - Ready-to-use examples for common patterns Core Concepts - Detailed explanations of KSML components Reference Documentation - Complete reference for all KSML operations Community and Support - Connect with other KSML users and get help","title":"Additional Resources"},{"location":"use-cases/data-transformation/","text":"Data Transformation with KSML This tutorial demonstrates how to build a data transformation pipeline using KSML. You'll learn how to convert data between different formats, enrich data with additional information, and handle complex transformations. Introduction Data transformation is a fundamental use case for stream processing. It allows you to: Convert data between different formats (JSON, XML, Avro, etc.) Normalize and clean data from various sources Enrich data with additional context or reference information Restructure data to meet downstream system requirements Filter out unnecessary information In this tutorial, we'll build a data transformation pipeline that processes customer data from a legacy system, transforms it into a standardized format, enriches it with additional information, and makes it available for downstream applications. Prerequisites Before starting this tutorial, you should: Understand basic KSML concepts (streams, functions, pipelines) Have completed the KSML Basics Tutorial Be familiar with Filtering and Transforming Have a basic understanding of Joins for data enrichment The Use Case Imagine you're working with a company that has acquired another business. You need to integrate customer data from the acquired company's legacy system into your modern data platform. The legacy data: Is in a different format (XML) than your system (JSON) Uses different field names and conventions Contains some fields you don't need Is missing some information that you need to add from reference data Defining the Data Models Source Data (XML) The legacy system provides customer data in XML format: <customer> <cust_id>12345</cust_id> <fname>John</fname> <lname>Doe</lname> <dob>1980-01-15</dob> <addr> <street>123 Main St</street> <city>Anytown</city> <state>CA</state> <zip>90210</zip> </addr> <phone>555-123-4567</phone> <legacy_segment>A</legacy_segment> <account_created>2015-03-20</account_created> </customer> Reference Data (JSON) You have a reference table topic with segment code (key) mappings to segment details (value): Key Value A {\"segment_name\": \"Premium\", \"discount_tier\": \"Tier 1\", \"marketing_group\": \"High Value\"} B {\"segment_name\": \"Standard\", \"discount_tier\": \"Tier 2\", \"marketing_group\": \"Medium Value\"} C {\"segment_name\": \"Basic\", \"discount_tier\": \"Tier 3\", \"marketing_group\": \"Growth Target\" } Target Data (JSON) You want to transform the data into this format: { \"customer_id\": \"12345\", \"name\": { \"first\": \"John\", \"last\": \"Doe\" }, \"contact_info\": { \"email\": \"john.doe@example.com\", \"phone\": \"555-123-4567\", \"address\": { \"street\": \"123 Main St\", \"city\": \"Anytown\", \"state\": \"CA\", \"postal_code\": \"90210\", \"country\": \"USA\" } }, \"birth_date\": \"1980-01-15\", \"customer_since\": \"2015-03-20\", \"segment\": \"Premium\", \"marketing_preferences\": { \"group\": \"High Value\", \"discount_tier\": \"Tier 1\" }, \"metadata\": { \"source\": \"legacy_system\", \"last_updated\": \"2023-07-01T12:00:00Z\" } } Creating the KSML Definition Now, let's create our KSML definition file: streams: legacy_customers: topic: legacy_customer_data keyType: string # customer_id valueType: xml # XML customer data transformed_customers: topic: standardized_customer_data keyType: string # customer_id valueType: json # transformed customer data tables: segment_reference: topic: customer_segments keyType: string # segment code valueType: json # segment details functions: transform_customer: type: mapValues code: | import datetime # Extract values from XML customer_id = value.get(\"cust_id\") first_name = value.get(\"fname\") last_name = value.get(\"lname\") birth_date = value.get(\"dob\") phone = value.get(\"phone\") legacy_segment = value.get(\"legacy_segment\") customer_since = value.get(\"account_created\") # Extract address address = value.get(\"addr\") street = address.get(\"street\") city = address.get(\"city\") state = address.get(\"state\") zip_code = address.get(\"zip\") # Generate email (not in source data) email = f\"{first_name.lower()}.{last_name.lower()}@example.com\" # Get segment info from reference data segment_info = value.get(\"segment_info\") segment_name = segment_info.get(\"segment_name\") if segment_info else \"Unknown\" discount_tier = segment_info.get(\"discount_tier\") if segment_info else \"Unknown\" marketing_group = segment_info.get(\"marketing_group\") if segment_info else \"Unknown\" # Current timestamp for metadata current_time = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\") # Create transformed customer object return { \"customer_id\": customer_id, \"name\": { \"first\": first_name, \"last\": last_name }, \"contact_info\": { \"email\": email, \"phone\": phone, \"address\": { \"street\": street, \"city\": city, \"state\": state, \"postal_code\": zip_code, \"country\": \"USA\" # Default value not in source } }, \"birth_date\": birth_date, \"customer_since\": customer_since, \"segment\": segment_name, \"marketing_preferences\": { \"group\": marketing_group, \"discount_tier\": discount_tier }, \"metadata\": { \"source\": \"legacy_system\", \"last_updated\": current_time } } resultType: json pipelines: customer_transformation_pipeline: from: legacy_customers via: - type: peek forEach: code: | log.info(\"Processing customer: {}\", key) - type: leftJoin foreignKeyExtractor: code: value.get(\"legacy_segment\") table: segment_reference valueJoiner: expression: | { **value1, \"segment_info\": value2 } - type: transformValue mapper: transform_customer - type: peek forEach: code: | log.info(\"Transformed customer: {}\", key) to: transformed_customers Setting up two producers for test data To test out the topology above, we create two test data producers. The first producer is a single shot producer that generates data for the customer_segments topic: tables: customer_segments: topic: customer_segments keyType: string # segment code valueType: json # segment details functions: customer_segment_generator: globalCode: | import random count = 0 code: | refs = { 0: {\"key\": \"A\", \"value\": {\"segment_name\": \"Premium\", \"discount_tier\": \"Tier 1\", \"marketing_group\": \"High Value\"}}, 1: {\"key\": \"B\", \"value\": {\"segment_name\": \"Standard\", \"discount_tier\": \"Tier 2\", \"marketing_group\": \"Medium Value\"}}, 2: {\"key\": \"C\", \"value\": {\"segment_name\": \"Basic\", \"discount_tier\": \"Tier 3\", \"marketing_group\": \"Growth Target\" }} } key = refs.get(count)[\"key\"] value = refs.get(count)[\"value\"] count = (count + 1) % 3 return (key, value) resultType: (string, struct) producers: customer_segment_producer: generator: customer_segment_generator to: customer_segments interval: 1 count: 3 The second producer produces a message every second to the legacy_customer_data topic, using a randomly chosen segment: streams: legacy_customer_data: topic: legacy_customer_data keyType: string # customer_id valueType: xml # XML customer data functions: legacy_customer_data_generator: globalCode: | import random code: | key = \"A\" value = { \"cust_id\": \"12345\", \"fname\": \"John\", \"lname\": \"Doe\", \"dob\": \"1980-01-15\", \"addr\": { \"street\": \"123 Main St\", \"city\": \"Anytown\", \"state\": \"CA\", \"zip\": \"90210\" }, \"phone\": \"555-123-4567\", \"legacy_segment\": random.choice([\"A\",\"B\",\"C\"]), \"account_created\": \"2015-03-20\" } return (key, value) resultType: (string, struct) producers: legacy_customer_data_producer: generator: legacy_customer_data_generator to: legacy_customer_data interval: 1s Running the Application To run the application: Save the KSML definition to data_transformation.yaml . Save the producers to customer_segment_producer.yaml and legacy_customer_data_producer.yaml . Set up your ksml-runner.yaml configuration, pointing to your Kafka installation. Start the customer_segment_producer to produce the sample segment information to Kafka. Start the legacy_customer_data_producer to produce some sample data to the input topic. Start the data_transformation topology to initiate the continuous data transformation logic. Monitor the output topic to see the transformed data. Advanced Transformation Techniques Handling Missing or Invalid Data In real-world scenarios, source data often has missing or invalid fields. You can enhance the transformation function to handle these cases: # Check if a field exists before accessing it birth_date = value.get(\"dob\") if value.get(\"dob\") is not None else \"Unknown\" # Provide default values state = address.get(\"state\", \"N/A\") # Validate data if phone and not phone.startswith(\"555-\"): log.warn(\"Invalid phone format for customer {}: {}\", customer_id, phone) phone = \"Unknown\" Schema Evolution Handling To handle changes in the source or target schema over time: # Version-aware transformation schema_version = value.get(\"version\", \"1.0\") if schema_version == \"1.0\": # Original transformation logic elif schema_version == \"2.0\": # Updated transformation logic for new schema else: log.error(\"Unknown schema version: {}\", schema_version) Conclusion In this tutorial, you've learned how to: Transform data between different formats (XML to JSON) Restructure data to match a target schema Enrich data with information from reference sources Handle missing or derived fields Process and validate data during transformation Data transformation is a powerful use case for KSML, allowing you to integrate data from various sources and prepare it for downstream applications without complex coding. Next Steps Learn about Real-Time Analytics to analyze your transformed data Explore Event-Driven Applications to trigger actions based on data changes Check out External Integration for connecting to external systems","title":"Data Transformation with KSML"},{"location":"use-cases/data-transformation/#data-transformation-with-ksml","text":"This tutorial demonstrates how to build a data transformation pipeline using KSML. You'll learn how to convert data between different formats, enrich data with additional information, and handle complex transformations.","title":"Data Transformation with KSML"},{"location":"use-cases/data-transformation/#introduction","text":"Data transformation is a fundamental use case for stream processing. It allows you to: Convert data between different formats (JSON, XML, Avro, etc.) Normalize and clean data from various sources Enrich data with additional context or reference information Restructure data to meet downstream system requirements Filter out unnecessary information In this tutorial, we'll build a data transformation pipeline that processes customer data from a legacy system, transforms it into a standardized format, enriches it with additional information, and makes it available for downstream applications.","title":"Introduction"},{"location":"use-cases/data-transformation/#prerequisites","text":"Before starting this tutorial, you should: Understand basic KSML concepts (streams, functions, pipelines) Have completed the KSML Basics Tutorial Be familiar with Filtering and Transforming Have a basic understanding of Joins for data enrichment","title":"Prerequisites"},{"location":"use-cases/data-transformation/#the-use-case","text":"Imagine you're working with a company that has acquired another business. You need to integrate customer data from the acquired company's legacy system into your modern data platform. The legacy data: Is in a different format (XML) than your system (JSON) Uses different field names and conventions Contains some fields you don't need Is missing some information that you need to add from reference data","title":"The Use Case"},{"location":"use-cases/data-transformation/#defining-the-data-models","text":"","title":"Defining the Data Models"},{"location":"use-cases/data-transformation/#source-data-xml","text":"The legacy system provides customer data in XML format: <customer> <cust_id>12345</cust_id> <fname>John</fname> <lname>Doe</lname> <dob>1980-01-15</dob> <addr> <street>123 Main St</street> <city>Anytown</city> <state>CA</state> <zip>90210</zip> </addr> <phone>555-123-4567</phone> <legacy_segment>A</legacy_segment> <account_created>2015-03-20</account_created> </customer>","title":"Source Data (XML)"},{"location":"use-cases/data-transformation/#reference-data-json","text":"You have a reference table topic with segment code (key) mappings to segment details (value): Key Value A {\"segment_name\": \"Premium\", \"discount_tier\": \"Tier 1\", \"marketing_group\": \"High Value\"} B {\"segment_name\": \"Standard\", \"discount_tier\": \"Tier 2\", \"marketing_group\": \"Medium Value\"} C {\"segment_name\": \"Basic\", \"discount_tier\": \"Tier 3\", \"marketing_group\": \"Growth Target\" }","title":"Reference Data (JSON)"},{"location":"use-cases/data-transformation/#target-data-json","text":"You want to transform the data into this format: { \"customer_id\": \"12345\", \"name\": { \"first\": \"John\", \"last\": \"Doe\" }, \"contact_info\": { \"email\": \"john.doe@example.com\", \"phone\": \"555-123-4567\", \"address\": { \"street\": \"123 Main St\", \"city\": \"Anytown\", \"state\": \"CA\", \"postal_code\": \"90210\", \"country\": \"USA\" } }, \"birth_date\": \"1980-01-15\", \"customer_since\": \"2015-03-20\", \"segment\": \"Premium\", \"marketing_preferences\": { \"group\": \"High Value\", \"discount_tier\": \"Tier 1\" }, \"metadata\": { \"source\": \"legacy_system\", \"last_updated\": \"2023-07-01T12:00:00Z\" } }","title":"Target Data (JSON)"},{"location":"use-cases/data-transformation/#creating-the-ksml-definition","text":"Now, let's create our KSML definition file: streams: legacy_customers: topic: legacy_customer_data keyType: string # customer_id valueType: xml # XML customer data transformed_customers: topic: standardized_customer_data keyType: string # customer_id valueType: json # transformed customer data tables: segment_reference: topic: customer_segments keyType: string # segment code valueType: json # segment details functions: transform_customer: type: mapValues code: | import datetime # Extract values from XML customer_id = value.get(\"cust_id\") first_name = value.get(\"fname\") last_name = value.get(\"lname\") birth_date = value.get(\"dob\") phone = value.get(\"phone\") legacy_segment = value.get(\"legacy_segment\") customer_since = value.get(\"account_created\") # Extract address address = value.get(\"addr\") street = address.get(\"street\") city = address.get(\"city\") state = address.get(\"state\") zip_code = address.get(\"zip\") # Generate email (not in source data) email = f\"{first_name.lower()}.{last_name.lower()}@example.com\" # Get segment info from reference data segment_info = value.get(\"segment_info\") segment_name = segment_info.get(\"segment_name\") if segment_info else \"Unknown\" discount_tier = segment_info.get(\"discount_tier\") if segment_info else \"Unknown\" marketing_group = segment_info.get(\"marketing_group\") if segment_info else \"Unknown\" # Current timestamp for metadata current_time = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\") # Create transformed customer object return { \"customer_id\": customer_id, \"name\": { \"first\": first_name, \"last\": last_name }, \"contact_info\": { \"email\": email, \"phone\": phone, \"address\": { \"street\": street, \"city\": city, \"state\": state, \"postal_code\": zip_code, \"country\": \"USA\" # Default value not in source } }, \"birth_date\": birth_date, \"customer_since\": customer_since, \"segment\": segment_name, \"marketing_preferences\": { \"group\": marketing_group, \"discount_tier\": discount_tier }, \"metadata\": { \"source\": \"legacy_system\", \"last_updated\": current_time } } resultType: json pipelines: customer_transformation_pipeline: from: legacy_customers via: - type: peek forEach: code: | log.info(\"Processing customer: {}\", key) - type: leftJoin foreignKeyExtractor: code: value.get(\"legacy_segment\") table: segment_reference valueJoiner: expression: | { **value1, \"segment_info\": value2 } - type: transformValue mapper: transform_customer - type: peek forEach: code: | log.info(\"Transformed customer: {}\", key) to: transformed_customers","title":"Creating the KSML Definition"},{"location":"use-cases/data-transformation/#setting-up-two-producers-for-test-data","text":"To test out the topology above, we create two test data producers. The first producer is a single shot producer that generates data for the customer_segments topic: tables: customer_segments: topic: customer_segments keyType: string # segment code valueType: json # segment details functions: customer_segment_generator: globalCode: | import random count = 0 code: | refs = { 0: {\"key\": \"A\", \"value\": {\"segment_name\": \"Premium\", \"discount_tier\": \"Tier 1\", \"marketing_group\": \"High Value\"}}, 1: {\"key\": \"B\", \"value\": {\"segment_name\": \"Standard\", \"discount_tier\": \"Tier 2\", \"marketing_group\": \"Medium Value\"}}, 2: {\"key\": \"C\", \"value\": {\"segment_name\": \"Basic\", \"discount_tier\": \"Tier 3\", \"marketing_group\": \"Growth Target\" }} } key = refs.get(count)[\"key\"] value = refs.get(count)[\"value\"] count = (count + 1) % 3 return (key, value) resultType: (string, struct) producers: customer_segment_producer: generator: customer_segment_generator to: customer_segments interval: 1 count: 3 The second producer produces a message every second to the legacy_customer_data topic, using a randomly chosen segment: streams: legacy_customer_data: topic: legacy_customer_data keyType: string # customer_id valueType: xml # XML customer data functions: legacy_customer_data_generator: globalCode: | import random code: | key = \"A\" value = { \"cust_id\": \"12345\", \"fname\": \"John\", \"lname\": \"Doe\", \"dob\": \"1980-01-15\", \"addr\": { \"street\": \"123 Main St\", \"city\": \"Anytown\", \"state\": \"CA\", \"zip\": \"90210\" }, \"phone\": \"555-123-4567\", \"legacy_segment\": random.choice([\"A\",\"B\",\"C\"]), \"account_created\": \"2015-03-20\" } return (key, value) resultType: (string, struct) producers: legacy_customer_data_producer: generator: legacy_customer_data_generator to: legacy_customer_data interval: 1s","title":"Setting up two producers for test data"},{"location":"use-cases/data-transformation/#running-the-application","text":"To run the application: Save the KSML definition to data_transformation.yaml . Save the producers to customer_segment_producer.yaml and legacy_customer_data_producer.yaml . Set up your ksml-runner.yaml configuration, pointing to your Kafka installation. Start the customer_segment_producer to produce the sample segment information to Kafka. Start the legacy_customer_data_producer to produce some sample data to the input topic. Start the data_transformation topology to initiate the continuous data transformation logic. Monitor the output topic to see the transformed data.","title":"Running the Application"},{"location":"use-cases/data-transformation/#advanced-transformation-techniques","text":"","title":"Advanced Transformation Techniques"},{"location":"use-cases/data-transformation/#handling-missing-or-invalid-data","text":"In real-world scenarios, source data often has missing or invalid fields. You can enhance the transformation function to handle these cases: # Check if a field exists before accessing it birth_date = value.get(\"dob\") if value.get(\"dob\") is not None else \"Unknown\" # Provide default values state = address.get(\"state\", \"N/A\") # Validate data if phone and not phone.startswith(\"555-\"): log.warn(\"Invalid phone format for customer {}: {}\", customer_id, phone) phone = \"Unknown\"","title":"Handling Missing or Invalid Data"},{"location":"use-cases/data-transformation/#schema-evolution-handling","text":"To handle changes in the source or target schema over time: # Version-aware transformation schema_version = value.get(\"version\", \"1.0\") if schema_version == \"1.0\": # Original transformation logic elif schema_version == \"2.0\": # Updated transformation logic for new schema else: log.error(\"Unknown schema version: {}\", schema_version)","title":"Schema Evolution Handling"},{"location":"use-cases/data-transformation/#conclusion","text":"In this tutorial, you've learned how to: Transform data between different formats (XML to JSON) Restructure data to match a target schema Enrich data with information from reference sources Handle missing or derived fields Process and validate data during transformation Data transformation is a powerful use case for KSML, allowing you to integrate data from various sources and prepare it for downstream applications without complex coding.","title":"Conclusion"},{"location":"use-cases/data-transformation/#next-steps","text":"Learn about Real-Time Analytics to analyze your transformed data Explore Event-Driven Applications to trigger actions based on data changes Check out External Integration for connecting to external systems","title":"Next Steps"},{"location":"use-cases/event-driven-applications/","text":"Event-Driven Applications with KSML This tutorial demonstrates how to build event-driven applications using KSML. You'll learn how to detect specific events in your data streams and trigger appropriate actions in response. Introduction Event-driven architecture is a powerful paradigm for building responsive, real-time applications. In this approach: Systems react to events as they occur Components communicate through events rather than direct calls Business logic is triggered by changes in state Applications can scale and evolve independently KSML is particularly well-suited for event-driven applications because it allows you to: Process streams of events in real-time Detect complex patterns and conditions Transform events into actionable insights Trigger downstream processes automatically Prerequisites Before starting this tutorial, you should: Understand basic KSML concepts (streams, functions, pipelines) Have completed the KSML Basics Tutorial Be familiar with Filtering and Transforming Have a basic understanding of Complex Event Processing The Use Case In this tutorial, we'll build an event-driven inventory management system for an e-commerce platform. The system will: Monitor product inventory levels in real-time Detect when items are running low Generate reorder events for the procurement system Alert warehouse staff about critical inventory situations Update inventory dashboards in real-time Setting Up the Environment First, let's set up our environment with Docker Compose: version: '3' services: zookeeper: image: confluentinc/cp-zookeeper:latest environment: ZOOKEEPER_CLIENT_PORT: 2181 kafka: image: confluentinc/cp-kafka:latest depends_on: - zookeeper environment: KAFKA_BROKER_ID: 1 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092 KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 ksml-runner: image: axual/ksml-runner:latest depends_on: - kafka volumes: - ./config:/config - ./definitions:/definitions Defining the Data Models Inventory Update Events { \"product_id\": \"prod-123\", \"product_name\": \"Wireless Headphones\", \"category\": \"electronics\", \"current_stock\": 15, \"warehouse_id\": \"wh-east-1\", \"timestamp\": 1625097600000, \"unit_price\": 79.99 } Order Events { \"order_id\": \"order-456\", \"customer_id\": \"cust-789\", \"items\": [ { \"product_id\": \"prod-123\", \"quantity\": 2, \"unit_price\": 79.99 } ], \"order_total\": 159.98, \"timestamp\": 1625097600000 } Reorder Events (Output) { \"event_id\": \"reorder-789\", \"product_id\": \"prod-123\", \"product_name\": \"Wireless Headphones\", \"current_stock\": 5, \"reorder_quantity\": 50, \"priority\": \"normal\", \"warehouse_id\": \"wh-east-1\", \"timestamp\": 1625097600000 } Alert Events (Output) { \"alert_id\": \"alert-123\", \"alert_type\": \"critical_inventory\", \"product_id\": \"prod-123\", \"product_name\": \"Wireless Headphones\", \"current_stock\": 2, \"threshold\": 5, \"warehouse_id\": \"wh-east-1\", \"timestamp\": 1625097600000, \"message\": \"Critical inventory level: Wireless Headphones (2 units remaining)\" } Creating the KSML Definition Now, let's create our KSML definition file: streams: inventory_updates: topic: inventory_updates keyType: string # product_id valueType: json # inventory data order_events: topic: order_events keyType: string # order_id valueType: json # order data product_reference: topic: product_reference keyType: string # product_id valueType: json # product details including thresholds reorder_events: topic: reorder_events keyType: string # event_id valueType: json # reorder data inventory_alerts: topic: inventory_alerts keyType: string # alert_id valueType: json # alert data functions: generate_reorder_event: type: mapValues parameters: - name: inventory type: object - name: product_info type: object code: | import uuid import time # Get reorder threshold and quantity from product reference data reorder_threshold = product_info.get(\"reorder_threshold\", 10) reorder_quantity = product_info.get(\"reorder_quantity\", 50) # Determine priority based on current stock current_stock = inventory.get(\"current_stock\", 0) priority = \"urgent\" if current_stock <= 5 else \"normal\" # Generate event ID event_id = f\"reorder-{uuid.uuid4().hex[:8]}\" # Create reorder event return { \"event_id\": event_id, \"product_id\": inventory.get(\"product_id\"), \"product_name\": inventory.get(\"product_name\"), \"current_stock\": current_stock, \"reorder_quantity\": reorder_quantity, \"priority\": priority, \"warehouse_id\": inventory.get(\"warehouse_id\"), \"timestamp\": int(time.time() * 1000) } generate_alert: type: mapValues parameters: - name: inventory type: object - name: product_info type: object code: | import uuid import time # Get critical threshold from product reference data critical_threshold = product_info.get(\"critical_threshold\", 5) # Get current stock current_stock = inventory.get(\"current_stock\", 0) product_name = inventory.get(\"product_name\", \"Unknown Product\") # Generate alert ID alert_id = f\"alert-{uuid.uuid4().hex[:8]}\" # Create alert message message = f\"Critical inventory level: {product_name} ({current_stock} units remaining)\" # Create alert event return { \"alert_id\": alert_id, \"alert_type\": \"critical_inventory\", \"product_id\": inventory.get(\"product_id\"), \"product_name\": product_name, \"current_stock\": current_stock, \"threshold\": critical_threshold, \"warehouse_id\": inventory.get(\"warehouse_id\"), \"timestamp\": int(time.time() * 1000), \"message\": message } update_inventory_from_order: type: mapValues parameters: - name: order type: object - name: inventory type: object code: | if inventory is None or order is None: return None # Find the item in the order that matches this product product_id = inventory.get(\"product_id\") order_items = order.get(\"items\", []) ordered_quantity = 0 for item in order_items: if item.get(\"product_id\") == product_id: ordered_quantity += item.get(\"quantity\", 0) # If this product wasn't in the order, return unchanged inventory if ordered_quantity == 0: return None # Update inventory with new stock level current_stock = inventory.get(\"current_stock\", 0) new_stock = max(0, current_stock - ordered_quantity) return { **inventory, \"current_stock\": new_stock, \"last_order_id\": order.get(\"order_id\"), \"last_updated\": int(time.time() * 1000) } pipelines: # Pipeline for detecting low inventory and generating reorder events reorder_pipeline: from: inventory_updates via: - type: join with: product_reference - type: filter if: code: | reorder_threshold = foreignValue.get(\"reorder_threshold\", 10) return value.get(\"current_stock\", 0) <= reorder_threshold - type: mapValues mapper: code: generate_reorder_event(value, foreignValue) to: reorder_events # Pipeline for detecting critical inventory levels and generating alerts alert_pipeline: from: inventory_updates via: - type: join with: product_reference - type: filter if: code: | critical_threshold = foreignValue.get(\"critical_threshold\", 5) return value.get(\"current_stock\", 0) <= critical_threshold - type: mapValues mapper: code: generate_alert(value, foreignValue) to: inventory_alerts # Pipeline for updating inventory based on orders order_processing_pipeline: from: order_events via: - type: flatMap mapper: code: | # For each item in the order, emit a key-value pair with product_id as key result = [] for item in value.get(\"items\", []): product_id = item.get(\"product_id\") if product_id: result.append((product_id, value)) return result - type: join with: inventory_updates - type: mapValues mapper: code: update_inventory_from_order(value, foreignValue) - type: filter if: expression: value is not None to: inventory_updates Running the Application To run the application: Save the KSML definition to definitions/event_driven.yaml Create a configuration file at config/application.properties with your Kafka connection details Start the Docker Compose environment: docker-compose up -d Produce sample data to the input topics Monitor the output topics to see the events being generated Testing the Event-Driven System You can test the system by producing sample data to the input topics: # Produce product reference data echo 'prod-123:{\"product_id\":\"prod-123\",\"reorder_threshold\":10,\"reorder_quantity\":50,\"critical_threshold\":5}' | \\ kafka-console-producer --broker-list localhost:9092 --topic product_reference --property \"parse.key=true\" --property \"key.separator=:\" # Produce inventory update echo 'prod-123:{\"product_id\":\"prod-123\",\"product_name\":\"Wireless Headphones\",\"category\":\"electronics\",\"current_stock\":8,\"warehouse_id\":\"wh-east-1\",\"timestamp\":1625097600000,\"unit_price\":79.99}' | \\ kafka-console-producer --broker-list localhost:9092 --topic inventory_updates --property \"parse.key=true\" --property \"key.separator=:\" # Produce order event echo 'order-456:{\"order_id\":\"order-456\",\"customer_id\":\"cust-789\",\"items\":[{\"product_id\":\"prod-123\",\"quantity\":3,\"unit_price\":79.99}],\"order_total\":239.97,\"timestamp\":1625097600000}' | \\ kafka-console-producer --broker-list localhost:9092 --topic order_events --property \"parse.key=true\" --property \"key.separator=:\" Then, consume from the output topics to see the events being generated: # Monitor reorder events kafka-console-consumer --bootstrap-server localhost:9092 --topic reorder_events --from-beginning # Monitor alerts kafka-console-consumer --bootstrap-server localhost:9092 --topic inventory_alerts --from-beginning # Monitor updated inventory kafka-console-consumer --bootstrap-server localhost:9092 --topic inventory_updates --from-beginning Extending the Event-Driven System Integration with External Systems To make this event-driven system truly useful, you can integrate it with external systems: Procurement System : Connect the reorder events to your procurement system to automatically create purchase orders Notification Service : Send the alerts to a notification service that can email or text warehouse staff Analytics Platform : Stream all events to an analytics platform for business intelligence Dashboard : Connect to a real-time dashboard for inventory visualization Adding More Event Types You can extend the system with additional event types: Price Change Events : Automatically adjust prices based on inventory levels or competitor data Promotion Events : Trigger promotions for overstocked items Fraud Detection Events : Flag suspicious order patterns Shipping Delay Events : Notify customers about potential delays due to inventory issues Best Practices for Event-Driven Applications When building event-driven applications with KSML, consider these best practices: Event Schema Design : Design your events to be self-contained and include all necessary context Idempotent Processing : Ensure your event handlers can process the same event multiple times without side effects Event Versioning : Include version information in your events to handle schema evolution Monitoring and Observability : Add logging and metrics to track event flow and processing Error Handling : Implement proper error handling and dead-letter queues for failed events Conclusion In this tutorial, you've learned how to: Build an event-driven application using KSML Detect specific conditions in your data streams Generate events in response to those conditions Process events to update state and trigger further actions Design an end-to-end event-driven architecture Event-driven applications are a powerful use case for KSML, allowing you to build responsive, real-time systems that react automatically to changing conditions. Next Steps Learn about Real-Time Analytics to analyze your event data Explore Data Transformation for more complex event processing Check out External Integration for connecting your events to external systems","title":"Event-Driven Applications with KSML"},{"location":"use-cases/event-driven-applications/#event-driven-applications-with-ksml","text":"This tutorial demonstrates how to build event-driven applications using KSML. You'll learn how to detect specific events in your data streams and trigger appropriate actions in response.","title":"Event-Driven Applications with KSML"},{"location":"use-cases/event-driven-applications/#introduction","text":"Event-driven architecture is a powerful paradigm for building responsive, real-time applications. In this approach: Systems react to events as they occur Components communicate through events rather than direct calls Business logic is triggered by changes in state Applications can scale and evolve independently KSML is particularly well-suited for event-driven applications because it allows you to: Process streams of events in real-time Detect complex patterns and conditions Transform events into actionable insights Trigger downstream processes automatically","title":"Introduction"},{"location":"use-cases/event-driven-applications/#prerequisites","text":"Before starting this tutorial, you should: Understand basic KSML concepts (streams, functions, pipelines) Have completed the KSML Basics Tutorial Be familiar with Filtering and Transforming Have a basic understanding of Complex Event Processing","title":"Prerequisites"},{"location":"use-cases/event-driven-applications/#the-use-case","text":"In this tutorial, we'll build an event-driven inventory management system for an e-commerce platform. The system will: Monitor product inventory levels in real-time Detect when items are running low Generate reorder events for the procurement system Alert warehouse staff about critical inventory situations Update inventory dashboards in real-time","title":"The Use Case"},{"location":"use-cases/event-driven-applications/#setting-up-the-environment","text":"First, let's set up our environment with Docker Compose: version: '3' services: zookeeper: image: confluentinc/cp-zookeeper:latest environment: ZOOKEEPER_CLIENT_PORT: 2181 kafka: image: confluentinc/cp-kafka:latest depends_on: - zookeeper environment: KAFKA_BROKER_ID: 1 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092 KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 ksml-runner: image: axual/ksml-runner:latest depends_on: - kafka volumes: - ./config:/config - ./definitions:/definitions","title":"Setting Up the Environment"},{"location":"use-cases/event-driven-applications/#defining-the-data-models","text":"","title":"Defining the Data Models"},{"location":"use-cases/event-driven-applications/#inventory-update-events","text":"{ \"product_id\": \"prod-123\", \"product_name\": \"Wireless Headphones\", \"category\": \"electronics\", \"current_stock\": 15, \"warehouse_id\": \"wh-east-1\", \"timestamp\": 1625097600000, \"unit_price\": 79.99 }","title":"Inventory Update Events"},{"location":"use-cases/event-driven-applications/#order-events","text":"{ \"order_id\": \"order-456\", \"customer_id\": \"cust-789\", \"items\": [ { \"product_id\": \"prod-123\", \"quantity\": 2, \"unit_price\": 79.99 } ], \"order_total\": 159.98, \"timestamp\": 1625097600000 }","title":"Order Events"},{"location":"use-cases/event-driven-applications/#reorder-events-output","text":"{ \"event_id\": \"reorder-789\", \"product_id\": \"prod-123\", \"product_name\": \"Wireless Headphones\", \"current_stock\": 5, \"reorder_quantity\": 50, \"priority\": \"normal\", \"warehouse_id\": \"wh-east-1\", \"timestamp\": 1625097600000 }","title":"Reorder Events (Output)"},{"location":"use-cases/event-driven-applications/#alert-events-output","text":"{ \"alert_id\": \"alert-123\", \"alert_type\": \"critical_inventory\", \"product_id\": \"prod-123\", \"product_name\": \"Wireless Headphones\", \"current_stock\": 2, \"threshold\": 5, \"warehouse_id\": \"wh-east-1\", \"timestamp\": 1625097600000, \"message\": \"Critical inventory level: Wireless Headphones (2 units remaining)\" }","title":"Alert Events (Output)"},{"location":"use-cases/event-driven-applications/#creating-the-ksml-definition","text":"Now, let's create our KSML definition file: streams: inventory_updates: topic: inventory_updates keyType: string # product_id valueType: json # inventory data order_events: topic: order_events keyType: string # order_id valueType: json # order data product_reference: topic: product_reference keyType: string # product_id valueType: json # product details including thresholds reorder_events: topic: reorder_events keyType: string # event_id valueType: json # reorder data inventory_alerts: topic: inventory_alerts keyType: string # alert_id valueType: json # alert data functions: generate_reorder_event: type: mapValues parameters: - name: inventory type: object - name: product_info type: object code: | import uuid import time # Get reorder threshold and quantity from product reference data reorder_threshold = product_info.get(\"reorder_threshold\", 10) reorder_quantity = product_info.get(\"reorder_quantity\", 50) # Determine priority based on current stock current_stock = inventory.get(\"current_stock\", 0) priority = \"urgent\" if current_stock <= 5 else \"normal\" # Generate event ID event_id = f\"reorder-{uuid.uuid4().hex[:8]}\" # Create reorder event return { \"event_id\": event_id, \"product_id\": inventory.get(\"product_id\"), \"product_name\": inventory.get(\"product_name\"), \"current_stock\": current_stock, \"reorder_quantity\": reorder_quantity, \"priority\": priority, \"warehouse_id\": inventory.get(\"warehouse_id\"), \"timestamp\": int(time.time() * 1000) } generate_alert: type: mapValues parameters: - name: inventory type: object - name: product_info type: object code: | import uuid import time # Get critical threshold from product reference data critical_threshold = product_info.get(\"critical_threshold\", 5) # Get current stock current_stock = inventory.get(\"current_stock\", 0) product_name = inventory.get(\"product_name\", \"Unknown Product\") # Generate alert ID alert_id = f\"alert-{uuid.uuid4().hex[:8]}\" # Create alert message message = f\"Critical inventory level: {product_name} ({current_stock} units remaining)\" # Create alert event return { \"alert_id\": alert_id, \"alert_type\": \"critical_inventory\", \"product_id\": inventory.get(\"product_id\"), \"product_name\": product_name, \"current_stock\": current_stock, \"threshold\": critical_threshold, \"warehouse_id\": inventory.get(\"warehouse_id\"), \"timestamp\": int(time.time() * 1000), \"message\": message } update_inventory_from_order: type: mapValues parameters: - name: order type: object - name: inventory type: object code: | if inventory is None or order is None: return None # Find the item in the order that matches this product product_id = inventory.get(\"product_id\") order_items = order.get(\"items\", []) ordered_quantity = 0 for item in order_items: if item.get(\"product_id\") == product_id: ordered_quantity += item.get(\"quantity\", 0) # If this product wasn't in the order, return unchanged inventory if ordered_quantity == 0: return None # Update inventory with new stock level current_stock = inventory.get(\"current_stock\", 0) new_stock = max(0, current_stock - ordered_quantity) return { **inventory, \"current_stock\": new_stock, \"last_order_id\": order.get(\"order_id\"), \"last_updated\": int(time.time() * 1000) } pipelines: # Pipeline for detecting low inventory and generating reorder events reorder_pipeline: from: inventory_updates via: - type: join with: product_reference - type: filter if: code: | reorder_threshold = foreignValue.get(\"reorder_threshold\", 10) return value.get(\"current_stock\", 0) <= reorder_threshold - type: mapValues mapper: code: generate_reorder_event(value, foreignValue) to: reorder_events # Pipeline for detecting critical inventory levels and generating alerts alert_pipeline: from: inventory_updates via: - type: join with: product_reference - type: filter if: code: | critical_threshold = foreignValue.get(\"critical_threshold\", 5) return value.get(\"current_stock\", 0) <= critical_threshold - type: mapValues mapper: code: generate_alert(value, foreignValue) to: inventory_alerts # Pipeline for updating inventory based on orders order_processing_pipeline: from: order_events via: - type: flatMap mapper: code: | # For each item in the order, emit a key-value pair with product_id as key result = [] for item in value.get(\"items\", []): product_id = item.get(\"product_id\") if product_id: result.append((product_id, value)) return result - type: join with: inventory_updates - type: mapValues mapper: code: update_inventory_from_order(value, foreignValue) - type: filter if: expression: value is not None to: inventory_updates","title":"Creating the KSML Definition"},{"location":"use-cases/event-driven-applications/#running-the-application","text":"To run the application: Save the KSML definition to definitions/event_driven.yaml Create a configuration file at config/application.properties with your Kafka connection details Start the Docker Compose environment: docker-compose up -d Produce sample data to the input topics Monitor the output topics to see the events being generated","title":"Running the Application"},{"location":"use-cases/event-driven-applications/#testing-the-event-driven-system","text":"You can test the system by producing sample data to the input topics: # Produce product reference data echo 'prod-123:{\"product_id\":\"prod-123\",\"reorder_threshold\":10,\"reorder_quantity\":50,\"critical_threshold\":5}' | \\ kafka-console-producer --broker-list localhost:9092 --topic product_reference --property \"parse.key=true\" --property \"key.separator=:\" # Produce inventory update echo 'prod-123:{\"product_id\":\"prod-123\",\"product_name\":\"Wireless Headphones\",\"category\":\"electronics\",\"current_stock\":8,\"warehouse_id\":\"wh-east-1\",\"timestamp\":1625097600000,\"unit_price\":79.99}' | \\ kafka-console-producer --broker-list localhost:9092 --topic inventory_updates --property \"parse.key=true\" --property \"key.separator=:\" # Produce order event echo 'order-456:{\"order_id\":\"order-456\",\"customer_id\":\"cust-789\",\"items\":[{\"product_id\":\"prod-123\",\"quantity\":3,\"unit_price\":79.99}],\"order_total\":239.97,\"timestamp\":1625097600000}' | \\ kafka-console-producer --broker-list localhost:9092 --topic order_events --property \"parse.key=true\" --property \"key.separator=:\" Then, consume from the output topics to see the events being generated: # Monitor reorder events kafka-console-consumer --bootstrap-server localhost:9092 --topic reorder_events --from-beginning # Monitor alerts kafka-console-consumer --bootstrap-server localhost:9092 --topic inventory_alerts --from-beginning # Monitor updated inventory kafka-console-consumer --bootstrap-server localhost:9092 --topic inventory_updates --from-beginning","title":"Testing the Event-Driven System"},{"location":"use-cases/event-driven-applications/#extending-the-event-driven-system","text":"","title":"Extending the Event-Driven System"},{"location":"use-cases/event-driven-applications/#integration-with-external-systems","text":"To make this event-driven system truly useful, you can integrate it with external systems: Procurement System : Connect the reorder events to your procurement system to automatically create purchase orders Notification Service : Send the alerts to a notification service that can email or text warehouse staff Analytics Platform : Stream all events to an analytics platform for business intelligence Dashboard : Connect to a real-time dashboard for inventory visualization","title":"Integration with External Systems"},{"location":"use-cases/event-driven-applications/#adding-more-event-types","text":"You can extend the system with additional event types: Price Change Events : Automatically adjust prices based on inventory levels or competitor data Promotion Events : Trigger promotions for overstocked items Fraud Detection Events : Flag suspicious order patterns Shipping Delay Events : Notify customers about potential delays due to inventory issues","title":"Adding More Event Types"},{"location":"use-cases/event-driven-applications/#best-practices-for-event-driven-applications","text":"When building event-driven applications with KSML, consider these best practices: Event Schema Design : Design your events to be self-contained and include all necessary context Idempotent Processing : Ensure your event handlers can process the same event multiple times without side effects Event Versioning : Include version information in your events to handle schema evolution Monitoring and Observability : Add logging and metrics to track event flow and processing Error Handling : Implement proper error handling and dead-letter queues for failed events","title":"Best Practices for Event-Driven Applications"},{"location":"use-cases/event-driven-applications/#conclusion","text":"In this tutorial, you've learned how to: Build an event-driven application using KSML Detect specific conditions in your data streams Generate events in response to those conditions Process events to update state and trigger further actions Design an end-to-end event-driven architecture Event-driven applications are a powerful use case for KSML, allowing you to build responsive, real-time systems that react automatically to changing conditions.","title":"Conclusion"},{"location":"use-cases/event-driven-applications/#next-steps","text":"Learn about Real-Time Analytics to analyze your event data Explore Data Transformation for more complex event processing Check out External Integration for connecting your events to external systems","title":"Next Steps"},{"location":"use-cases/microservices-integration/","text":"Microservices Integration with KSML This guide demonstrates how to use KSML to integrate with microservices architectures, enabling event-driven communication between services. Overview Microservices architectures break applications into small, independently deployable services. Kafka serves as an ideal backbone for communication between these services, and KSML makes it easy to build the integration layer. Use Case Scenarios 1. Service-to-Service Communication Use KSML to create data pipelines that transform and route events between microservices: pipelines: - name: order-processing-pipeline inputs: - name: new-orders topic: new-orders keyType: STRING valueType: AVRO valueSchema: Order operations: - type: mapValues mapper: code: | # Enrich order with additional information order_id = value.get(\"orderId\") customer_id = value.get(\"customerId\") # Add processing timestamp value[\"processingTimestamp\"] = int(time.time() * 1000) return value outputs: - name: processed-orders topic: processed-orders 2. Event Sourcing Implement event sourcing patterns where services publish events to a log and other services consume these events: pipelines: - name: event-sourcing-pipeline inputs: - name: domain-events topic: domain-events keyType: STRING valueType: JSON operations: - type: branch predicates: - name: user-events predicate: expression: value.get(\"eventType\").startswith(\"USER_\") - name: order-events predicate: expression: value.get(\"eventType\").startswith(\"ORDER_\") - name: payment-events predicate: expression: value.get(\"eventType\").startswith(\"PAYMENT_\") outputs: - name: user-events topic: user-events - name: order-events topic: order-events - name: payment-events topic: payment-events 3. Command-Query Responsibility Segregation (CQRS) Implement CQRS patterns where write and read operations use different models: pipelines: - name: cqrs-pipeline inputs: - name: write-events topic: write-events keyType: STRING valueType: JSON operations: - type: mapValues mapper: code: | # Transform write model to read model event_type = value.get(\"eventType\") entity_id = value.get(\"entityId\") data = value.get(\"data\") read_model = { \"id\": entity_id, \"timestamp\": int(time.time() * 1000), \"type\": event_type } # Add specific fields based on event type if event_type == \"USER_CREATED\": read_model[\"username\"] = data.get(\"username\") read_model[\"email\"] = data.get(\"email\") elif event_type == \"USER_UPDATED\": read_model[\"updatedFields\"] = data.get(\"changedFields\") return read_model outputs: - name: read-model topic: read-model Best Practices 1. Schema Evolution When integrating microservices, schema evolution is critical. Use KSML with Avro schemas and a schema registry: inputs: - name: service-events topic: service-events keyType: STRING valueType: AVRO valueSchema: ServiceEvent schemaRegistry: http://schema-registry:8081 2. Error Handling Implement robust error handling to ensure service resilience: operations: - type: mapValues mapper: code: | try: # Transform message return transformed_value except Exception as e: # Log error and return a structured error message return { \"error\": True, \"originalMessage\": value, \"errorMessage\": str(e), \"timestamp\": int(time.time() * 1000) } 3. Service Discovery Use configuration to make service endpoints configurable: configuration: properties: service.discovery.url: http://service-registry:8500 Integration Patterns 1. API Gateway Integration Connect KSML pipelines with API gateways to bridge HTTP and event-driven worlds: # Consume events from API gateway pipelines: - name: api-gateway-integration inputs: - name: api-requests topic: api-gateway-requests keyType: STRING valueType: JSON operations: - type: mapValues mapper: code: | # Transform HTTP request to domain event return { \"eventType\": \"API_REQUEST\", \"path\": value.get(\"path\"), \"method\": value.get(\"method\"), \"payload\": value.get(\"body\"), \"requestId\": value.get(\"requestId\"), \"timestamp\": int(time.time() * 1000) } outputs: - name: domain-events topic: domain-events 2. Database Change Data Capture (CDC) Integrate with CDC tools like Debezium to capture database changes: pipelines: - name: cdc-integration inputs: - name: db-changes topic: mysql.inventory.customers keyType: JSON valueType: JSON operations: - type: filter predicate: expression: value != null and value.get(\"op\") in [\"c\", \"u\"] - type: mapValues mapper: code: | operation = value.get(\"op\") data = value.get(\"after\") return { \"eventType\": \"CUSTOMER_UPDATED\" if operation == \"u\" else \"CUSTOMER_CREATED\", \"customerId\": data.get(\"id\"), \"customerData\": data, \"timestamp\": int(time.time() * 1000) } outputs: - name: customer-events topic: customer-events Monitoring and Observability Implement monitoring for your microservices integration: configuration: metrics: enabled: true reporter: prometheus tags: application: \"microservices-integration\" team: \"platform-team\" Conclusion KSML provides a powerful yet simple way to implement integration patterns in microservices architectures. By leveraging Kafka's distributed nature and KSML's declarative approach, you can build resilient, scalable, and maintainable integration solutions. Next Steps Explore Event-Driven Applications for more advanced patterns Learn about Performance Optimization for high-throughput scenarios Check out External Integration for connecting with external systems","title":"Microservices Integration with KSML"},{"location":"use-cases/microservices-integration/#microservices-integration-with-ksml","text":"This guide demonstrates how to use KSML to integrate with microservices architectures, enabling event-driven communication between services.","title":"Microservices Integration with KSML"},{"location":"use-cases/microservices-integration/#overview","text":"Microservices architectures break applications into small, independently deployable services. Kafka serves as an ideal backbone for communication between these services, and KSML makes it easy to build the integration layer.","title":"Overview"},{"location":"use-cases/microservices-integration/#use-case-scenarios","text":"","title":"Use Case Scenarios"},{"location":"use-cases/microservices-integration/#1-service-to-service-communication","text":"Use KSML to create data pipelines that transform and route events between microservices: pipelines: - name: order-processing-pipeline inputs: - name: new-orders topic: new-orders keyType: STRING valueType: AVRO valueSchema: Order operations: - type: mapValues mapper: code: | # Enrich order with additional information order_id = value.get(\"orderId\") customer_id = value.get(\"customerId\") # Add processing timestamp value[\"processingTimestamp\"] = int(time.time() * 1000) return value outputs: - name: processed-orders topic: processed-orders","title":"1. Service-to-Service Communication"},{"location":"use-cases/microservices-integration/#2-event-sourcing","text":"Implement event sourcing patterns where services publish events to a log and other services consume these events: pipelines: - name: event-sourcing-pipeline inputs: - name: domain-events topic: domain-events keyType: STRING valueType: JSON operations: - type: branch predicates: - name: user-events predicate: expression: value.get(\"eventType\").startswith(\"USER_\") - name: order-events predicate: expression: value.get(\"eventType\").startswith(\"ORDER_\") - name: payment-events predicate: expression: value.get(\"eventType\").startswith(\"PAYMENT_\") outputs: - name: user-events topic: user-events - name: order-events topic: order-events - name: payment-events topic: payment-events","title":"2. Event Sourcing"},{"location":"use-cases/microservices-integration/#3-command-query-responsibility-segregation-cqrs","text":"Implement CQRS patterns where write and read operations use different models: pipelines: - name: cqrs-pipeline inputs: - name: write-events topic: write-events keyType: STRING valueType: JSON operations: - type: mapValues mapper: code: | # Transform write model to read model event_type = value.get(\"eventType\") entity_id = value.get(\"entityId\") data = value.get(\"data\") read_model = { \"id\": entity_id, \"timestamp\": int(time.time() * 1000), \"type\": event_type } # Add specific fields based on event type if event_type == \"USER_CREATED\": read_model[\"username\"] = data.get(\"username\") read_model[\"email\"] = data.get(\"email\") elif event_type == \"USER_UPDATED\": read_model[\"updatedFields\"] = data.get(\"changedFields\") return read_model outputs: - name: read-model topic: read-model","title":"3. Command-Query Responsibility Segregation (CQRS)"},{"location":"use-cases/microservices-integration/#best-practices","text":"","title":"Best Practices"},{"location":"use-cases/microservices-integration/#1-schema-evolution","text":"When integrating microservices, schema evolution is critical. Use KSML with Avro schemas and a schema registry: inputs: - name: service-events topic: service-events keyType: STRING valueType: AVRO valueSchema: ServiceEvent schemaRegistry: http://schema-registry:8081","title":"1. Schema Evolution"},{"location":"use-cases/microservices-integration/#2-error-handling","text":"Implement robust error handling to ensure service resilience: operations: - type: mapValues mapper: code: | try: # Transform message return transformed_value except Exception as e: # Log error and return a structured error message return { \"error\": True, \"originalMessage\": value, \"errorMessage\": str(e), \"timestamp\": int(time.time() * 1000) }","title":"2. Error Handling"},{"location":"use-cases/microservices-integration/#3-service-discovery","text":"Use configuration to make service endpoints configurable: configuration: properties: service.discovery.url: http://service-registry:8500","title":"3. Service Discovery"},{"location":"use-cases/microservices-integration/#integration-patterns","text":"","title":"Integration Patterns"},{"location":"use-cases/microservices-integration/#1-api-gateway-integration","text":"Connect KSML pipelines with API gateways to bridge HTTP and event-driven worlds: # Consume events from API gateway pipelines: - name: api-gateway-integration inputs: - name: api-requests topic: api-gateway-requests keyType: STRING valueType: JSON operations: - type: mapValues mapper: code: | # Transform HTTP request to domain event return { \"eventType\": \"API_REQUEST\", \"path\": value.get(\"path\"), \"method\": value.get(\"method\"), \"payload\": value.get(\"body\"), \"requestId\": value.get(\"requestId\"), \"timestamp\": int(time.time() * 1000) } outputs: - name: domain-events topic: domain-events","title":"1. API Gateway Integration"},{"location":"use-cases/microservices-integration/#2-database-change-data-capture-cdc","text":"Integrate with CDC tools like Debezium to capture database changes: pipelines: - name: cdc-integration inputs: - name: db-changes topic: mysql.inventory.customers keyType: JSON valueType: JSON operations: - type: filter predicate: expression: value != null and value.get(\"op\") in [\"c\", \"u\"] - type: mapValues mapper: code: | operation = value.get(\"op\") data = value.get(\"after\") return { \"eventType\": \"CUSTOMER_UPDATED\" if operation == \"u\" else \"CUSTOMER_CREATED\", \"customerId\": data.get(\"id\"), \"customerData\": data, \"timestamp\": int(time.time() * 1000) } outputs: - name: customer-events topic: customer-events","title":"2. Database Change Data Capture (CDC)"},{"location":"use-cases/microservices-integration/#monitoring-and-observability","text":"Implement monitoring for your microservices integration: configuration: metrics: enabled: true reporter: prometheus tags: application: \"microservices-integration\" team: \"platform-team\"","title":"Monitoring and Observability"},{"location":"use-cases/microservices-integration/#conclusion","text":"KSML provides a powerful yet simple way to implement integration patterns in microservices architectures. By leveraging Kafka's distributed nature and KSML's declarative approach, you can build resilient, scalable, and maintainable integration solutions.","title":"Conclusion"},{"location":"use-cases/microservices-integration/#next-steps","text":"Explore Event-Driven Applications for more advanced patterns Learn about Performance Optimization for high-throughput scenarios Check out External Integration for connecting with external systems","title":"Next Steps"},{"location":"use-cases/real-time-analytics/","text":"Real-Time Analytics with KSML This tutorial demonstrates how to build a real-time analytics application using KSML. You'll learn how to process streaming data, calculate metrics in real-time, and visualize the results. Introduction Real-time analytics is one of the most common use cases for stream processing. By analyzing data as it arrives, you can: Detect trends and patterns as they emerge Respond quickly to changing conditions Make data-driven decisions with minimal latency Provide up-to-date dashboards and visualizations In this tutorial, we'll build a real-time analytics pipeline that processes a stream of e-commerce transactions, calculates various metrics, and makes the results available for visualization. Prerequisites Before starting this tutorial, you should: Understand basic KSML concepts (streams, functions, pipelines) Have completed the KSML Basics Tutorial Be familiar with Aggregations Have a basic understanding of Windowed Operations The Use Case Imagine you're running an e-commerce platform and want to analyze transaction data in real-time. You want to track: Total sales by product category Average order value Transaction volume by region Conversion rates from different marketing channels Setting Up the Environment First, let's set up our environment with Docker Compose: version: '3' services: zookeeper: image: confluentinc/cp-zookeeper:latest environment: ZOOKEEPER_CLIENT_PORT: 2181 kafka: image: confluentinc/cp-kafka:latest depends_on: - zookeeper environment: KAFKA_BROKER_ID: 1 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092 KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 ksml-runner: image: axual/ksml-runner:latest depends_on: - kafka volumes: - ./config:/config - ./definitions:/definitions Defining the Data Model Our transaction data will have the following structure: { \"transaction_id\": \"12345\", \"timestamp\": 1625097600000, \"customer_id\": \"cust-789\", \"product_id\": \"prod-456\", \"product_category\": \"electronics\", \"quantity\": 1, \"price\": 499.99, \"region\": \"north_america\", \"marketing_channel\": \"social_media\" } Creating the KSML Definition Now, let's create our KSML definition file: streams: transactions: topic: ecommerce_transactions keyType: string # transaction_id valueType: json # transaction data sales_by_category: topic: sales_by_category keyType: string # product_category valueType: json # aggregated sales data avg_order_value: topic: avg_order_value keyType: string # time window valueType: json # average order value transactions_by_region: topic: transactions_by_region keyType: string # region valueType: json # transaction count conversion_by_channel: topic: conversion_by_channel keyType: string # marketing_channel valueType: json # conversion metrics functions: extract_category: type: mapValues parameters: - name: value type: object code: | return value.get(\"product_category\") calculate_total: type: aggregate parameters: - name: value type: object - name: aggregate type: object code: | if aggregate is None: return {\"total_sales\": value.get(\"price\") * value.get(\"quantity\"), \"count\": 1} else: return { \"total_sales\": aggregate.get(\"total_sales\") + (value.get(\"price\") * value.get(\"quantity\")), \"count\": aggregate.get(\"count\") + 1 } pipelines: # Pipeline for sales by category sales_by_category_pipeline: from: transactions via: - type: selectKey keySelector: expression: value.get(\"product_category\") - type: groupByKey - type: aggregate initializer: expression: {\"total_sales\": 0, \"count\": 0} aggregator: code: calculate_total(value, aggregate) to: sales_by_category # Pipeline for average order value (windowed) avg_order_value_pipeline: from: transactions via: - type: groupByKey - type: windowedBy timeDifference: 60000 # 1 minute window - type: aggregate initializer: expression: {\"total_sales\": 0, \"count\": 0} aggregator: code: calculate_total(value, aggregate) - type: mapValues mapper: expression: {\"avg_order_value\": aggregate.get(\"total_sales\") / aggregate.get(\"count\")} to: avg_order_value # Pipeline for transactions by region transactions_by_region_pipeline: from: transactions via: - type: selectKey keySelector: expression: value.get(\"region\") - type: groupByKey - type: count to: transactions_by_region # Pipeline for conversion by marketing channel conversion_by_channel_pipeline: from: transactions via: - type: selectKey keySelector: expression: value.get(\"marketing_channel\") - type: groupByKey - type: aggregate initializer: expression: {\"views\": 0, \"purchases\": 1, \"conversion_rate\": 0} aggregator: code: | if aggregate is None: return {\"views\": 0, \"purchases\": 1, \"conversion_rate\": 0} else: purchases = aggregate.get(\"purchases\") + 1 views = aggregate.get(\"views\") return { \"views\": views, \"purchases\": purchases, \"conversion_rate\": purchases / views if views > 0 else 0 } to: conversion_by_channel Running the Application To run the application: Save the KSML definition to definitions/analytics.yaml Create a configuration file at config/application.properties with your Kafka connection details Start the Docker Compose environment: docker-compose up -d Monitor the output topics to see the real-time analytics results Visualizing the Results You can connect the output topics to visualization tools like: Grafana Kibana Custom dashboards using web frameworks For example, to create a simple dashboard with Grafana: Set up Grafana to connect to your Kafka topics Create dashboards for each metric: Bar chart for sales by category Line chart for average order value over time Map visualization for transactions by region Gauge charts for conversion rates Extending the Application You can extend this application in several ways: Add anomaly detection to identify unusual patterns Implement trend analysis to detect changing consumer behavior Create alerts for specific conditions (e.g., sales dropping below a threshold) Enrich the data with additional information from reference data sources Conclusion In this tutorial, you've learned how to: Process streaming transaction data in real-time Calculate various business metrics using KSML Structure a real-time analytics application Prepare the results for visualization Real-time analytics is a powerful use case for KSML, allowing you to gain immediate insights from your streaming data without complex coding. Next Steps Learn about Data Transformation for more complex processing Explore Event-Driven Applications to trigger actions based on analytics Check out Performance Optimization for handling high-volume data","title":"Real-Time Analytics with KSML"},{"location":"use-cases/real-time-analytics/#real-time-analytics-with-ksml","text":"This tutorial demonstrates how to build a real-time analytics application using KSML. You'll learn how to process streaming data, calculate metrics in real-time, and visualize the results.","title":"Real-Time Analytics with KSML"},{"location":"use-cases/real-time-analytics/#introduction","text":"Real-time analytics is one of the most common use cases for stream processing. By analyzing data as it arrives, you can: Detect trends and patterns as they emerge Respond quickly to changing conditions Make data-driven decisions with minimal latency Provide up-to-date dashboards and visualizations In this tutorial, we'll build a real-time analytics pipeline that processes a stream of e-commerce transactions, calculates various metrics, and makes the results available for visualization.","title":"Introduction"},{"location":"use-cases/real-time-analytics/#prerequisites","text":"Before starting this tutorial, you should: Understand basic KSML concepts (streams, functions, pipelines) Have completed the KSML Basics Tutorial Be familiar with Aggregations Have a basic understanding of Windowed Operations","title":"Prerequisites"},{"location":"use-cases/real-time-analytics/#the-use-case","text":"Imagine you're running an e-commerce platform and want to analyze transaction data in real-time. You want to track: Total sales by product category Average order value Transaction volume by region Conversion rates from different marketing channels","title":"The Use Case"},{"location":"use-cases/real-time-analytics/#setting-up-the-environment","text":"First, let's set up our environment with Docker Compose: version: '3' services: zookeeper: image: confluentinc/cp-zookeeper:latest environment: ZOOKEEPER_CLIENT_PORT: 2181 kafka: image: confluentinc/cp-kafka:latest depends_on: - zookeeper environment: KAFKA_BROKER_ID: 1 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092 KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 ksml-runner: image: axual/ksml-runner:latest depends_on: - kafka volumes: - ./config:/config - ./definitions:/definitions","title":"Setting Up the Environment"},{"location":"use-cases/real-time-analytics/#defining-the-data-model","text":"Our transaction data will have the following structure: { \"transaction_id\": \"12345\", \"timestamp\": 1625097600000, \"customer_id\": \"cust-789\", \"product_id\": \"prod-456\", \"product_category\": \"electronics\", \"quantity\": 1, \"price\": 499.99, \"region\": \"north_america\", \"marketing_channel\": \"social_media\" }","title":"Defining the Data Model"},{"location":"use-cases/real-time-analytics/#creating-the-ksml-definition","text":"Now, let's create our KSML definition file: streams: transactions: topic: ecommerce_transactions keyType: string # transaction_id valueType: json # transaction data sales_by_category: topic: sales_by_category keyType: string # product_category valueType: json # aggregated sales data avg_order_value: topic: avg_order_value keyType: string # time window valueType: json # average order value transactions_by_region: topic: transactions_by_region keyType: string # region valueType: json # transaction count conversion_by_channel: topic: conversion_by_channel keyType: string # marketing_channel valueType: json # conversion metrics functions: extract_category: type: mapValues parameters: - name: value type: object code: | return value.get(\"product_category\") calculate_total: type: aggregate parameters: - name: value type: object - name: aggregate type: object code: | if aggregate is None: return {\"total_sales\": value.get(\"price\") * value.get(\"quantity\"), \"count\": 1} else: return { \"total_sales\": aggregate.get(\"total_sales\") + (value.get(\"price\") * value.get(\"quantity\")), \"count\": aggregate.get(\"count\") + 1 } pipelines: # Pipeline for sales by category sales_by_category_pipeline: from: transactions via: - type: selectKey keySelector: expression: value.get(\"product_category\") - type: groupByKey - type: aggregate initializer: expression: {\"total_sales\": 0, \"count\": 0} aggregator: code: calculate_total(value, aggregate) to: sales_by_category # Pipeline for average order value (windowed) avg_order_value_pipeline: from: transactions via: - type: groupByKey - type: windowedBy timeDifference: 60000 # 1 minute window - type: aggregate initializer: expression: {\"total_sales\": 0, \"count\": 0} aggregator: code: calculate_total(value, aggregate) - type: mapValues mapper: expression: {\"avg_order_value\": aggregate.get(\"total_sales\") / aggregate.get(\"count\")} to: avg_order_value # Pipeline for transactions by region transactions_by_region_pipeline: from: transactions via: - type: selectKey keySelector: expression: value.get(\"region\") - type: groupByKey - type: count to: transactions_by_region # Pipeline for conversion by marketing channel conversion_by_channel_pipeline: from: transactions via: - type: selectKey keySelector: expression: value.get(\"marketing_channel\") - type: groupByKey - type: aggregate initializer: expression: {\"views\": 0, \"purchases\": 1, \"conversion_rate\": 0} aggregator: code: | if aggregate is None: return {\"views\": 0, \"purchases\": 1, \"conversion_rate\": 0} else: purchases = aggregate.get(\"purchases\") + 1 views = aggregate.get(\"views\") return { \"views\": views, \"purchases\": purchases, \"conversion_rate\": purchases / views if views > 0 else 0 } to: conversion_by_channel","title":"Creating the KSML Definition"},{"location":"use-cases/real-time-analytics/#running-the-application","text":"To run the application: Save the KSML definition to definitions/analytics.yaml Create a configuration file at config/application.properties with your Kafka connection details Start the Docker Compose environment: docker-compose up -d Monitor the output topics to see the real-time analytics results","title":"Running the Application"},{"location":"use-cases/real-time-analytics/#visualizing-the-results","text":"You can connect the output topics to visualization tools like: Grafana Kibana Custom dashboards using web frameworks For example, to create a simple dashboard with Grafana: Set up Grafana to connect to your Kafka topics Create dashboards for each metric: Bar chart for sales by category Line chart for average order value over time Map visualization for transactions by region Gauge charts for conversion rates","title":"Visualizing the Results"},{"location":"use-cases/real-time-analytics/#extending-the-application","text":"You can extend this application in several ways: Add anomaly detection to identify unusual patterns Implement trend analysis to detect changing consumer behavior Create alerts for specific conditions (e.g., sales dropping below a threshold) Enrich the data with additional information from reference data sources","title":"Extending the Application"},{"location":"use-cases/real-time-analytics/#conclusion","text":"In this tutorial, you've learned how to: Process streaming transaction data in real-time Calculate various business metrics using KSML Structure a real-time analytics application Prepare the results for visualization Real-time analytics is a powerful use case for KSML, allowing you to gain immediate insights from your streaming data without complex coding.","title":"Conclusion"},{"location":"use-cases/real-time-analytics/#next-steps","text":"Learn about Data Transformation for more complex processing Explore Event-Driven Applications to trigger actions based on analytics Check out Performance Optimization for handling high-volume data","title":"Next Steps"}]}