# Processor demonstrating retry strategies with exponential backoff

streams:
  failing_operations:
    topic: failing_operations
    keyType: string
    valueType: json

stores:
  retry_state_store:
    type: keyValue
    keyType: string
    valueType: string
    persistent: true
    caching: true

functions:
  retry_handler:
    type: valueTransformer
    globalCode: |
      import json
      import time
      import random
      
      # Simple retry configuration
      MAX_RETRIES = 3
      BASE_DELAY_MS = 1000  # 1 second base delay
      
      def get_retry_count(operation_id, store):
        """Get current retry count for operation"""
        count_str = store.get(f"retries_{operation_id}")
        return int(count_str) if count_str else 0
      
      def calculate_backoff_delay(attempt):
        """Calculate exponential backoff delay with jitter"""
        # Exponential backoff: 1s, 2s, 4s, 8s...
        delay = BASE_DELAY_MS * (2 ** attempt)
        # Add 10% jitter to prevent thundering herd
        jitter = delay * 0.1 * (random.random() * 2 - 1)
        return int(delay + jitter)
    
    code: |
      # Extract fields from JSON operation
      operation_type = value.get("operation_type")
      error_type = value.get("error_type", "none")
      operation_id_num = value.get("operation_id", 0)
      timestamp = value.get("timestamp", int(time.time() * 1000))
      should_succeed = value.get("should_succeed", True)
      client_id = value.get("client_id", "unknown")
      resource_id = value.get("resource_id", "unknown")
      metadata = value.get("metadata", {})
      current_time = int(time.time() * 1000)
      
      if not operation_type:
        return None
      
      operation_id = key
      current_retry = get_retry_count(operation_id, retry_state_store)
      
      # Handle successful operations
      if error_type == "none":
        if current_retry > 0:
          # Clear retry state for successful retry
          retry_state_store.delete(f"retries_{operation_id}")
          log.info("Operation {} succeeded after {} retries", operation_id, current_retry)
          return {
            "status": "SUCCESS_AFTER_RETRY",
            "operation_type": operation_type,
            "operation_id": operation_id_num,
            "client_id": client_id,
            "resource_id": resource_id,
            "retries": current_retry,
            "timestamp": timestamp,
            "processing_time": current_time - timestamp,
            "metadata": metadata
          }
        else:
          log.info("Operation {} succeeded on first attempt", operation_id)
          return {
            "status": "SUCCESS",
            "operation_type": operation_type,
            "operation_id": operation_id_num,
            "client_id": client_id,
            "resource_id": resource_id,
            "timestamp": timestamp,
            "processing_time": current_time - timestamp,
            "metadata": metadata
          }
      
      # Handle failed operations
      log.warn("Operation {} failed: {} (attempt {})", operation_id, error_type, current_retry + 1)
      
      # Check if error is retryable
      if error_type == "invalid_data":
        # Non-retryable error - send to permanent failure
        if current_retry > 0:
          retry_state_store.delete(f"retries_{operation_id}")
        log.error("Non-retryable error for {}: {}", operation_id, error_type)
        return {
          "status": "PERMANENT_FAILURE",
          "operation_type": operation_type,
          "operation_id": operation_id_num,
          "client_id": client_id,
          "resource_id": resource_id,
          "error_type": error_type,
          "attempts": current_retry + 1,
          "timestamp": timestamp,
          "processing_time": current_time - timestamp,
          "metadata": metadata
        }
      
      # Check retry limit for retryable errors (timeout, connection_error)
      if current_retry >= MAX_RETRIES:
        # Max retries exceeded
        retry_state_store.delete(f"retries_{operation_id}")
        log.error("Max retries exceeded for {}: {} attempts", operation_id, current_retry + 1)
        return {
          "status": "MAX_RETRIES_EXCEEDED",
          "operation_type": operation_type,
          "operation_id": operation_id_num,
          "client_id": client_id,
          "resource_id": resource_id,
          "error_type": error_type,
          "attempts": current_retry + 1,
          "timestamp": timestamp,
          "processing_time": current_time - timestamp,
          "metadata": metadata
        }
      
      # Schedule retry with exponential backoff
      new_retry_count = current_retry + 1
      retry_state_store.put(f"retries_{operation_id}", str(new_retry_count))
      
      backoff_delay = calculate_backoff_delay(current_retry)
      log.info("Scheduling retry {} for {} with {}ms delay", new_retry_count, operation_id, backoff_delay)
      
      return {
        "status": "RETRY_SCHEDULED",
        "operation_type": operation_type,
        "operation_id": operation_id_num,
        "client_id": client_id,
        "resource_id": resource_id,
        "error_type": error_type,
        "attempt": new_retry_count,
        "backoff_delay_ms": backoff_delay,
        "timestamp": timestamp,
        "processing_time": current_time - timestamp,
        "metadata": metadata
      }
      
    expression: result if result else None
    resultType: json
    stores:
      - retry_state_store

pipelines:
  retry_pipeline:
    from: failing_operations
    via:
      - type: mapValues
        mapper: retry_handler
      - type: filter
        if:
          expression: value is not None
    to:
      topic: retry_results
      keyType: string
      valueType: json